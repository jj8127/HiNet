21-12-24 10:50:10.472 - INFO: DataParallel(
  (module): Model(
    (model): Hinet(
      (inv1): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
      )
      (inv2): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
      )
      (inv3): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
      )
      (inv4): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
      )
      (inv5): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
      )
      (inv6): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
      )
      (inv7): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
      )
      (inv8): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
      )
      (inv9): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
      )
      (inv10): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
      )
      (inv11): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
      )
      (inv12): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
      )
      (inv13): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
      )
      (inv14): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
      )
      (inv15): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
      )
      (inv16): INV_block(
        (r): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (y): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
        (f): ResidualDenseBlock_out(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.01, inplace)
        )
      )
    )
  )
)
21-12-24 10:51:24.005 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 10:51:24.006 - INFO: Train epoch 1651:   Loss: 996.6664 | r_Loss: 87.0803 | g_Loss: 499.5519 | l_Loss: 61.7132 | 
21-12-24 10:52:37.878 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 10:52:37.879 - INFO: Train epoch 1652:   Loss: 934.8195 | r_Loss: 81.4228 | g_Loss: 465.0092 | l_Loss: 62.6962 | 
21-12-24 10:53:51.594 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 10:53:51.595 - INFO: Train epoch 1653:   Loss: 909.2602 | r_Loss: 85.9651 | g_Loss: 422.5426 | l_Loss: 56.8919 | 
21-12-24 10:55:05.322 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 10:55:05.323 - INFO: Train epoch 1654:   Loss: 834.6590 | r_Loss: 79.7185 | g_Loss: 393.1451 | l_Loss: 42.9211 | 
21-12-24 10:56:18.955 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 10:56:18.956 - INFO: Train epoch 1655:   Loss: 1296.1235 | r_Loss: 151.0322 | g_Loss: 462.2363 | l_Loss: 78.7261 | 
21-12-24 10:57:32.335 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 10:57:32.336 - INFO: Train epoch 1656:   Loss: 836.4362 | r_Loss: 74.6929 | g_Loss: 416.5005 | l_Loss: 46.4711 | 
21-12-24 10:58:45.987 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 10:58:45.988 - INFO: Train epoch 1657:   Loss: 770.1497 | r_Loss: 69.8997 | g_Loss: 372.9066 | l_Loss: 47.7447 | 
21-12-24 10:59:59.057 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 10:59:59.058 - INFO: Train epoch 1658:   Loss: 879.3608 | r_Loss: 84.0686 | g_Loss: 396.0211 | l_Loss: 62.9966 | 
21-12-24 11:01:12.716 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:01:12.717 - INFO: Train epoch 1659:   Loss: 795.9085 | r_Loss: 77.6426 | g_Loss: 362.8962 | l_Loss: 44.7994 | 
21-12-24 11:02:26.173 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:02:26.173 - INFO: Train epoch 1660:   Loss: 812.1923 | r_Loss: 80.2613 | g_Loss: 361.0567 | l_Loss: 49.8293 | 
21-12-24 11:03:39.323 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:03:39.325 - INFO: Train epoch 1661:   Loss: 844.8791 | r_Loss: 85.5502 | g_Loss: 362.2384 | l_Loss: 54.8897 | 
21-12-24 11:04:53.042 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:04:53.043 - INFO: Train epoch 1662:   Loss: 760.9282 | r_Loss: 75.5782 | g_Loss: 341.6751 | l_Loss: 41.3623 | 
21-12-24 11:06:06.290 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:06:06.291 - INFO: Train epoch 1663:   Loss: 783.3647 | r_Loss: 78.4957 | g_Loss: 348.9161 | l_Loss: 41.9701 | 
21-12-24 11:07:19.850 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:07:19.851 - INFO: Train epoch 1664:   Loss: 714.5819 | r_Loss: 70.6148 | g_Loss: 325.3690 | l_Loss: 36.1389 | 
21-12-24 11:08:33.661 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:08:33.662 - INFO: Train epoch 1665:   Loss: 828.1388 | r_Loss: 86.6758 | g_Loss: 353.0427 | l_Loss: 41.7171 | 
21-12-24 11:09:47.023 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:09:47.024 - INFO: Train epoch 1666:   Loss: 743.3908 | r_Loss: 73.9117 | g_Loss: 335.4105 | l_Loss: 38.4218 | 
21-12-24 11:11:00.957 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:11:00.958 - INFO: Train epoch 1667:   Loss: 757.1275 | r_Loss: 77.4477 | g_Loss: 329.1774 | l_Loss: 40.7116 | 
21-12-24 11:12:14.564 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:12:14.565 - INFO: Train epoch 1668:   Loss: 785.3291 | r_Loss: 77.8449 | g_Loss: 346.5850 | l_Loss: 49.5194 | 
21-12-24 11:13:28.145 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:13:28.147 - INFO: Train epoch 1669:   Loss: 728.2735 | r_Loss: 72.8905 | g_Loss: 321.7528 | l_Loss: 42.0681 | 
21-12-24 11:14:41.429 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:14:41.430 - INFO: Train epoch 1670:   Loss: 732.5073 | r_Loss: 74.8059 | g_Loss: 315.3424 | l_Loss: 43.1353 | 
21-12-24 11:15:55.292 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:15:55.293 - INFO: Train epoch 1671:   Loss: 771.3462 | r_Loss: 80.3056 | g_Loss: 325.4308 | l_Loss: 44.3874 | 
21-12-24 11:17:08.662 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:17:08.663 - INFO: Train epoch 1672:   Loss: 842.4972 | r_Loss: 92.4568 | g_Loss: 339.2938 | l_Loss: 40.9191 | 
21-12-24 11:18:22.081 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:18:22.082 - INFO: Train epoch 1673:   Loss: 8610.9523 | r_Loss: 1506.7883 | g_Loss: 958.7069 | l_Loss: 118.3040 | 
21-12-24 11:19:34.991 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:19:34.991 - INFO: Train epoch 1674:   Loss: 1021.1046 | r_Loss: 94.6165 | g_Loss: 483.1054 | l_Loss: 64.9166 | 
21-12-24 11:20:48.303 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:20:48.304 - INFO: Train epoch 1675:   Loss: 951.6090 | r_Loss: 84.5241 | g_Loss: 471.4960 | l_Loss: 57.4927 | 
21-12-24 11:22:01.863 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:22:01.864 - INFO: Train epoch 1676:   Loss: 890.1895 | r_Loss: 77.6636 | g_Loss: 442.2973 | l_Loss: 59.5743 | 
21-12-24 11:23:14.835 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:23:14.836 - INFO: Train epoch 1677:   Loss: 885.7175 | r_Loss: 78.1532 | g_Loss: 446.9193 | l_Loss: 48.0322 | 
21-12-24 11:24:28.151 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:24:28.152 - INFO: Train epoch 1678:   Loss: 824.7316 | r_Loss: 71.6695 | g_Loss: 419.0889 | l_Loss: 47.2951 | 
21-12-24 11:25:41.614 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:25:41.616 - INFO: Train epoch 1679:   Loss: 869.4428 | r_Loss: 77.9959 | g_Loss: 419.5999 | l_Loss: 59.8634 | 
21-12-24 11:27:31.224 - INFO: TEST:   PSNR_S: 44.2622 | PSNR_C: 35.5947 | 
21-12-24 11:27:31.225 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:27:31.225 - INFO: Train epoch 1680:   Loss: 866.8122 | r_Loss: 76.9340 | g_Loss: 423.7489 | l_Loss: 58.3931 | 
21-12-24 11:28:44.604 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:28:44.605 - INFO: Train epoch 1681:   Loss: 826.8867 | r_Loss: 71.9132 | g_Loss: 408.8606 | l_Loss: 58.4598 | 
21-12-24 11:29:58.321 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:29:58.322 - INFO: Train epoch 1682:   Loss: 748.3975 | r_Loss: 65.2267 | g_Loss: 381.7432 | l_Loss: 40.5209 | 
21-12-24 11:31:11.977 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:31:11.978 - INFO: Train epoch 1683:   Loss: 813.6189 | r_Loss: 71.7737 | g_Loss: 397.9074 | l_Loss: 56.8428 | 
21-12-24 11:32:25.389 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:32:25.390 - INFO: Train epoch 1684:   Loss: 825.0517 | r_Loss: 73.7446 | g_Loss: 408.7961 | l_Loss: 47.5326 | 
21-12-24 11:33:38.962 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:33:38.963 - INFO: Train epoch 1685:   Loss: 775.1033 | r_Loss: 69.8173 | g_Loss: 383.5295 | l_Loss: 42.4871 | 
21-12-24 11:34:52.218 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:34:52.219 - INFO: Train epoch 1686:   Loss: 806.1944 | r_Loss: 71.8815 | g_Loss: 385.0441 | l_Loss: 61.7429 | 
21-12-24 11:36:05.957 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:36:05.958 - INFO: Train epoch 1687:   Loss: 778.4672 | r_Loss: 71.2008 | g_Loss: 380.8677 | l_Loss: 41.5954 | 
21-12-24 11:37:19.420 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:37:19.421 - INFO: Train epoch 1688:   Loss: 750.4331 | r_Loss: 66.9524 | g_Loss: 376.2731 | l_Loss: 39.3979 | 
21-12-24 11:38:33.515 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:38:33.516 - INFO: Train epoch 1689:   Loss: 781.7109 | r_Loss: 72.6099 | g_Loss: 369.7764 | l_Loss: 48.8849 | 
21-12-24 11:39:46.912 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:39:46.913 - INFO: Train epoch 1690:   Loss: 772.5375 | r_Loss: 70.7663 | g_Loss: 372.8491 | l_Loss: 45.8568 | 
21-12-24 11:41:00.649 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:41:00.650 - INFO: Train epoch 1691:   Loss: 763.5094 | r_Loss: 70.2843 | g_Loss: 368.8476 | l_Loss: 43.2403 | 
21-12-24 11:42:14.152 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:42:14.153 - INFO: Train epoch 1692:   Loss: 769.3426 | r_Loss: 70.8585 | g_Loss: 371.8062 | l_Loss: 43.2441 | 
21-12-24 11:43:27.780 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:43:27.781 - INFO: Train epoch 1693:   Loss: 781.7902 | r_Loss: 74.1837 | g_Loss: 370.6968 | l_Loss: 40.1747 | 
21-12-24 11:44:41.724 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:44:41.725 - INFO: Train epoch 1694:   Loss: 789.3826 | r_Loss: 75.9377 | g_Loss: 359.0156 | l_Loss: 50.6786 | 
21-12-24 11:45:55.547 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:45:55.548 - INFO: Train epoch 1695:   Loss: 749.3144 | r_Loss: 69.1799 | g_Loss: 354.0228 | l_Loss: 49.3919 | 
21-12-24 11:47:08.998 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:47:08.999 - INFO: Train epoch 1696:   Loss: 753.8007 | r_Loss: 71.2242 | g_Loss: 344.8389 | l_Loss: 52.8406 | 
21-12-24 11:48:22.610 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:48:22.611 - INFO: Train epoch 1697:   Loss: 743.3431 | r_Loss: 70.1488 | g_Loss: 347.5078 | l_Loss: 45.0913 | 
21-12-24 11:49:36.235 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:49:36.236 - INFO: Train epoch 1698:   Loss: 719.0849 | r_Loss: 66.2507 | g_Loss: 346.8729 | l_Loss: 40.9583 | 
21-12-24 11:50:49.769 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:50:49.770 - INFO: Train epoch 1699:   Loss: 767.6972 | r_Loss: 74.3354 | g_Loss: 353.6101 | l_Loss: 42.4099 | 
21-12-24 11:52:03.341 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:52:03.342 - INFO: Train epoch 1700:   Loss: 719.5966 | r_Loss: 68.5536 | g_Loss: 336.9405 | l_Loss: 39.8880 | 
21-12-24 11:53:17.154 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:53:17.156 - INFO: Train epoch 1701:   Loss: 719.8267 | r_Loss: 68.8395 | g_Loss: 333.4468 | l_Loss: 42.1825 | 
21-12-24 11:54:30.797 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:54:30.799 - INFO: Train epoch 1702:   Loss: 743.6404 | r_Loss: 72.3179 | g_Loss: 337.3837 | l_Loss: 44.6671 | 
21-12-24 11:55:44.226 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:55:44.227 - INFO: Train epoch 1703:   Loss: 769.4671 | r_Loss: 76.2474 | g_Loss: 347.0613 | l_Loss: 41.1687 | 
21-12-24 11:56:57.361 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:56:57.362 - INFO: Train epoch 1704:   Loss: 747.3007 | r_Loss: 73.5217 | g_Loss: 341.2174 | l_Loss: 38.4746 | 
21-12-24 11:58:10.751 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:58:10.753 - INFO: Train epoch 1705:   Loss: 759.1546 | r_Loss: 74.5607 | g_Loss: 339.2166 | l_Loss: 47.1346 | 
21-12-24 11:59:24.179 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 11:59:24.180 - INFO: Train epoch 1706:   Loss: 710.6230 | r_Loss: 70.4662 | g_Loss: 319.5313 | l_Loss: 38.7606 | 
21-12-24 12:00:37.830 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:00:37.831 - INFO: Train epoch 1707:   Loss: 700.4298 | r_Loss: 70.4592 | g_Loss: 308.3489 | l_Loss: 39.7849 | 
21-12-24 12:01:51.071 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:01:51.072 - INFO: Train epoch 1708:   Loss: 666.6130 | r_Loss: 63.6407 | g_Loss: 311.5169 | l_Loss: 36.8924 | 
21-12-24 12:03:04.688 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:03:04.689 - INFO: Train epoch 1709:   Loss: 710.7510 | r_Loss: 72.6898 | g_Loss: 312.6813 | l_Loss: 34.6208 | 
21-12-24 12:04:54.376 - INFO: TEST:   PSNR_S: 44.5309 | PSNR_C: 36.7876 | 
21-12-24 12:04:54.378 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:04:54.379 - INFO: Train epoch 1710:   Loss: 748.2437 | r_Loss: 73.5660 | g_Loss: 332.4286 | l_Loss: 47.9853 | 
21-12-24 12:06:08.112 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:06:08.113 - INFO: Train epoch 1711:   Loss: 730.4399 | r_Loss: 73.9148 | g_Loss: 319.4427 | l_Loss: 41.4229 | 
21-12-24 12:07:21.838 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:07:21.839 - INFO: Train epoch 1712:   Loss: 706.9692 | r_Loss: 70.8856 | g_Loss: 314.8030 | l_Loss: 37.7382 | 
21-12-24 12:08:35.159 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:08:35.160 - INFO: Train epoch 1713:   Loss: 736.0849 | r_Loss: 75.6680 | g_Loss: 314.9232 | l_Loss: 42.8219 | 
21-12-24 12:09:48.651 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:09:48.652 - INFO: Train epoch 1714:   Loss: 711.8496 | r_Loss: 71.4041 | g_Loss: 314.3108 | l_Loss: 40.5185 | 
21-12-24 12:11:02.199 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:11:02.200 - INFO: Train epoch 1715:   Loss: 753.2928 | r_Loss: 77.4933 | g_Loss: 326.8601 | l_Loss: 38.9664 | 
21-12-24 12:12:15.538 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:12:15.539 - INFO: Train epoch 1716:   Loss: 734.1763 | r_Loss: 75.5589 | g_Loss: 316.3935 | l_Loss: 39.9884 | 
21-12-24 12:13:29.243 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:13:29.245 - INFO: Train epoch 1717:   Loss: 714.3771 | r_Loss: 73.4440 | g_Loss: 304.0243 | l_Loss: 43.1328 | 
21-12-24 12:14:42.505 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:14:42.505 - INFO: Train epoch 1718:   Loss: 714.9687 | r_Loss: 72.2714 | g_Loss: 310.6197 | l_Loss: 42.9923 | 
21-12-24 12:15:55.500 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:15:55.501 - INFO: Train epoch 1719:   Loss: 704.3244 | r_Loss: 71.2147 | g_Loss: 307.3500 | l_Loss: 40.9007 | 
21-12-24 12:17:09.363 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:17:09.365 - INFO: Train epoch 1720:   Loss: 691.3586 | r_Loss: 71.5271 | g_Loss: 294.6778 | l_Loss: 39.0454 | 
21-12-24 12:18:22.588 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:18:22.589 - INFO: Train epoch 1721:   Loss: 664.2191 | r_Loss: 67.4086 | g_Loss: 293.2737 | l_Loss: 33.9023 | 
21-12-24 12:19:36.656 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:19:36.657 - INFO: Train epoch 1722:   Loss: 657.1564 | r_Loss: 65.6682 | g_Loss: 288.2780 | l_Loss: 40.5373 | 
21-12-24 12:20:49.840 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:20:49.841 - INFO: Train epoch 1723:   Loss: 711.8719 | r_Loss: 75.0266 | g_Loss: 295.2638 | l_Loss: 41.4753 | 
21-12-24 12:22:03.414 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:22:03.415 - INFO: Train epoch 1724:   Loss: 705.0644 | r_Loss: 70.7484 | g_Loss: 311.1821 | l_Loss: 40.1403 | 
21-12-24 12:23:16.876 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:23:16.877 - INFO: Train epoch 1725:   Loss: 720.0282 | r_Loss: 74.0241 | g_Loss: 310.4715 | l_Loss: 39.4361 | 
21-12-24 12:24:30.434 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:24:30.435 - INFO: Train epoch 1726:   Loss: 732.0975 | r_Loss: 77.5719 | g_Loss: 305.1166 | l_Loss: 39.1216 | 
21-12-24 12:25:43.854 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:25:43.855 - INFO: Train epoch 1727:   Loss: 714.6404 | r_Loss: 74.9158 | g_Loss: 303.4786 | l_Loss: 36.5827 | 
21-12-24 12:26:57.341 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:26:57.343 - INFO: Train epoch 1728:   Loss: 710.4739 | r_Loss: 75.9522 | g_Loss: 300.6479 | l_Loss: 30.0649 | 
21-12-24 12:28:11.148 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:28:11.149 - INFO: Train epoch 1729:   Loss: 698.6187 | r_Loss: 71.9532 | g_Loss: 300.3817 | l_Loss: 38.4709 | 
21-12-24 12:29:24.938 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:29:24.939 - INFO: Train epoch 1730:   Loss: 666.1466 | r_Loss: 68.0425 | g_Loss: 290.4145 | l_Loss: 35.5197 | 
21-12-24 12:30:38.439 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:30:38.440 - INFO: Train epoch 1731:   Loss: 738.4115 | r_Loss: 79.9069 | g_Loss: 304.8984 | l_Loss: 33.9787 | 
21-12-24 12:31:52.032 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:31:52.033 - INFO: Train epoch 1732:   Loss: 715.5669 | r_Loss: 75.0963 | g_Loss: 303.7472 | l_Loss: 36.3380 | 
21-12-24 12:33:05.460 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:33:05.461 - INFO: Train epoch 1733:   Loss: 716.5168 | r_Loss: 74.1250 | g_Loss: 307.2458 | l_Loss: 38.6461 | 
21-12-24 12:34:19.710 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:34:19.711 - INFO: Train epoch 1734:   Loss: 755.9281 | r_Loss: 80.3550 | g_Loss: 316.5978 | l_Loss: 37.5553 | 
21-12-24 12:35:33.259 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:35:33.260 - INFO: Train epoch 1735:   Loss: 698.3875 | r_Loss: 76.0861 | g_Loss: 283.9956 | l_Loss: 33.9616 | 
21-12-24 12:36:46.572 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:36:46.573 - INFO: Train epoch 1736:   Loss: 695.3843 | r_Loss: 73.2075 | g_Loss: 298.3507 | l_Loss: 30.9962 | 
21-12-24 12:38:00.649 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:38:00.650 - INFO: Train epoch 1737:   Loss: 722.9559 | r_Loss: 76.7901 | g_Loss: 305.3573 | l_Loss: 33.6481 | 
21-12-24 12:39:14.767 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:39:14.768 - INFO: Train epoch 1738:   Loss: 750.9706 | r_Loss: 78.2312 | g_Loss: 315.2320 | l_Loss: 44.5824 | 
21-12-24 12:40:28.346 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:40:28.347 - INFO: Train epoch 1739:   Loss: 736.5281 | r_Loss: 78.5347 | g_Loss: 307.1150 | l_Loss: 36.7398 | 
21-12-24 12:42:17.876 - INFO: TEST:   PSNR_S: 44.5592 | PSNR_C: 37.1728 | 
21-12-24 12:42:17.877 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:42:17.878 - INFO: Train epoch 1740:   Loss: 688.5199 | r_Loss: 69.7688 | g_Loss: 304.6378 | l_Loss: 35.0379 | 
21-12-24 12:43:31.222 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:43:31.223 - INFO: Train epoch 1741:   Loss: 665.3084 | r_Loss: 68.0839 | g_Loss: 292.8388 | l_Loss: 32.0500 | 
21-12-24 12:44:44.551 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:44:44.552 - INFO: Train epoch 1742:   Loss: 735.3801 | r_Loss: 77.4265 | g_Loss: 310.8973 | l_Loss: 37.3504 | 
21-12-24 12:45:57.866 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:45:57.867 - INFO: Train epoch 1743:   Loss: 695.8742 | r_Loss: 70.2452 | g_Loss: 295.6354 | l_Loss: 49.0126 | 
21-12-24 12:47:11.589 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:47:11.591 - INFO: Train epoch 1744:   Loss: 691.0718 | r_Loss: 73.7173 | g_Loss: 289.4705 | l_Loss: 33.0146 | 
21-12-24 12:48:25.438 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:48:25.439 - INFO: Train epoch 1745:   Loss: 733.7554 | r_Loss: 78.5749 | g_Loss: 298.6014 | l_Loss: 42.2794 | 
21-12-24 12:49:39.087 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:49:39.088 - INFO: Train epoch 1746:   Loss: 683.7668 | r_Loss: 70.8411 | g_Loss: 295.1287 | l_Loss: 34.4324 | 
21-12-24 12:50:52.392 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:50:52.394 - INFO: Train epoch 1747:   Loss: 659.3999 | r_Loss: 66.7903 | g_Loss: 290.0912 | l_Loss: 35.3573 | 
21-12-24 12:52:05.981 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:52:05.982 - INFO: Train epoch 1748:   Loss: 681.8155 | r_Loss: 70.6287 | g_Loss: 291.1384 | l_Loss: 37.5336 | 
21-12-24 12:53:19.309 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:53:19.310 - INFO: Train epoch 1749:   Loss: 682.0511 | r_Loss: 69.2791 | g_Loss: 295.0000 | l_Loss: 40.6557 | 
21-12-24 12:54:32.975 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:54:32.976 - INFO: Train epoch 1750:   Loss: 694.0233 | r_Loss: 74.4357 | g_Loss: 290.2026 | l_Loss: 31.6420 | 
21-12-24 12:55:46.359 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:55:46.360 - INFO: Train epoch 1751:   Loss: 679.9877 | r_Loss: 71.4661 | g_Loss: 285.6241 | l_Loss: 37.0331 | 
21-12-24 12:57:00.474 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:57:00.475 - INFO: Train epoch 1752:   Loss: 658.5425 | r_Loss: 66.5151 | g_Loss: 291.9117 | l_Loss: 34.0555 | 
21-12-24 12:58:13.947 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:58:13.948 - INFO: Train epoch 1753:   Loss: 667.6711 | r_Loss: 68.3255 | g_Loss: 288.2551 | l_Loss: 37.7888 | 
21-12-24 12:59:27.901 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 12:59:27.902 - INFO: Train epoch 1754:   Loss: 698.7648 | r_Loss: 73.4291 | g_Loss: 295.5948 | l_Loss: 36.0244 | 
21-12-24 13:00:41.804 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:00:41.805 - INFO: Train epoch 1755:   Loss: 696.7584 | r_Loss: 73.2533 | g_Loss: 293.8157 | l_Loss: 36.6764 | 
21-12-24 13:01:55.604 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:01:55.606 - INFO: Train epoch 1756:   Loss: 725.7841 | r_Loss: 77.8175 | g_Loss: 296.2575 | l_Loss: 40.4394 | 
21-12-24 13:03:09.485 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:03:09.486 - INFO: Train epoch 1757:   Loss: 884.0669 | r_Loss: 104.0200 | g_Loss: 316.1741 | l_Loss: 47.7926 | 
21-12-24 13:04:22.981 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:04:22.982 - INFO: Train epoch 1758:   Loss: 694.3711 | r_Loss: 70.8976 | g_Loss: 298.5111 | l_Loss: 41.3721 | 
21-12-24 13:05:36.913 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:05:36.914 - INFO: Train epoch 1759:   Loss: 695.9676 | r_Loss: 69.2147 | g_Loss: 307.5922 | l_Loss: 42.3021 | 
21-12-24 13:06:50.393 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:06:50.394 - INFO: Train epoch 1760:   Loss: 712.6273 | r_Loss: 73.1807 | g_Loss: 310.1358 | l_Loss: 36.5878 | 
21-12-24 13:08:04.164 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:08:04.165 - INFO: Train epoch 1761:   Loss: 697.5223 | r_Loss: 73.8286 | g_Loss: 295.4237 | l_Loss: 32.9554 | 
21-12-24 13:09:17.993 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:09:17.994 - INFO: Train epoch 1762:   Loss: 674.5606 | r_Loss: 70.1752 | g_Loss: 289.6457 | l_Loss: 34.0391 | 
21-12-24 13:10:31.925 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:10:31.926 - INFO: Train epoch 1763:   Loss: 711.1937 | r_Loss: 76.1995 | g_Loss: 296.9209 | l_Loss: 33.2753 | 
21-12-24 13:11:45.527 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:11:45.528 - INFO: Train epoch 1764:   Loss: 688.7824 | r_Loss: 72.6844 | g_Loss: 290.7455 | l_Loss: 34.6148 | 
21-12-24 13:12:59.072 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:12:59.073 - INFO: Train epoch 1765:   Loss: 742.8691 | r_Loss: 78.6104 | g_Loss: 309.5635 | l_Loss: 40.2537 | 
21-12-24 13:14:12.598 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:14:12.599 - INFO: Train epoch 1766:   Loss: 712.0398 | r_Loss: 76.8489 | g_Loss: 291.0487 | l_Loss: 36.7467 | 
21-12-24 13:15:26.393 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:15:26.394 - INFO: Train epoch 1767:   Loss: 689.3794 | r_Loss: 69.6028 | g_Loss: 287.1737 | l_Loss: 54.1916 | 
21-12-24 13:16:39.835 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:16:39.836 - INFO: Train epoch 1768:   Loss: 691.9738 | r_Loss: 72.2257 | g_Loss: 287.1889 | l_Loss: 43.6561 | 
21-12-24 13:17:52.922 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:17:52.923 - INFO: Train epoch 1769:   Loss: 681.9947 | r_Loss: 73.5295 | g_Loss: 285.3682 | l_Loss: 28.9791 | 
21-12-24 13:19:42.987 - INFO: TEST:   PSNR_S: 44.6176 | PSNR_C: 37.4177 | 
21-12-24 13:19:42.989 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:19:42.990 - INFO: Train epoch 1770:   Loss: 730.8154 | r_Loss: 77.5737 | g_Loss: 298.4191 | l_Loss: 44.5278 | 
21-12-24 13:20:56.877 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:20:56.878 - INFO: Train epoch 1771:   Loss: 692.0084 | r_Loss: 75.0203 | g_Loss: 285.2535 | l_Loss: 31.6536 | 
21-12-24 13:22:10.851 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:22:10.852 - INFO: Train epoch 1772:   Loss: 725.6709 | r_Loss: 77.1570 | g_Loss: 303.5220 | l_Loss: 36.3639 | 
21-12-24 13:23:24.636 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:23:24.637 - INFO: Train epoch 1773:   Loss: 764.1123 | r_Loss: 76.6675 | g_Loss: 340.7704 | l_Loss: 40.0043 | 
21-12-24 13:24:38.281 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:24:38.282 - INFO: Train epoch 1774:   Loss: 659.8171 | r_Loss: 67.0161 | g_Loss: 289.4995 | l_Loss: 35.2372 | 
21-12-24 13:25:51.553 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:25:51.554 - INFO: Train epoch 1775:   Loss: 676.6983 | r_Loss: 69.6707 | g_Loss: 286.0884 | l_Loss: 42.2561 | 
21-12-24 13:27:04.840 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:27:04.841 - INFO: Train epoch 1776:   Loss: 658.6153 | r_Loss: 68.2647 | g_Loss: 281.1455 | l_Loss: 36.1463 | 
21-12-24 13:28:18.394 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:28:18.395 - INFO: Train epoch 1777:   Loss: 617.6532 | r_Loss: 61.3925 | g_Loss: 272.6765 | l_Loss: 38.0141 | 
21-12-24 13:29:31.964 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:29:31.965 - INFO: Train epoch 1778:   Loss: 699.6648 | r_Loss: 73.1363 | g_Loss: 296.0462 | l_Loss: 37.9370 | 
21-12-24 13:30:45.685 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:30:45.686 - INFO: Train epoch 1779:   Loss: 645.1173 | r_Loss: 68.0590 | g_Loss: 274.8078 | l_Loss: 30.0147 | 
21-12-24 13:31:59.456 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:31:59.457 - INFO: Train epoch 1780:   Loss: 736.3907 | r_Loss: 85.5363 | g_Loss: 275.2029 | l_Loss: 33.5065 | 
21-12-24 13:33:12.936 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:33:12.937 - INFO: Train epoch 1781:   Loss: 27199.5270 | r_Loss: 4951.1001 | g_Loss: 2165.3570 | l_Loss: 278.6699 | 
21-12-24 13:34:26.689 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:34:26.690 - INFO: Train epoch 1782:   Loss: 1888.3177 | r_Loss: 178.5296 | g_Loss: 882.1664 | l_Loss: 113.5032 | 
21-12-24 13:35:40.244 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:35:40.245 - INFO: Train epoch 1783:   Loss: 1413.8933 | r_Loss: 129.4908 | g_Loss: 688.0817 | l_Loss: 78.3574 | 
21-12-24 13:36:53.641 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:36:53.642 - INFO: Train epoch 1784:   Loss: 1389.9506 | r_Loss: 128.6146 | g_Loss: 663.9321 | l_Loss: 82.9453 | 
21-12-24 13:38:07.355 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:38:07.357 - INFO: Train epoch 1785:   Loss: 1283.1526 | r_Loss: 118.0200 | g_Loss: 616.7083 | l_Loss: 76.3441 | 
21-12-24 13:39:21.441 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:39:21.442 - INFO: Train epoch 1786:   Loss: 1148.8036 | r_Loss: 102.8540 | g_Loss: 566.2212 | l_Loss: 68.3123 | 
21-12-24 13:40:35.080 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:40:35.081 - INFO: Train epoch 1787:   Loss: 1195.0455 | r_Loss: 106.0786 | g_Loss: 600.8800 | l_Loss: 63.7727 | 
21-12-24 13:41:48.641 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:41:48.642 - INFO: Train epoch 1788:   Loss: 1090.6651 | r_Loss: 95.9577 | g_Loss: 544.1177 | l_Loss: 66.7587 | 
21-12-24 13:43:01.827 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:43:01.828 - INFO: Train epoch 1789:   Loss: 1026.4087 | r_Loss: 94.0206 | g_Loss: 500.7070 | l_Loss: 55.5987 | 
21-12-24 13:44:14.952 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:44:14.953 - INFO: Train epoch 1790:   Loss: 997.3083 | r_Loss: 90.9614 | g_Loss: 484.6996 | l_Loss: 57.8017 | 
21-12-24 13:45:28.664 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:45:28.665 - INFO: Train epoch 1791:   Loss: 975.2179 | r_Loss: 86.8784 | g_Loss: 488.3203 | l_Loss: 52.5054 | 
21-12-24 13:46:42.290 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:46:42.291 - INFO: Train epoch 1792:   Loss: 962.6513 | r_Loss: 84.0564 | g_Loss: 492.5444 | l_Loss: 49.8250 | 
21-12-24 13:47:55.914 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:47:55.915 - INFO: Train epoch 1793:   Loss: 970.4572 | r_Loss: 88.2591 | g_Loss: 470.4195 | l_Loss: 58.7422 | 
21-12-24 13:49:09.677 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:49:09.678 - INFO: Train epoch 1794:   Loss: 931.1605 | r_Loss: 83.7026 | g_Loss: 458.2896 | l_Loss: 54.3581 | 
21-12-24 13:50:23.000 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:50:23.001 - INFO: Train epoch 1795:   Loss: 954.0510 | r_Loss: 86.8411 | g_Loss: 458.6640 | l_Loss: 61.1813 | 
21-12-24 13:51:36.615 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:51:36.616 - INFO: Train epoch 1796:   Loss: 900.0978 | r_Loss: 81.6517 | g_Loss: 435.6577 | l_Loss: 56.1816 | 
21-12-24 13:52:50.309 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:52:50.310 - INFO: Train epoch 1797:   Loss: 891.5913 | r_Loss: 81.1589 | g_Loss: 432.3371 | l_Loss: 53.4599 | 
21-12-24 13:54:04.073 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:54:04.074 - INFO: Train epoch 1798:   Loss: 851.1819 | r_Loss: 74.6060 | g_Loss: 428.3256 | l_Loss: 49.8262 | 
21-12-24 13:55:17.724 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:55:17.725 - INFO: Train epoch 1799:   Loss: 844.9514 | r_Loss: 75.8346 | g_Loss: 416.5922 | l_Loss: 49.1864 | 
21-12-24 13:57:07.039 - INFO: TEST:   PSNR_S: 43.9725 | PSNR_C: 35.5244 | 
21-12-24 13:57:07.040 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:57:07.040 - INFO: Train epoch 1800:   Loss: 836.3367 | r_Loss: 77.4354 | g_Loss: 405.5109 | l_Loss: 43.6486 | 
21-12-24 13:58:20.329 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:58:20.331 - INFO: Train epoch 1801:   Loss: 805.8711 | r_Loss: 70.8932 | g_Loss: 402.9265 | l_Loss: 48.4787 | 
21-12-24 13:59:34.663 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 13:59:34.664 - INFO: Train epoch 1802:   Loss: 820.0081 | r_Loss: 72.4727 | g_Loss: 404.8849 | l_Loss: 52.7595 | 
21-12-24 14:00:47.701 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:00:47.702 - INFO: Train epoch 1803:   Loss: 777.7857 | r_Loss: 69.2405 | g_Loss: 379.6074 | l_Loss: 51.9759 | 
21-12-24 14:02:01.086 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:02:01.087 - INFO: Train epoch 1804:   Loss: 771.2026 | r_Loss: 67.6609 | g_Loss: 377.1543 | l_Loss: 55.7439 | 
21-12-24 14:03:14.531 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:03:14.532 - INFO: Train epoch 1805:   Loss: 808.4646 | r_Loss: 74.1017 | g_Loss: 382.9217 | l_Loss: 55.0345 | 
21-12-24 14:04:28.040 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:04:28.041 - INFO: Train epoch 1806:   Loss: 752.9266 | r_Loss: 66.2621 | g_Loss: 373.4613 | l_Loss: 48.1547 | 
21-12-24 14:05:41.439 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:05:41.440 - INFO: Train epoch 1807:   Loss: 771.5367 | r_Loss: 70.2679 | g_Loss: 370.3910 | l_Loss: 49.8061 | 
21-12-24 14:06:54.644 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:06:54.645 - INFO: Train epoch 1808:   Loss: 786.3187 | r_Loss: 71.5571 | g_Loss: 383.4627 | l_Loss: 45.0707 | 
21-12-24 14:08:08.253 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:08:08.254 - INFO: Train epoch 1809:   Loss: 804.5923 | r_Loss: 73.8412 | g_Loss: 387.4913 | l_Loss: 47.8951 | 
21-12-24 14:09:21.810 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:09:21.811 - INFO: Train epoch 1810:   Loss: 798.1764 | r_Loss: 75.3360 | g_Loss: 375.2541 | l_Loss: 46.2421 | 
21-12-24 14:10:35.363 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:10:35.364 - INFO: Train epoch 1811:   Loss: 775.0778 | r_Loss: 69.6894 | g_Loss: 371.9672 | l_Loss: 54.6635 | 
21-12-24 14:11:49.217 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:11:49.218 - INFO: Train epoch 1812:   Loss: 752.2019 | r_Loss: 70.8881 | g_Loss: 353.6663 | l_Loss: 44.0949 | 
21-12-24 14:13:02.676 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:13:02.677 - INFO: Train epoch 1813:   Loss: 748.0589 | r_Loss: 67.3545 | g_Loss: 366.3562 | l_Loss: 44.9300 | 
21-12-24 14:14:16.495 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:14:16.496 - INFO: Train epoch 1814:   Loss: 717.4607 | r_Loss: 63.5924 | g_Loss: 359.1337 | l_Loss: 40.3648 | 
21-12-24 14:15:30.137 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:15:30.138 - INFO: Train epoch 1815:   Loss: 706.5849 | r_Loss: 63.7316 | g_Loss: 342.7885 | l_Loss: 45.1386 | 
21-12-24 14:16:43.558 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:16:43.559 - INFO: Train epoch 1816:   Loss: 714.4677 | r_Loss: 63.0740 | g_Loss: 352.6346 | l_Loss: 46.4630 | 
21-12-24 14:17:56.758 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:17:56.759 - INFO: Train epoch 1817:   Loss: 755.3819 | r_Loss: 70.8035 | g_Loss: 356.0815 | l_Loss: 45.2829 | 
21-12-24 14:19:10.791 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:19:10.792 - INFO: Train epoch 1818:   Loss: 718.0255 | r_Loss: 65.1656 | g_Loss: 343.7945 | l_Loss: 48.4029 | 
21-12-24 14:20:24.294 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:20:24.296 - INFO: Train epoch 1819:   Loss: 737.0700 | r_Loss: 68.8999 | g_Loss: 352.0209 | l_Loss: 40.5498 | 
21-12-24 14:21:38.014 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:21:38.015 - INFO: Train epoch 1820:   Loss: 742.7412 | r_Loss: 70.2665 | g_Loss: 347.9807 | l_Loss: 43.4283 | 
21-12-24 14:22:51.623 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:22:51.624 - INFO: Train epoch 1821:   Loss: 679.8593 | r_Loss: 60.2650 | g_Loss: 336.2007 | l_Loss: 42.3334 | 
21-12-24 14:24:04.937 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:24:04.938 - INFO: Train epoch 1822:   Loss: 720.1367 | r_Loss: 68.2421 | g_Loss: 342.1020 | l_Loss: 36.8243 | 
21-12-24 14:25:18.388 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:25:18.389 - INFO: Train epoch 1823:   Loss: 693.1084 | r_Loss: 63.7521 | g_Loss: 332.6333 | l_Loss: 41.7147 | 
21-12-24 14:26:32.182 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:26:32.182 - INFO: Train epoch 1824:   Loss: 686.0146 | r_Loss: 64.0352 | g_Loss: 329.2265 | l_Loss: 36.6120 | 
21-12-24 14:27:46.192 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:27:46.193 - INFO: Train epoch 1825:   Loss: 688.8245 | r_Loss: 62.8375 | g_Loss: 331.6962 | l_Loss: 42.9406 | 
21-12-24 14:28:59.633 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:28:59.634 - INFO: Train epoch 1826:   Loss: 711.7002 | r_Loss: 70.4528 | g_Loss: 324.2055 | l_Loss: 35.2307 | 
21-12-24 14:30:13.167 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:30:13.168 - INFO: Train epoch 1827:   Loss: 673.0654 | r_Loss: 60.8528 | g_Loss: 332.2980 | l_Loss: 36.5034 | 
21-12-24 14:31:26.771 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:31:26.772 - INFO: Train epoch 1828:   Loss: 672.9244 | r_Loss: 64.6316 | g_Loss: 310.8706 | l_Loss: 38.8958 | 
21-12-24 14:32:40.221 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:32:40.222 - INFO: Train epoch 1829:   Loss: 682.3891 | r_Loss: 64.7451 | g_Loss: 326.1835 | l_Loss: 32.4800 | 
21-12-24 14:34:29.696 - INFO: TEST:   PSNR_S: 44.7531 | PSNR_C: 36.7158 | 
21-12-24 14:34:29.697 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:34:29.698 - INFO: Train epoch 1830:   Loss: 713.1444 | r_Loss: 68.6513 | g_Loss: 331.5930 | l_Loss: 38.2950 | 
21-12-24 14:35:43.375 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:35:43.376 - INFO: Train epoch 1831:   Loss: 674.9687 | r_Loss: 64.8393 | g_Loss: 312.9078 | l_Loss: 37.8644 | 
21-12-24 14:36:56.907 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:36:56.908 - INFO: Train epoch 1832:   Loss: 729.9047 | r_Loss: 67.8977 | g_Loss: 332.4869 | l_Loss: 57.9293 | 
21-12-24 14:38:10.569 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:38:10.570 - INFO: Train epoch 1833:   Loss: 673.7224 | r_Loss: 62.5087 | g_Loss: 314.4841 | l_Loss: 46.6946 | 
21-12-24 14:39:24.160 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:39:24.161 - INFO: Train epoch 1834:   Loss: 702.8203 | r_Loss: 68.0163 | g_Loss: 322.8687 | l_Loss: 39.8700 | 
21-12-24 14:40:37.556 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:40:37.557 - INFO: Train epoch 1835:   Loss: 681.4264 | r_Loss: 64.5655 | g_Loss: 319.8888 | l_Loss: 38.7100 | 
21-12-24 14:41:50.818 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:41:50.819 - INFO: Train epoch 1836:   Loss: 704.8515 | r_Loss: 67.5858 | g_Loss: 328.6246 | l_Loss: 38.2980 | 
21-12-24 14:43:04.405 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:43:04.406 - INFO: Train epoch 1837:   Loss: 688.5916 | r_Loss: 65.6036 | g_Loss: 315.1386 | l_Loss: 45.4349 | 
21-12-24 14:44:17.843 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:44:17.844 - INFO: Train epoch 1838:   Loss: 666.2931 | r_Loss: 64.6960 | g_Loss: 300.1363 | l_Loss: 42.6768 | 
21-12-24 14:45:31.440 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:45:31.441 - INFO: Train epoch 1839:   Loss: 684.7102 | r_Loss: 66.1549 | g_Loss: 314.6221 | l_Loss: 39.3138 | 
21-12-24 14:46:45.028 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:46:45.029 - INFO: Train epoch 1840:   Loss: 682.8615 | r_Loss: 66.8284 | g_Loss: 307.8528 | l_Loss: 40.8665 | 
21-12-24 14:47:58.424 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:47:58.425 - INFO: Train epoch 1841:   Loss: 674.7829 | r_Loss: 64.2279 | g_Loss: 313.0539 | l_Loss: 40.5897 | 
21-12-24 14:49:12.179 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:49:12.180 - INFO: Train epoch 1842:   Loss: 705.4461 | r_Loss: 69.1956 | g_Loss: 318.0932 | l_Loss: 41.3752 | 
21-12-24 14:50:26.334 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:50:26.335 - INFO: Train epoch 1843:   Loss: 658.2322 | r_Loss: 64.3833 | g_Loss: 303.1554 | l_Loss: 33.1605 | 
21-12-24 14:51:39.936 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:51:39.937 - INFO: Train epoch 1844:   Loss: 709.8064 | r_Loss: 74.3519 | g_Loss: 302.2732 | l_Loss: 35.7735 | 
21-12-24 14:52:53.280 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:52:53.282 - INFO: Train epoch 1845:   Loss: 664.4267 | r_Loss: 63.9445 | g_Loss: 302.0227 | l_Loss: 42.6813 | 
21-12-24 14:54:07.087 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:54:07.088 - INFO: Train epoch 1846:   Loss: 682.1295 | r_Loss: 66.1928 | g_Loss: 307.7126 | l_Loss: 43.4530 | 
21-12-24 14:55:20.602 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:55:20.603 - INFO: Train epoch 1847:   Loss: 679.1990 | r_Loss: 69.0970 | g_Loss: 295.2008 | l_Loss: 38.5129 | 
21-12-24 14:56:34.185 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:56:34.186 - INFO: Train epoch 1848:   Loss: 670.6749 | r_Loss: 66.4798 | g_Loss: 302.5884 | l_Loss: 35.6876 | 
21-12-24 14:57:48.070 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:57:48.072 - INFO: Train epoch 1849:   Loss: 698.5283 | r_Loss: 69.9021 | g_Loss: 301.7318 | l_Loss: 47.2860 | 
21-12-24 14:59:01.995 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 14:59:01.996 - INFO: Train epoch 1850:   Loss: 672.0781 | r_Loss: 66.1542 | g_Loss: 301.5685 | l_Loss: 39.7388 | 
21-12-24 15:00:15.202 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:00:15.203 - INFO: Train epoch 1851:   Loss: 682.8503 | r_Loss: 68.8609 | g_Loss: 303.9424 | l_Loss: 34.6033 | 
21-12-24 15:01:28.748 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:01:28.748 - INFO: Train epoch 1852:   Loss: 681.8842 | r_Loss: 68.4287 | g_Loss: 303.4624 | l_Loss: 36.2782 | 
21-12-24 15:02:42.278 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:02:42.279 - INFO: Train epoch 1853:   Loss: 657.8965 | r_Loss: 66.5098 | g_Loss: 291.5939 | l_Loss: 33.7534 | 
21-12-24 15:03:56.072 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:03:56.073 - INFO: Train epoch 1854:   Loss: 633.9719 | r_Loss: 60.7079 | g_Loss: 295.9366 | l_Loss: 34.4956 | 
21-12-24 15:05:10.000 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:05:10.001 - INFO: Train epoch 1855:   Loss: 625.3379 | r_Loss: 61.9797 | g_Loss: 279.1265 | l_Loss: 36.3129 | 
21-12-24 15:06:23.193 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:06:23.195 - INFO: Train epoch 1856:   Loss: 666.8640 | r_Loss: 70.0536 | g_Loss: 282.9565 | l_Loss: 33.6396 | 
21-12-24 15:07:36.858 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:07:36.859 - INFO: Train epoch 1857:   Loss: 725.2017 | r_Loss: 75.1786 | g_Loss: 306.9707 | l_Loss: 42.3381 | 
21-12-24 15:08:50.631 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:08:50.632 - INFO: Train epoch 1858:   Loss: 630.8458 | r_Loss: 63.8043 | g_Loss: 276.4533 | l_Loss: 35.3708 | 
21-12-24 15:10:04.172 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:10:04.173 - INFO: Train epoch 1859:   Loss: 653.6839 | r_Loss: 66.5865 | g_Loss: 285.3445 | l_Loss: 35.4069 | 
21-12-24 15:11:54.014 - INFO: TEST:   PSNR_S: 45.0200 | PSNR_C: 37.3160 | 
21-12-24 15:11:54.015 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:11:54.015 - INFO: Train epoch 1860:   Loss: 675.3478 | r_Loss: 69.4280 | g_Loss: 286.3882 | l_Loss: 41.8196 | 
21-12-24 15:13:07.377 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:13:07.378 - INFO: Train epoch 1861:   Loss: 735.9340 | r_Loss: 73.5292 | g_Loss: 324.1120 | l_Loss: 44.1758 | 
21-12-24 15:14:21.002 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:14:21.003 - INFO: Train epoch 1862:   Loss: 644.5409 | r_Loss: 65.6331 | g_Loss: 281.6457 | l_Loss: 34.7298 | 
21-12-24 15:15:34.572 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:15:34.573 - INFO: Train epoch 1863:   Loss: 678.3646 | r_Loss: 69.0200 | g_Loss: 294.2955 | l_Loss: 38.9690 | 
21-12-24 15:16:47.936 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:16:47.937 - INFO: Train epoch 1864:   Loss: 643.1459 | r_Loss: 65.5357 | g_Loss: 277.2762 | l_Loss: 38.1911 | 
21-12-24 15:18:01.557 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:18:01.558 - INFO: Train epoch 1865:   Loss: 666.7288 | r_Loss: 70.3506 | g_Loss: 275.4242 | l_Loss: 39.5514 | 
21-12-24 15:19:15.906 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:19:15.907 - INFO: Train epoch 1866:   Loss: 705.0768 | r_Loss: 74.8824 | g_Loss: 301.8500 | l_Loss: 28.8151 | 
21-12-24 15:20:29.795 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:20:29.796 - INFO: Train epoch 1867:   Loss: 605.0372 | r_Loss: 57.4753 | g_Loss: 271.8301 | l_Loss: 45.8307 | 
21-12-24 15:21:43.137 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:21:43.138 - INFO: Train epoch 1868:   Loss: 670.4567 | r_Loss: 68.6070 | g_Loss: 295.8435 | l_Loss: 31.5782 | 
21-12-24 15:22:56.951 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:22:56.952 - INFO: Train epoch 1869:   Loss: 667.2033 | r_Loss: 68.8726 | g_Loss: 285.9217 | l_Loss: 36.9188 | 
21-12-24 15:24:10.512 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:24:10.513 - INFO: Train epoch 1870:   Loss: 607.8520 | r_Loss: 60.5080 | g_Loss: 269.0332 | l_Loss: 36.2788 | 
21-12-24 15:25:24.101 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:25:24.103 - INFO: Train epoch 1871:   Loss: 646.1062 | r_Loss: 67.4696 | g_Loss: 270.2080 | l_Loss: 38.5501 | 
21-12-24 15:26:37.607 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:26:37.608 - INFO: Train epoch 1872:   Loss: 632.7603 | r_Loss: 65.2664 | g_Loss: 268.3516 | l_Loss: 38.0768 | 
21-12-24 15:27:50.678 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:27:50.680 - INFO: Train epoch 1873:   Loss: 635.1568 | r_Loss: 64.3990 | g_Loss: 275.7519 | l_Loss: 37.4102 | 
21-12-24 15:29:04.465 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:29:04.466 - INFO: Train epoch 1874:   Loss: 691.5381 | r_Loss: 76.1473 | g_Loss: 278.2882 | l_Loss: 32.5134 | 
21-12-24 15:30:18.315 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:30:18.316 - INFO: Train epoch 1875:   Loss: 645.2537 | r_Loss: 64.4660 | g_Loss: 281.3924 | l_Loss: 41.5316 | 
21-12-24 15:31:31.939 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:31:31.940 - INFO: Train epoch 1876:   Loss: 682.9236 | r_Loss: 73.2017 | g_Loss: 286.0000 | l_Loss: 30.9152 | 
21-12-24 15:32:45.637 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:32:45.638 - INFO: Train epoch 1877:   Loss: 697.0865 | r_Loss: 74.1529 | g_Loss: 290.5709 | l_Loss: 35.7511 | 
21-12-24 15:33:59.194 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:33:59.195 - INFO: Train epoch 1878:   Loss: 643.6496 | r_Loss: 65.3598 | g_Loss: 281.4079 | l_Loss: 35.4425 | 
21-12-24 15:35:12.681 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:35:12.682 - INFO: Train epoch 1879:   Loss: 627.7748 | r_Loss: 63.3863 | g_Loss: 273.2666 | l_Loss: 37.5768 | 
21-12-24 15:36:26.642 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:36:26.644 - INFO: Train epoch 1880:   Loss: 676.9040 | r_Loss: 71.7531 | g_Loss: 282.0461 | l_Loss: 36.0926 | 
21-12-24 15:37:40.369 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:37:40.370 - INFO: Train epoch 1881:   Loss: 697.5580 | r_Loss: 74.1386 | g_Loss: 288.5074 | l_Loss: 38.3574 | 
21-12-24 15:38:54.024 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:38:54.025 - INFO: Train epoch 1882:   Loss: 678.3468 | r_Loss: 76.7319 | g_Loss: 260.3516 | l_Loss: 34.3355 | 
21-12-24 15:40:07.595 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:40:07.596 - INFO: Train epoch 1883:   Loss: 598.1925 | r_Loss: 59.2218 | g_Loss: 266.1003 | l_Loss: 35.9834 | 
21-12-24 15:41:20.961 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:41:20.962 - INFO: Train epoch 1884:   Loss: 638.1156 | r_Loss: 67.0024 | g_Loss: 272.1458 | l_Loss: 30.9578 | 
21-12-24 15:42:34.515 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:42:34.516 - INFO: Train epoch 1885:   Loss: 586.9661 | r_Loss: 59.0618 | g_Loss: 260.7465 | l_Loss: 30.9108 | 
21-12-24 15:43:48.069 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:43:48.070 - INFO: Train epoch 1886:   Loss: 621.7998 | r_Loss: 63.6335 | g_Loss: 269.0621 | l_Loss: 34.5701 | 
21-12-24 15:45:01.901 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:45:01.902 - INFO: Train epoch 1887:   Loss: 632.1954 | r_Loss: 63.2525 | g_Loss: 279.8214 | l_Loss: 36.1113 | 
21-12-24 15:46:15.501 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:46:15.502 - INFO: Train epoch 1888:   Loss: 658.1341 | r_Loss: 65.4487 | g_Loss: 285.3926 | l_Loss: 45.4980 | 
21-12-24 15:47:29.234 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:47:29.234 - INFO: Train epoch 1889:   Loss: 909.0179 | r_Loss: 115.5207 | g_Loss: 290.2729 | l_Loss: 41.1415 | 
21-12-24 15:49:19.279 - INFO: TEST:   PSNR_S: 40.8152 | PSNR_C: 34.4156 | 
21-12-24 15:49:19.281 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:49:19.281 - INFO: Train epoch 1890:   Loss: 11961.4131 | r_Loss: 2206.1668 | g_Loss: 837.1147 | l_Loss: 93.4647 | 
21-12-24 15:50:32.795 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:50:32.796 - INFO: Train epoch 1891:   Loss: 1056.5178 | r_Loss: 105.6675 | g_Loss: 472.8878 | l_Loss: 55.2924 | 
21-12-24 15:51:45.912 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:51:45.913 - INFO: Train epoch 1892:   Loss: 937.6495 | r_Loss: 86.5954 | g_Loss: 453.7878 | l_Loss: 50.8846 | 
21-12-24 15:52:59.979 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:52:59.980 - INFO: Train epoch 1893:   Loss: 892.4382 | r_Loss: 80.5243 | g_Loss: 431.7219 | l_Loss: 58.0949 | 
21-12-24 15:54:13.609 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:54:13.610 - INFO: Train epoch 1894:   Loss: 820.0113 | r_Loss: 73.0635 | g_Loss: 401.6995 | l_Loss: 52.9941 | 
21-12-24 15:55:27.435 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:55:27.435 - INFO: Train epoch 1895:   Loss: 856.1665 | r_Loss: 76.1337 | g_Loss: 424.6960 | l_Loss: 50.8020 | 
21-12-24 15:56:41.103 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:56:41.104 - INFO: Train epoch 1896:   Loss: 794.8029 | r_Loss: 68.6865 | g_Loss: 400.6130 | l_Loss: 50.7576 | 
21-12-24 15:57:54.452 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:57:54.453 - INFO: Train epoch 1897:   Loss: 754.6464 | r_Loss: 66.9735 | g_Loss: 380.2996 | l_Loss: 39.4793 | 
21-12-24 15:59:08.282 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 15:59:08.284 - INFO: Train epoch 1898:   Loss: 777.1012 | r_Loss: 68.3311 | g_Loss: 395.6892 | l_Loss: 39.7566 | 
21-12-24 16:00:22.361 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:00:22.361 - INFO: Train epoch 1899:   Loss: 713.2783 | r_Loss: 59.8727 | g_Loss: 366.4051 | l_Loss: 47.5099 | 
21-12-24 16:01:35.782 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:01:35.783 - INFO: Train epoch 1900:   Loss: 761.8036 | r_Loss: 69.1024 | g_Loss: 374.0197 | l_Loss: 42.2716 | 
21-12-24 16:02:49.407 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:02:49.408 - INFO: Train epoch 1901:   Loss: 722.4671 | r_Loss: 64.3758 | g_Loss: 356.5424 | l_Loss: 44.0456 | 
21-12-24 16:04:02.702 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:04:02.703 - INFO: Train epoch 1902:   Loss: 709.2062 | r_Loss: 61.4920 | g_Loss: 361.7559 | l_Loss: 39.9904 | 
21-12-24 16:05:15.947 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:05:15.948 - INFO: Train epoch 1903:   Loss: 702.7438 | r_Loss: 63.0182 | g_Loss: 349.6163 | l_Loss: 38.0367 | 
21-12-24 16:06:29.683 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:06:29.684 - INFO: Train epoch 1904:   Loss: 720.2896 | r_Loss: 64.3664 | g_Loss: 350.9877 | l_Loss: 47.4701 | 
21-12-24 16:07:43.428 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:07:43.429 - INFO: Train epoch 1905:   Loss: 738.7760 | r_Loss: 66.3340 | g_Loss: 359.4850 | l_Loss: 47.6209 | 
21-12-24 16:08:56.876 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:08:56.877 - INFO: Train epoch 1906:   Loss: 724.6545 | r_Loss: 66.7975 | g_Loss: 346.2669 | l_Loss: 44.4001 | 
21-12-24 16:10:10.565 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:10:10.566 - INFO: Train epoch 1907:   Loss: 662.3915 | r_Loss: 59.9612 | g_Loss: 324.4650 | l_Loss: 38.1203 | 
21-12-24 16:11:24.295 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:11:24.296 - INFO: Train epoch 1908:   Loss: 711.5876 | r_Loss: 64.3935 | g_Loss: 346.6104 | l_Loss: 43.0099 | 
21-12-24 16:12:37.761 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:12:37.763 - INFO: Train epoch 1909:   Loss: 671.8348 | r_Loss: 60.9112 | g_Loss: 325.7905 | l_Loss: 41.4885 | 
21-12-24 16:13:51.279 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:13:51.280 - INFO: Train epoch 1910:   Loss: 679.2678 | r_Loss: 61.1218 | g_Loss: 329.8926 | l_Loss: 43.7663 | 
21-12-24 16:15:04.835 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:15:04.836 - INFO: Train epoch 1911:   Loss: 703.2545 | r_Loss: 63.8917 | g_Loss: 341.1247 | l_Loss: 42.6712 | 
21-12-24 16:16:18.379 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:16:18.380 - INFO: Train epoch 1912:   Loss: 685.0896 | r_Loss: 63.7026 | g_Loss: 321.7318 | l_Loss: 44.8450 | 
21-12-24 16:17:32.177 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:17:32.179 - INFO: Train epoch 1913:   Loss: 688.0055 | r_Loss: 62.2110 | g_Loss: 337.6132 | l_Loss: 39.3373 | 
21-12-24 16:18:46.178 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:18:46.180 - INFO: Train epoch 1914:   Loss: 658.8671 | r_Loss: 60.6661 | g_Loss: 313.2766 | l_Loss: 42.2601 | 
21-12-24 16:19:59.918 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:19:59.919 - INFO: Train epoch 1915:   Loss: 632.3056 | r_Loss: 56.3226 | g_Loss: 310.9987 | l_Loss: 39.6941 | 
21-12-24 16:21:13.566 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:21:13.568 - INFO: Train epoch 1916:   Loss: 637.0151 | r_Loss: 60.5904 | g_Loss: 296.2315 | l_Loss: 37.8315 | 
21-12-24 16:22:27.256 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:22:27.257 - INFO: Train epoch 1917:   Loss: 625.8112 | r_Loss: 59.2367 | g_Loss: 293.0478 | l_Loss: 36.5800 | 
21-12-24 16:23:40.931 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:23:40.932 - INFO: Train epoch 1918:   Loss: 624.0560 | r_Loss: 58.8510 | g_Loss: 288.7823 | l_Loss: 41.0186 | 
21-12-24 16:24:55.134 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:24:55.135 - INFO: Train epoch 1919:   Loss: 679.4857 | r_Loss: 66.6878 | g_Loss: 305.2384 | l_Loss: 40.8083 | 
21-12-24 16:26:45.192 - INFO: TEST:   PSNR_S: 45.3411 | PSNR_C: 37.1076 | 
21-12-24 16:26:45.193 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:26:45.194 - INFO: Train epoch 1920:   Loss: 636.0305 | r_Loss: 58.8598 | g_Loss: 299.1346 | l_Loss: 42.5970 | 
21-12-24 16:27:58.768 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:27:58.769 - INFO: Train epoch 1921:   Loss: 650.4100 | r_Loss: 60.3835 | g_Loss: 307.8647 | l_Loss: 40.6280 | 
21-12-24 16:29:12.412 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:29:12.414 - INFO: Train epoch 1922:   Loss: 658.5720 | r_Loss: 62.3796 | g_Loss: 305.2417 | l_Loss: 41.4323 | 
21-12-24 16:30:26.234 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:30:26.235 - INFO: Train epoch 1923:   Loss: 646.2142 | r_Loss: 62.8893 | g_Loss: 296.9604 | l_Loss: 34.8074 | 
21-12-24 16:31:39.850 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:31:39.851 - INFO: Train epoch 1924:   Loss: 677.0013 | r_Loss: 64.8470 | g_Loss: 310.8356 | l_Loss: 41.9306 | 
21-12-24 16:32:53.311 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:32:53.312 - INFO: Train epoch 1925:   Loss: 635.3908 | r_Loss: 60.9040 | g_Loss: 297.3460 | l_Loss: 33.5247 | 
21-12-24 16:34:07.179 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:34:07.180 - INFO: Train epoch 1926:   Loss: 616.5689 | r_Loss: 58.4606 | g_Loss: 274.0732 | l_Loss: 50.1926 | 
21-12-24 16:35:20.906 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:35:20.908 - INFO: Train epoch 1927:   Loss: 632.1898 | r_Loss: 64.2374 | g_Loss: 283.7478 | l_Loss: 27.2549 | 
21-12-24 16:36:35.009 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:36:35.010 - INFO: Train epoch 1928:   Loss: 598.7088 | r_Loss: 57.3555 | g_Loss: 274.9868 | l_Loss: 36.9442 | 
21-12-24 16:37:48.498 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:37:48.499 - INFO: Train epoch 1929:   Loss: 634.3287 | r_Loss: 62.2063 | g_Loss: 290.7204 | l_Loss: 32.5767 | 
21-12-24 16:39:02.076 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:39:02.077 - INFO: Train epoch 1930:   Loss: 623.9433 | r_Loss: 59.8283 | g_Loss: 287.3509 | l_Loss: 37.4509 | 
21-12-24 16:40:15.622 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:40:15.623 - INFO: Train epoch 1931:   Loss: 600.1869 | r_Loss: 57.8146 | g_Loss: 275.4819 | l_Loss: 35.6321 | 
21-12-24 16:41:29.239 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:41:29.240 - INFO: Train epoch 1932:   Loss: 643.4028 | r_Loss: 63.5580 | g_Loss: 286.0055 | l_Loss: 39.6074 | 
21-12-24 16:42:42.872 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:42:42.873 - INFO: Train epoch 1933:   Loss: 648.3691 | r_Loss: 64.9372 | g_Loss: 280.2251 | l_Loss: 43.4581 | 
21-12-24 16:43:56.672 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:43:56.673 - INFO: Train epoch 1934:   Loss: 637.2269 | r_Loss: 59.8688 | g_Loss: 293.7668 | l_Loss: 44.1164 | 
21-12-24 16:45:10.102 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:45:10.103 - INFO: Train epoch 1935:   Loss: 585.4444 | r_Loss: 56.4364 | g_Loss: 270.4253 | l_Loss: 32.8372 | 
21-12-24 16:46:23.793 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:46:23.794 - INFO: Train epoch 1936:   Loss: 607.2162 | r_Loss: 59.5265 | g_Loss: 270.3215 | l_Loss: 39.2621 | 
21-12-24 16:47:37.425 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:47:37.426 - INFO: Train epoch 1937:   Loss: 618.0518 | r_Loss: 62.3136 | g_Loss: 273.2433 | l_Loss: 33.2404 | 
21-12-24 16:48:50.828 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:48:50.829 - INFO: Train epoch 1938:   Loss: 605.9599 | r_Loss: 59.9540 | g_Loss: 279.9192 | l_Loss: 26.2707 | 
21-12-24 16:50:04.920 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:50:04.921 - INFO: Train epoch 1939:   Loss: 636.2723 | r_Loss: 63.2826 | g_Loss: 289.7265 | l_Loss: 30.1326 | 
21-12-24 16:51:18.408 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:51:18.409 - INFO: Train epoch 1940:   Loss: 636.5492 | r_Loss: 65.3328 | g_Loss: 273.3096 | l_Loss: 36.5757 | 
21-12-24 16:52:32.258 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:52:32.259 - INFO: Train epoch 1941:   Loss: 628.7292 | r_Loss: 62.4662 | g_Loss: 277.4726 | l_Loss: 38.9255 | 
21-12-24 16:53:45.801 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:53:45.803 - INFO: Train epoch 1942:   Loss: 661.5400 | r_Loss: 67.2657 | g_Loss: 284.9851 | l_Loss: 40.2263 | 
21-12-24 16:54:59.754 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:54:59.755 - INFO: Train epoch 1943:   Loss: 680.2688 | r_Loss: 71.9889 | g_Loss: 282.0589 | l_Loss: 38.2652 | 
21-12-24 16:56:13.443 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:56:13.444 - INFO: Train epoch 1944:   Loss: 586.0674 | r_Loss: 59.2695 | g_Loss: 264.7375 | l_Loss: 24.9826 | 
21-12-24 16:57:26.931 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:57:26.932 - INFO: Train epoch 1945:   Loss: 697.1279 | r_Loss: 76.0401 | g_Loss: 287.3003 | l_Loss: 29.6272 | 
21-12-24 16:58:40.215 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:58:40.216 - INFO: Train epoch 1946:   Loss: 623.6015 | r_Loss: 63.8315 | g_Loss: 270.3138 | l_Loss: 34.1305 | 
21-12-24 16:59:53.558 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 16:59:53.559 - INFO: Train epoch 1947:   Loss: 648.2332 | r_Loss: 65.9899 | g_Loss: 284.1317 | l_Loss: 34.1520 | 
21-12-24 17:01:06.921 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:01:06.923 - INFO: Train epoch 1948:   Loss: 619.3136 | r_Loss: 63.2762 | g_Loss: 266.4289 | l_Loss: 36.5038 | 
21-12-24 17:02:20.815 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:02:20.816 - INFO: Train epoch 1949:   Loss: 587.3828 | r_Loss: 60.7599 | g_Loss: 251.8979 | l_Loss: 31.6854 | 
21-12-24 17:04:10.451 - INFO: TEST:   PSNR_S: 45.3047 | PSNR_C: 37.6403 | 
21-12-24 17:04:10.453 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:04:10.453 - INFO: Train epoch 1950:   Loss: 612.2067 | r_Loss: 63.2890 | g_Loss: 268.3908 | l_Loss: 27.3711 | 
21-12-24 17:05:24.228 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:05:24.229 - INFO: Train epoch 1951:   Loss: 618.8943 | r_Loss: 65.4116 | g_Loss: 263.3706 | l_Loss: 28.4659 | 
21-12-24 17:06:37.589 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:06:37.590 - INFO: Train epoch 1952:   Loss: 793.5032 | r_Loss: 96.9822 | g_Loss: 277.7022 | l_Loss: 30.8901 | 
21-12-24 17:07:51.062 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:07:51.063 - INFO: Train epoch 1953:   Loss: 630.4517 | r_Loss: 63.2685 | g_Loss: 281.3518 | l_Loss: 32.7573 | 
21-12-24 17:09:05.053 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:09:05.054 - INFO: Train epoch 1954:   Loss: 613.0163 | r_Loss: 62.1412 | g_Loss: 270.5010 | l_Loss: 31.8094 | 
21-12-24 17:10:18.853 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:10:18.854 - INFO: Train epoch 1955:   Loss: 595.5946 | r_Loss: 59.1312 | g_Loss: 267.1997 | l_Loss: 32.7387 | 
21-12-24 17:11:32.409 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:11:32.411 - INFO: Train epoch 1956:   Loss: 618.6234 | r_Loss: 61.4558 | g_Loss: 279.1566 | l_Loss: 32.1875 | 
21-12-24 17:12:46.183 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:12:46.184 - INFO: Train epoch 1957:   Loss: 586.3047 | r_Loss: 59.2662 | g_Loss: 262.7378 | l_Loss: 27.2361 | 
21-12-24 17:13:59.741 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:13:59.742 - INFO: Train epoch 1958:   Loss: 587.9225 | r_Loss: 57.8819 | g_Loss: 265.8033 | l_Loss: 32.7095 | 
21-12-24 17:15:13.911 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:15:13.912 - INFO: Train epoch 1959:   Loss: 620.8686 | r_Loss: 62.0006 | g_Loss: 272.1723 | l_Loss: 38.6931 | 
21-12-24 17:16:27.320 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:16:27.321 - INFO: Train epoch 1960:   Loss: 578.1654 | r_Loss: 58.2360 | g_Loss: 253.7806 | l_Loss: 33.2050 | 
21-12-24 17:17:40.645 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:17:40.646 - INFO: Train epoch 1961:   Loss: 610.2687 | r_Loss: 62.0060 | g_Loss: 270.1520 | l_Loss: 30.0868 | 
21-12-24 17:18:54.000 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:18:54.002 - INFO: Train epoch 1962:   Loss: 612.2146 | r_Loss: 62.1913 | g_Loss: 264.2066 | l_Loss: 37.0517 | 
21-12-24 17:20:07.578 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:20:07.579 - INFO: Train epoch 1963:   Loss: 625.5423 | r_Loss: 64.3910 | g_Loss: 271.1981 | l_Loss: 32.3893 | 
21-12-24 17:21:20.993 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:21:20.994 - INFO: Train epoch 1964:   Loss: 633.2656 | r_Loss: 64.5620 | g_Loss: 273.6413 | l_Loss: 36.8142 | 
21-12-24 17:22:34.909 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:22:34.910 - INFO: Train epoch 1965:   Loss: 624.3680 | r_Loss: 64.8415 | g_Loss: 262.3753 | l_Loss: 37.7853 | 
21-12-24 17:23:48.488 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:23:48.489 - INFO: Train epoch 1966:   Loss: 628.7107 | r_Loss: 65.1324 | g_Loss: 268.7161 | l_Loss: 34.3324 | 
21-12-24 17:25:01.861 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:25:01.862 - INFO: Train epoch 1967:   Loss: 795.4167 | r_Loss: 90.9734 | g_Loss: 298.1314 | l_Loss: 42.4184 | 
21-12-24 17:26:15.707 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:26:15.708 - INFO: Train epoch 1968:   Loss: 632.6498 | r_Loss: 64.8428 | g_Loss: 274.1638 | l_Loss: 34.2718 | 
21-12-24 17:27:29.110 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:27:29.111 - INFO: Train epoch 1969:   Loss: 581.5862 | r_Loss: 59.0073 | g_Loss: 258.3323 | l_Loss: 28.2175 | 
21-12-24 17:28:42.559 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:28:42.560 - INFO: Train epoch 1970:   Loss: 635.6687 | r_Loss: 63.4828 | g_Loss: 277.9344 | l_Loss: 40.3203 | 
21-12-24 17:29:55.941 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:29:55.942 - INFO: Train epoch 1971:   Loss: 632.5222 | r_Loss: 68.0719 | g_Loss: 261.3104 | l_Loss: 30.8526 | 
21-12-24 17:31:09.359 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:31:09.360 - INFO: Train epoch 1972:   Loss: 611.2465 | r_Loss: 62.1317 | g_Loss: 269.5029 | l_Loss: 31.0851 | 
21-12-24 17:32:22.906 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:32:22.907 - INFO: Train epoch 1973:   Loss: 613.8267 | r_Loss: 64.7421 | g_Loss: 256.6255 | l_Loss: 33.4906 | 
21-12-24 17:33:36.698 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:33:36.699 - INFO: Train epoch 1974:   Loss: 614.9026 | r_Loss: 63.6278 | g_Loss: 260.1656 | l_Loss: 36.5981 | 
21-12-24 17:34:50.377 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:34:50.378 - INFO: Train epoch 1975:   Loss: 631.7577 | r_Loss: 65.7075 | g_Loss: 262.1822 | l_Loss: 41.0379 | 
21-12-24 17:36:03.938 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:36:03.939 - INFO: Train epoch 1976:   Loss: 587.8239 | r_Loss: 58.8955 | g_Loss: 265.3509 | l_Loss: 27.9954 | 
21-12-24 17:37:17.355 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:37:17.356 - INFO: Train epoch 1977:   Loss: 587.2894 | r_Loss: 59.9837 | g_Loss: 259.3176 | l_Loss: 28.0534 | 
21-12-24 17:38:30.883 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:38:30.885 - INFO: Train epoch 1978:   Loss: 613.0306 | r_Loss: 61.8243 | g_Loss: 266.7836 | l_Loss: 37.1257 | 
21-12-24 17:39:44.347 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:39:44.348 - INFO: Train epoch 1979:   Loss: 619.4720 | r_Loss: 63.5345 | g_Loss: 272.2066 | l_Loss: 29.5930 | 
21-12-24 17:41:33.922 - INFO: TEST:   PSNR_S: 45.4568 | PSNR_C: 37.8514 | 
21-12-24 17:41:33.924 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:41:33.924 - INFO: Train epoch 1980:   Loss: 583.4941 | r_Loss: 61.8989 | g_Loss: 246.5665 | l_Loss: 27.4330 | 
21-12-24 17:42:47.060 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:42:47.061 - INFO: Train epoch 1981:   Loss: 632.6852 | r_Loss: 66.2075 | g_Loss: 275.2881 | l_Loss: 26.3594 | 
21-12-24 17:44:00.728 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:44:00.729 - INFO: Train epoch 1982:   Loss: 629.7519 | r_Loss: 64.3060 | g_Loss: 273.7787 | l_Loss: 34.4431 | 
21-12-24 17:45:13.919 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:45:13.920 - INFO: Train epoch 1983:   Loss: 620.1705 | r_Loss: 63.3343 | g_Loss: 264.7587 | l_Loss: 38.7405 | 
21-12-24 17:46:27.423 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:46:27.424 - INFO: Train epoch 1984:   Loss: 607.7524 | r_Loss: 62.4182 | g_Loss: 258.6069 | l_Loss: 37.0544 | 
21-12-24 17:47:40.867 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:47:40.868 - INFO: Train epoch 1985:   Loss: 649.9184 | r_Loss: 65.0347 | g_Loss: 288.6903 | l_Loss: 36.0547 | 
21-12-24 17:48:54.303 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:48:54.304 - INFO: Train epoch 1986:   Loss: 587.1150 | r_Loss: 60.9735 | g_Loss: 257.6428 | l_Loss: 24.6049 | 
21-12-24 17:50:07.616 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:50:07.617 - INFO: Train epoch 1987:   Loss: 12177.0717 | r_Loss: 2223.8411 | g_Loss: 941.7559 | l_Loss: 116.1102 | 
21-12-24 17:51:21.285 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:51:21.287 - INFO: Train epoch 1988:   Loss: 1326.1720 | r_Loss: 147.1959 | g_Loss: 529.3060 | l_Loss: 60.8864 | 
21-12-24 17:52:34.615 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:52:34.616 - INFO: Train epoch 1989:   Loss: 1111.7152 | r_Loss: 116.3364 | g_Loss: 474.1011 | l_Loss: 55.9324 | 
21-12-24 17:53:47.975 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:53:47.976 - INFO: Train epoch 1990:   Loss: 1013.3722 | r_Loss: 102.3423 | g_Loss: 446.8372 | l_Loss: 54.8237 | 
21-12-24 17:55:01.241 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:55:01.242 - INFO: Train epoch 1991:   Loss: 1012.6008 | r_Loss: 99.3762 | g_Loss: 455.9877 | l_Loss: 59.7324 | 
21-12-24 17:56:14.555 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:56:14.556 - INFO: Train epoch 1992:   Loss: 894.5727 | r_Loss: 86.6377 | g_Loss: 404.6056 | l_Loss: 56.7788 | 
21-12-24 17:57:27.893 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:57:27.895 - INFO: Train epoch 1993:   Loss: 863.3781 | r_Loss: 81.6897 | g_Loss: 404.1323 | l_Loss: 50.7975 | 
21-12-24 17:58:41.868 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:58:41.869 - INFO: Train epoch 1994:   Loss: 822.8185 | r_Loss: 76.9013 | g_Loss: 387.2363 | l_Loss: 51.0759 | 
21-12-24 17:59:55.435 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 17:59:55.436 - INFO: Train epoch 1995:   Loss: 802.0839 | r_Loss: 77.0935 | g_Loss: 374.6093 | l_Loss: 42.0071 | 
21-12-24 18:01:08.990 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:01:08.991 - INFO: Train epoch 1996:   Loss: 812.7463 | r_Loss: 78.2166 | g_Loss: 377.6234 | l_Loss: 44.0399 | 
21-12-24 18:02:22.508 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:02:22.509 - INFO: Train epoch 1997:   Loss: 785.9746 | r_Loss: 73.8684 | g_Loss: 367.8361 | l_Loss: 48.7967 | 
21-12-24 18:03:35.956 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:03:35.957 - INFO: Train epoch 1998:   Loss: 749.2332 | r_Loss: 70.6028 | g_Loss: 356.0359 | l_Loss: 40.1833 | 
21-12-24 18:04:49.300 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:04:49.301 - INFO: Train epoch 1999:   Loss: 749.4429 | r_Loss: 71.3410 | g_Loss: 350.8643 | l_Loss: 41.8736 | 
21-12-24 18:06:02.458 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:06:02.460 - INFO: Train epoch 2000:   Loss: 751.5022 | r_Loss: 71.1040 | g_Loss: 351.7186 | l_Loss: 44.2635 | 
21-12-24 18:07:16.259 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:07:16.261 - INFO: Train epoch 2001:   Loss: 725.8917 | r_Loss: 67.8326 | g_Loss: 345.8735 | l_Loss: 40.8550 | 
21-12-24 18:08:29.417 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:08:29.418 - INFO: Train epoch 2002:   Loss: 706.2910 | r_Loss: 67.3700 | g_Loss: 327.3787 | l_Loss: 42.0621 | 
21-12-24 18:09:42.476 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:09:42.476 - INFO: Train epoch 2003:   Loss: 681.5402 | r_Loss: 63.0435 | g_Loss: 328.7410 | l_Loss: 37.5814 | 
21-12-24 18:10:56.069 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:10:56.070 - INFO: Train epoch 2004:   Loss: 717.3996 | r_Loss: 65.1688 | g_Loss: 340.7284 | l_Loss: 50.8273 | 
21-12-24 18:12:09.344 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:12:09.344 - INFO: Train epoch 2005:   Loss: 716.3013 | r_Loss: 68.1353 | g_Loss: 333.9845 | l_Loss: 41.6404 | 
21-12-24 18:13:22.821 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:13:22.821 - INFO: Train epoch 2006:   Loss: 698.4550 | r_Loss: 65.5618 | g_Loss: 333.4192 | l_Loss: 37.2271 | 
21-12-24 18:14:36.338 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:14:36.339 - INFO: Train epoch 2007:   Loss: 693.6376 | r_Loss: 65.3407 | g_Loss: 324.8096 | l_Loss: 42.1247 | 
21-12-24 18:15:49.738 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:15:49.740 - INFO: Train epoch 2008:   Loss: 688.8421 | r_Loss: 64.6166 | g_Loss: 324.3113 | l_Loss: 41.4478 | 
21-12-24 18:17:03.167 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:17:03.168 - INFO: Train epoch 2009:   Loss: 716.0856 | r_Loss: 67.7879 | g_Loss: 333.4016 | l_Loss: 43.7444 | 
21-12-24 18:18:53.098 - INFO: TEST:   PSNR_S: 45.1175 | PSNR_C: 36.7520 | 
21-12-24 18:18:53.100 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:18:53.101 - INFO: Train epoch 2010:   Loss: 608.4285 | r_Loss: 55.8832 | g_Loss: 291.8996 | l_Loss: 37.1130 | 
21-12-24 18:20:06.437 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:20:06.438 - INFO: Train epoch 2011:   Loss: 708.6412 | r_Loss: 68.5439 | g_Loss: 327.3189 | l_Loss: 38.6029 | 
21-12-24 18:21:19.923 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:21:19.924 - INFO: Train epoch 2012:   Loss: 663.0554 | r_Loss: 61.6973 | g_Loss: 315.9949 | l_Loss: 38.5741 | 
21-12-24 18:22:33.164 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:22:33.165 - INFO: Train epoch 2013:   Loss: 668.1866 | r_Loss: 63.8422 | g_Loss: 315.4560 | l_Loss: 33.5198 | 
21-12-24 18:23:46.737 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:23:46.738 - INFO: Train epoch 2014:   Loss: 634.6868 | r_Loss: 59.9277 | g_Loss: 295.8869 | l_Loss: 39.1615 | 
21-12-24 18:25:00.204 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:25:00.205 - INFO: Train epoch 2015:   Loss: 619.1524 | r_Loss: 57.6669 | g_Loss: 290.9759 | l_Loss: 39.8420 | 
21-12-24 18:26:14.051 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:26:14.052 - INFO: Train epoch 2016:   Loss: 622.1457 | r_Loss: 60.3048 | g_Loss: 285.7946 | l_Loss: 34.8270 | 
21-12-24 18:27:27.353 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:27:27.354 - INFO: Train epoch 2017:   Loss: 635.6873 | r_Loss: 61.4487 | g_Loss: 296.2848 | l_Loss: 32.1588 | 
21-12-24 18:28:40.918 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:28:40.918 - INFO: Train epoch 2018:   Loss: 655.0203 | r_Loss: 64.0857 | g_Loss: 294.2851 | l_Loss: 40.3068 | 
21-12-24 18:29:54.052 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:29:54.053 - INFO: Train epoch 2019:   Loss: 656.4808 | r_Loss: 63.7495 | g_Loss: 302.9159 | l_Loss: 34.8174 | 
21-12-24 18:31:07.340 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:31:07.341 - INFO: Train epoch 2020:   Loss: 658.0201 | r_Loss: 65.2244 | g_Loss: 295.9575 | l_Loss: 35.9406 | 
21-12-24 18:32:20.971 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:32:20.972 - INFO: Train epoch 2021:   Loss: 676.6056 | r_Loss: 64.7296 | g_Loss: 309.2870 | l_Loss: 43.6705 | 
21-12-24 18:33:34.548 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:33:34.549 - INFO: Train epoch 2022:   Loss: 600.8246 | r_Loss: 56.0690 | g_Loss: 286.6454 | l_Loss: 33.8341 | 
21-12-24 18:34:47.994 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:34:47.995 - INFO: Train epoch 2023:   Loss: 629.6818 | r_Loss: 60.5555 | g_Loss: 287.4399 | l_Loss: 39.4643 | 
21-12-24 18:36:01.388 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:36:01.389 - INFO: Train epoch 2024:   Loss: 608.7965 | r_Loss: 60.1987 | g_Loss: 272.1046 | l_Loss: 35.6987 | 
21-12-24 18:37:15.356 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:37:15.357 - INFO: Train epoch 2025:   Loss: 656.4832 | r_Loss: 64.4006 | g_Loss: 291.9456 | l_Loss: 42.5345 | 
21-12-24 18:38:28.730 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:38:28.731 - INFO: Train epoch 2026:   Loss: 667.5681 | r_Loss: 65.8187 | g_Loss: 298.9589 | l_Loss: 39.5158 | 
21-12-24 18:39:42.184 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:39:42.185 - INFO: Train epoch 2027:   Loss: 621.1933 | r_Loss: 60.8942 | g_Loss: 282.9756 | l_Loss: 33.7467 | 
21-12-24 18:40:55.798 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:40:55.799 - INFO: Train epoch 2028:   Loss: 604.4173 | r_Loss: 56.4200 | g_Loss: 288.3503 | l_Loss: 33.9670 | 
21-12-24 18:42:08.941 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:42:08.942 - INFO: Train epoch 2029:   Loss: 636.4399 | r_Loss: 62.1395 | g_Loss: 280.3760 | l_Loss: 45.3662 | 
21-12-24 18:43:22.672 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:43:22.673 - INFO: Train epoch 2030:   Loss: 594.2249 | r_Loss: 58.3599 | g_Loss: 265.2315 | l_Loss: 37.1941 | 
21-12-24 18:44:36.398 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:44:36.399 - INFO: Train epoch 2031:   Loss: 620.3902 | r_Loss: 63.3986 | g_Loss: 267.7229 | l_Loss: 35.6744 | 
21-12-24 18:45:50.123 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:45:50.124 - INFO: Train epoch 2032:   Loss: 571.7649 | r_Loss: 56.1656 | g_Loss: 261.3795 | l_Loss: 29.5571 | 
21-12-24 18:47:03.557 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:47:03.558 - INFO: Train epoch 2033:   Loss: 606.9786 | r_Loss: 58.5625 | g_Loss: 282.9861 | l_Loss: 31.1802 | 
21-12-24 18:48:16.783 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:48:16.784 - INFO: Train epoch 2034:   Loss: 595.9614 | r_Loss: 58.6328 | g_Loss: 267.4713 | l_Loss: 35.3261 | 
21-12-24 18:49:30.256 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:49:30.257 - INFO: Train epoch 2035:   Loss: 621.5777 | r_Loss: 63.5517 | g_Loss: 272.7368 | l_Loss: 31.0824 | 
21-12-24 18:50:43.463 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:50:43.464 - INFO: Train epoch 2036:   Loss: 606.8375 | r_Loss: 61.0973 | g_Loss: 273.1465 | l_Loss: 28.2046 | 
21-12-24 18:51:56.897 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:51:56.898 - INFO: Train epoch 2037:   Loss: 618.4793 | r_Loss: 62.8638 | g_Loss: 271.7600 | l_Loss: 32.4002 | 
21-12-24 18:53:10.148 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:53:10.149 - INFO: Train epoch 2038:   Loss: 659.1715 | r_Loss: 66.9698 | g_Loss: 290.3307 | l_Loss: 33.9918 | 
21-12-24 18:54:23.249 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:54:23.250 - INFO: Train epoch 2039:   Loss: 614.0109 | r_Loss: 59.1002 | g_Loss: 276.4518 | l_Loss: 42.0579 | 
21-12-24 18:56:12.845 - INFO: TEST:   PSNR_S: 45.3207 | PSNR_C: 37.5570 | 
21-12-24 18:56:12.846 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:56:12.846 - INFO: Train epoch 2040:   Loss: 621.7762 | r_Loss: 62.8809 | g_Loss: 272.9762 | l_Loss: 34.3955 | 
21-12-24 18:57:26.674 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:57:26.675 - INFO: Train epoch 2041:   Loss: 599.3791 | r_Loss: 58.8264 | g_Loss: 264.2889 | l_Loss: 40.9582 | 
21-12-24 18:58:40.213 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:58:40.215 - INFO: Train epoch 2042:   Loss: 613.6683 | r_Loss: 64.3475 | g_Loss: 266.3007 | l_Loss: 25.6302 | 
21-12-24 18:59:53.832 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 18:59:53.834 - INFO: Train epoch 2043:   Loss: 583.6748 | r_Loss: 57.4175 | g_Loss: 260.2652 | l_Loss: 36.3222 | 
21-12-24 19:01:07.145 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:01:07.146 - INFO: Train epoch 2044:   Loss: 610.9311 | r_Loss: 61.7668 | g_Loss: 267.2660 | l_Loss: 34.8314 | 
21-12-24 19:02:20.963 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:02:20.964 - INFO: Train epoch 2045:   Loss: 577.8724 | r_Loss: 57.4485 | g_Loss: 258.1535 | l_Loss: 32.4765 | 
21-12-24 19:03:34.212 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:03:34.213 - INFO: Train epoch 2046:   Loss: 647.8192 | r_Loss: 67.0812 | g_Loss: 279.7912 | l_Loss: 32.6220 | 
21-12-24 19:04:47.616 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:04:47.617 - INFO: Train epoch 2047:   Loss: 580.2238 | r_Loss: 56.5505 | g_Loss: 261.5964 | l_Loss: 35.8749 | 
21-12-24 19:06:01.048 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:06:01.049 - INFO: Train epoch 2048:   Loss: 620.5916 | r_Loss: 63.2416 | g_Loss: 268.9603 | l_Loss: 35.4235 | 
21-12-24 19:07:14.489 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:07:14.490 - INFO: Train epoch 2049:   Loss: 591.4287 | r_Loss: 59.7033 | g_Loss: 259.7360 | l_Loss: 33.1760 | 
21-12-24 19:08:28.303 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:08:28.304 - INFO: Train epoch 2050:   Loss: 585.8399 | r_Loss: 58.8181 | g_Loss: 258.4100 | l_Loss: 33.3393 | 
21-12-24 19:09:41.680 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:09:41.681 - INFO: Train epoch 2051:   Loss: 585.3919 | r_Loss: 58.3329 | g_Loss: 260.6730 | l_Loss: 33.0546 | 
21-12-24 19:10:54.995 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:10:54.996 - INFO: Train epoch 2052:   Loss: 614.2025 | r_Loss: 61.7678 | g_Loss: 271.7930 | l_Loss: 33.5707 | 
21-12-24 19:12:09.113 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:12:09.114 - INFO: Train epoch 2053:   Loss: 811.2597 | r_Loss: 98.3334 | g_Loss: 283.1136 | l_Loss: 36.4792 | 
21-12-24 19:13:22.423 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:13:22.424 - INFO: Train epoch 2054:   Loss: 588.0144 | r_Loss: 55.9099 | g_Loss: 265.8312 | l_Loss: 42.6335 | 
21-12-24 19:14:35.782 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:14:35.783 - INFO: Train epoch 2055:   Loss: 584.1778 | r_Loss: 59.3582 | g_Loss: 259.4061 | l_Loss: 27.9807 | 
21-12-24 19:15:49.214 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:15:49.215 - INFO: Train epoch 2056:   Loss: 670.0383 | r_Loss: 68.5046 | g_Loss: 297.3853 | l_Loss: 30.1300 | 
21-12-24 19:17:02.748 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:17:02.749 - INFO: Train epoch 2057:   Loss: 593.3636 | r_Loss: 57.8398 | g_Loss: 270.8876 | l_Loss: 33.2772 | 
21-12-24 19:18:16.064 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:18:16.065 - INFO: Train epoch 2058:   Loss: 590.3879 | r_Loss: 58.7892 | g_Loss: 265.0000 | l_Loss: 31.4419 | 
21-12-24 19:19:29.704 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:19:29.705 - INFO: Train epoch 2059:   Loss: 626.5792 | r_Loss: 63.9581 | g_Loss: 265.3119 | l_Loss: 41.4768 | 
21-12-24 19:20:43.319 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:20:43.320 - INFO: Train epoch 2060:   Loss: 605.0838 | r_Loss: 61.2045 | g_Loss: 258.5201 | l_Loss: 40.5410 | 
21-12-24 19:21:57.451 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:21:57.452 - INFO: Train epoch 2061:   Loss: 615.6128 | r_Loss: 62.7622 | g_Loss: 267.6820 | l_Loss: 34.1196 | 
21-12-24 19:23:10.847 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:23:10.849 - INFO: Train epoch 2062:   Loss: 608.1586 | r_Loss: 63.1778 | g_Loss: 261.1231 | l_Loss: 31.1465 | 
21-12-24 19:24:24.456 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:24:24.458 - INFO: Train epoch 2063:   Loss: 600.4301 | r_Loss: 61.0596 | g_Loss: 266.0264 | l_Loss: 29.1057 | 
21-12-24 19:25:38.278 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:25:38.279 - INFO: Train epoch 2064:   Loss: 568.5084 | r_Loss: 56.5029 | g_Loss: 262.2236 | l_Loss: 23.7703 | 
21-12-24 19:26:51.723 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:26:51.724 - INFO: Train epoch 2065:   Loss: 605.0953 | r_Loss: 60.4288 | g_Loss: 264.3672 | l_Loss: 38.5840 | 
21-12-24 19:28:05.768 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:28:05.769 - INFO: Train epoch 2066:   Loss: 586.4251 | r_Loss: 61.1549 | g_Loss: 252.6677 | l_Loss: 27.9831 | 
21-12-24 19:29:19.507 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:29:19.509 - INFO: Train epoch 2067:   Loss: 565.4580 | r_Loss: 56.7114 | g_Loss: 248.3273 | l_Loss: 33.5736 | 
21-12-24 19:30:33.077 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:30:33.078 - INFO: Train epoch 2068:   Loss: 640.4105 | r_Loss: 66.3730 | g_Loss: 268.9954 | l_Loss: 39.5498 | 
21-12-24 19:31:46.666 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:31:46.668 - INFO: Train epoch 2069:   Loss: 663.9168 | r_Loss: 72.2682 | g_Loss: 271.5784 | l_Loss: 30.9972 | 
21-12-24 19:33:36.325 - INFO: TEST:   PSNR_S: 45.6989 | PSNR_C: 37.9242 | 
21-12-24 19:33:36.326 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:33:36.327 - INFO: Train epoch 2070:   Loss: 575.8256 | r_Loss: 58.9576 | g_Loss: 252.4620 | l_Loss: 28.5757 | 
21-12-24 19:34:49.929 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:34:49.931 - INFO: Train epoch 2071:   Loss: 589.4905 | r_Loss: 58.2241 | g_Loss: 256.0445 | l_Loss: 42.3257 | 
21-12-24 19:36:03.817 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:36:03.818 - INFO: Train epoch 2072:   Loss: 578.0674 | r_Loss: 59.1621 | g_Loss: 254.7216 | l_Loss: 27.5352 | 
21-12-24 19:37:17.415 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:37:17.416 - INFO: Train epoch 2073:   Loss: 599.3524 | r_Loss: 60.7755 | g_Loss: 265.2043 | l_Loss: 30.2708 | 
21-12-24 19:38:30.719 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:38:30.720 - INFO: Train epoch 2074:   Loss: 601.7044 | r_Loss: 61.9335 | g_Loss: 253.4678 | l_Loss: 38.5691 | 
21-12-24 19:39:44.111 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:39:44.112 - INFO: Train epoch 2075:   Loss: 602.0565 | r_Loss: 61.3546 | g_Loss: 265.5864 | l_Loss: 29.6972 | 
21-12-24 19:40:58.112 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:40:58.114 - INFO: Train epoch 2076:   Loss: 587.1171 | r_Loss: 59.7722 | g_Loss: 248.5386 | l_Loss: 39.7174 | 
21-12-24 19:42:11.807 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:42:11.808 - INFO: Train epoch 2077:   Loss: 574.5993 | r_Loss: 59.2057 | g_Loss: 251.7776 | l_Loss: 26.7933 | 
21-12-24 19:43:25.569 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:43:25.570 - INFO: Train epoch 2078:   Loss: 626.3902 | r_Loss: 66.5088 | g_Loss: 265.7307 | l_Loss: 28.1154 | 
21-12-24 19:44:38.789 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:44:38.791 - INFO: Train epoch 2079:   Loss: 567.8792 | r_Loss: 56.3761 | g_Loss: 247.6813 | l_Loss: 38.3177 | 
21-12-24 19:45:52.260 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:45:52.261 - INFO: Train epoch 2080:   Loss: 603.9240 | r_Loss: 62.0461 | g_Loss: 262.5035 | l_Loss: 31.1898 | 
21-12-24 19:47:05.730 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:47:05.732 - INFO: Train epoch 2081:   Loss: 603.5378 | r_Loss: 62.5034 | g_Loss: 257.5332 | l_Loss: 33.4875 | 
21-12-24 19:48:19.210 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:48:19.211 - INFO: Train epoch 2082:   Loss: 620.7764 | r_Loss: 64.5120 | g_Loss: 266.9704 | l_Loss: 31.2460 | 
21-12-24 19:49:32.620 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:49:32.622 - INFO: Train epoch 2083:   Loss: 629.5641 | r_Loss: 64.3834 | g_Loss: 275.2602 | l_Loss: 32.3868 | 
21-12-24 19:50:46.175 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:50:46.176 - INFO: Train epoch 2084:   Loss: 596.5758 | r_Loss: 62.3626 | g_Loss: 262.3234 | l_Loss: 22.4395 | 
21-12-24 19:51:59.607 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:51:59.608 - INFO: Train epoch 2085:   Loss: 583.8447 | r_Loss: 60.7892 | g_Loss: 253.8456 | l_Loss: 26.0531 | 
21-12-24 19:53:13.722 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:53:13.723 - INFO: Train epoch 2086:   Loss: 620.0317 | r_Loss: 65.0637 | g_Loss: 263.4192 | l_Loss: 31.2940 | 
21-12-24 19:54:27.411 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:54:27.412 - INFO: Train epoch 2087:   Loss: 576.4570 | r_Loss: 58.0738 | g_Loss: 249.5292 | l_Loss: 36.5586 | 
21-12-24 19:55:41.084 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:55:41.085 - INFO: Train epoch 2088:   Loss: 627.9779 | r_Loss: 63.9151 | g_Loss: 272.6048 | l_Loss: 35.7974 | 
21-12-24 19:56:54.862 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:56:54.863 - INFO: Train epoch 2089:   Loss: 549.3365 | r_Loss: 54.8260 | g_Loss: 247.4111 | l_Loss: 27.7955 | 
21-12-24 19:58:08.195 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:58:08.196 - INFO: Train epoch 2090:   Loss: 620.7209 | r_Loss: 65.9888 | g_Loss: 254.2906 | l_Loss: 36.4861 | 
21-12-24 19:59:21.983 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 19:59:21.985 - INFO: Train epoch 2091:   Loss: 604.6688 | r_Loss: 62.2217 | g_Loss: 258.7897 | l_Loss: 34.7707 | 
21-12-24 20:00:35.579 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:00:35.580 - INFO: Train epoch 2092:   Loss: 593.5051 | r_Loss: 60.1060 | g_Loss: 256.4493 | l_Loss: 36.5256 | 
21-12-24 20:01:48.887 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:01:48.888 - INFO: Train epoch 2093:   Loss: 598.3478 | r_Loss: 61.9879 | g_Loss: 259.2312 | l_Loss: 29.1773 | 
21-12-24 20:03:02.119 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:03:02.121 - INFO: Train epoch 2094:   Loss: 569.9269 | r_Loss: 56.1280 | g_Loss: 252.0562 | l_Loss: 37.2309 | 
21-12-24 20:04:15.620 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:04:15.621 - INFO: Train epoch 2095:   Loss: 563.9193 | r_Loss: 58.4704 | g_Loss: 243.7142 | l_Loss: 27.8531 | 
21-12-24 20:05:29.199 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:05:29.200 - INFO: Train epoch 2096:   Loss: 648.5836 | r_Loss: 73.5737 | g_Loss: 253.2119 | l_Loss: 27.5034 | 
21-12-24 20:06:42.575 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:06:42.576 - INFO: Train epoch 2097:   Loss: 573.1320 | r_Loss: 57.1994 | g_Loss: 250.6656 | l_Loss: 36.4691 | 
21-12-24 20:07:56.233 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:07:56.234 - INFO: Train epoch 2098:   Loss: 564.7176 | r_Loss: 57.6931 | g_Loss: 241.1465 | l_Loss: 35.1057 | 
21-12-24 20:09:09.686 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:09:09.688 - INFO: Train epoch 2099:   Loss: 542.8961 | r_Loss: 55.6516 | g_Loss: 232.7191 | l_Loss: 31.9189 | 
21-12-24 20:10:59.605 - INFO: TEST:   PSNR_S: 39.4885 | PSNR_C: 33.3258 | 
21-12-24 20:10:59.607 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:10:59.607 - INFO: Train epoch 2100:   Loss: 12985.4924 | r_Loss: 2373.9663 | g_Loss: 1001.7327 | l_Loss: 113.9282 | 
21-12-24 20:12:13.919 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:12:13.920 - INFO: Train epoch 2101:   Loss: 1261.9005 | r_Loss: 133.1727 | g_Loss: 530.8890 | l_Loss: 65.1478 | 
21-12-24 20:13:27.355 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:13:27.356 - INFO: Train epoch 2102:   Loss: 1000.2285 | r_Loss: 101.4475 | g_Loss: 441.1989 | l_Loss: 51.7921 | 
21-12-24 20:14:40.894 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:14:40.895 - INFO: Train epoch 2103:   Loss: 985.7283 | r_Loss: 98.3942 | g_Loss: 438.1642 | l_Loss: 55.5932 | 
21-12-24 20:15:54.518 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:15:54.519 - INFO: Train epoch 2104:   Loss: 882.8259 | r_Loss: 80.9873 | g_Loss: 410.0627 | l_Loss: 67.8269 | 
21-12-24 20:17:07.843 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:17:07.844 - INFO: Train epoch 2105:   Loss: 899.1950 | r_Loss: 83.1695 | g_Loss: 424.8014 | l_Loss: 58.5463 | 
21-12-24 20:18:21.677 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:18:21.678 - INFO: Train epoch 2106:   Loss: 783.1773 | r_Loss: 71.8375 | g_Loss: 375.1869 | l_Loss: 48.8028 | 
21-12-24 20:19:35.342 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:19:35.344 - INFO: Train epoch 2107:   Loss: 761.7270 | r_Loss: 69.7515 | g_Loss: 362.9575 | l_Loss: 50.0123 | 
21-12-24 20:20:49.160 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:20:49.161 - INFO: Train epoch 2108:   Loss: 776.4543 | r_Loss: 69.4831 | g_Loss: 379.4947 | l_Loss: 49.5439 | 
21-12-24 20:22:02.467 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:22:02.468 - INFO: Train epoch 2109:   Loss: 779.2716 | r_Loss: 70.7938 | g_Loss: 372.8234 | l_Loss: 52.4791 | 
21-12-24 20:23:15.955 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:23:15.956 - INFO: Train epoch 2110:   Loss: 744.9255 | r_Loss: 66.1459 | g_Loss: 358.1913 | l_Loss: 56.0046 | 
21-12-24 20:24:29.694 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:24:29.695 - INFO: Train epoch 2111:   Loss: 746.9425 | r_Loss: 67.1245 | g_Loss: 364.4836 | l_Loss: 46.8364 | 
21-12-24 20:25:43.505 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:25:43.506 - INFO: Train epoch 2112:   Loss: 717.2841 | r_Loss: 63.9799 | g_Loss: 355.1667 | l_Loss: 42.2178 | 
21-12-24 20:26:57.440 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:26:57.441 - INFO: Train epoch 2113:   Loss: 672.0792 | r_Loss: 60.9283 | g_Loss: 328.8383 | l_Loss: 38.5995 | 
21-12-24 20:28:10.736 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:28:10.737 - INFO: Train epoch 2114:   Loss: 690.0522 | r_Loss: 60.4152 | g_Loss: 339.5373 | l_Loss: 48.4389 | 
21-12-24 20:29:24.105 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:29:24.106 - INFO: Train epoch 2115:   Loss: 695.0441 | r_Loss: 61.5661 | g_Loss: 349.3268 | l_Loss: 37.8868 | 
21-12-24 20:30:37.433 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:30:37.434 - INFO: Train epoch 2116:   Loss: 700.4783 | r_Loss: 63.3824 | g_Loss: 340.4502 | l_Loss: 43.1159 | 
21-12-24 20:31:51.342 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:31:51.343 - INFO: Train epoch 2117:   Loss: 645.9065 | r_Loss: 57.9492 | g_Loss: 321.8809 | l_Loss: 34.2796 | 
21-12-24 20:33:04.823 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:33:04.824 - INFO: Train epoch 2118:   Loss: 662.4146 | r_Loss: 60.5944 | g_Loss: 317.4227 | l_Loss: 42.0198 | 
21-12-24 20:34:18.181 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:34:18.182 - INFO: Train epoch 2119:   Loss: 649.1915 | r_Loss: 59.9825 | g_Loss: 313.2727 | l_Loss: 36.0063 | 
21-12-24 20:35:31.229 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:35:31.230 - INFO: Train epoch 2120:   Loss: 640.7685 | r_Loss: 57.0155 | g_Loss: 318.6620 | l_Loss: 37.0288 | 
21-12-24 20:36:44.713 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:36:44.715 - INFO: Train epoch 2121:   Loss: 618.3744 | r_Loss: 55.2547 | g_Loss: 303.8970 | l_Loss: 38.2037 | 
21-12-24 20:37:57.854 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:37:57.855 - INFO: Train epoch 2122:   Loss: 621.7857 | r_Loss: 53.8981 | g_Loss: 306.0681 | l_Loss: 46.2273 | 
21-12-24 20:39:10.964 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:39:10.965 - INFO: Train epoch 2123:   Loss: 626.1538 | r_Loss: 55.7208 | g_Loss: 306.4480 | l_Loss: 41.1020 | 
21-12-24 20:40:24.673 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:40:24.674 - INFO: Train epoch 2124:   Loss: 649.8204 | r_Loss: 58.9257 | g_Loss: 312.6841 | l_Loss: 42.5076 | 
21-12-24 20:41:38.256 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:41:38.257 - INFO: Train epoch 2125:   Loss: 644.4760 | r_Loss: 56.8312 | g_Loss: 308.0693 | l_Loss: 52.2509 | 
21-12-24 20:42:51.968 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:42:51.969 - INFO: Train epoch 2126:   Loss: 599.9067 | r_Loss: 54.4805 | g_Loss: 290.1981 | l_Loss: 37.3063 | 
21-12-24 20:44:05.124 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:44:05.125 - INFO: Train epoch 2127:   Loss: 603.2827 | r_Loss: 55.7047 | g_Loss: 292.8055 | l_Loss: 31.9538 | 
21-12-24 20:45:18.569 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:45:18.570 - INFO: Train epoch 2128:   Loss: 603.6346 | r_Loss: 55.4636 | g_Loss: 287.4851 | l_Loss: 38.8315 | 
21-12-24 20:46:31.937 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:46:31.939 - INFO: Train epoch 2129:   Loss: 587.3629 | r_Loss: 54.3161 | g_Loss: 272.0835 | l_Loss: 43.6987 | 
21-12-24 20:48:21.557 - INFO: TEST:   PSNR_S: 45.6841 | PSNR_C: 37.2848 | 
21-12-24 20:48:21.558 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:48:21.558 - INFO: Train epoch 2130:   Loss: 569.6523 | r_Loss: 51.7360 | g_Loss: 279.4352 | l_Loss: 31.5371 | 
21-12-24 20:49:34.972 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:49:34.973 - INFO: Train epoch 2131:   Loss: 649.6059 | r_Loss: 63.3131 | g_Loss: 296.8643 | l_Loss: 36.1762 | 
21-12-24 20:50:48.497 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:50:48.498 - INFO: Train epoch 2132:   Loss: 625.8270 | r_Loss: 59.8831 | g_Loss: 286.6863 | l_Loss: 39.7253 | 
21-12-24 20:52:01.591 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:52:01.592 - INFO: Train epoch 2133:   Loss: 571.3463 | r_Loss: 51.2623 | g_Loss: 276.4195 | l_Loss: 38.6153 | 
21-12-24 20:53:14.859 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:53:14.860 - INFO: Train epoch 2134:   Loss: 581.8075 | r_Loss: 54.5641 | g_Loss: 271.7849 | l_Loss: 37.2023 | 
21-12-24 20:54:28.620 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:54:28.621 - INFO: Train epoch 2135:   Loss: 572.1239 | r_Loss: 52.6107 | g_Loss: 268.1429 | l_Loss: 40.9273 | 
21-12-24 20:55:41.959 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:55:41.960 - INFO: Train epoch 2136:   Loss: 577.8400 | r_Loss: 53.4013 | g_Loss: 271.3547 | l_Loss: 39.4790 | 
21-12-24 20:56:55.280 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:56:55.282 - INFO: Train epoch 2137:   Loss: 578.5917 | r_Loss: 53.7497 | g_Loss: 277.1518 | l_Loss: 32.6915 | 
21-12-24 20:58:08.680 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:58:08.681 - INFO: Train epoch 2138:   Loss: 585.8887 | r_Loss: 55.1588 | g_Loss: 267.8288 | l_Loss: 42.2662 | 
21-12-24 20:59:22.161 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 20:59:22.163 - INFO: Train epoch 2139:   Loss: 574.4831 | r_Loss: 55.7636 | g_Loss: 259.6620 | l_Loss: 36.0029 | 
21-12-24 21:00:35.932 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:00:35.933 - INFO: Train epoch 2140:   Loss: 560.4972 | r_Loss: 52.1855 | g_Loss: 266.1804 | l_Loss: 33.3892 | 
21-12-24 21:01:49.336 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:01:49.338 - INFO: Train epoch 2141:   Loss: 638.3803 | r_Loss: 66.5477 | g_Loss: 274.0852 | l_Loss: 31.5568 | 
21-12-24 21:03:03.605 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:03:03.607 - INFO: Train epoch 2142:   Loss: 588.6420 | r_Loss: 60.8198 | g_Loss: 254.8868 | l_Loss: 29.6563 | 
21-12-24 21:04:17.207 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:04:17.208 - INFO: Train epoch 2143:   Loss: 574.2052 | r_Loss: 56.8124 | g_Loss: 259.2476 | l_Loss: 30.8956 | 
21-12-24 21:05:30.871 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:05:30.873 - INFO: Train epoch 2144:   Loss: 544.9614 | r_Loss: 51.2841 | g_Loss: 255.2096 | l_Loss: 33.3315 | 
21-12-24 21:06:44.309 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:06:44.310 - INFO: Train epoch 2145:   Loss: 557.4074 | r_Loss: 53.8924 | g_Loss: 248.3736 | l_Loss: 39.5717 | 
21-12-24 21:07:57.988 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:07:57.989 - INFO: Train epoch 2146:   Loss: 585.8423 | r_Loss: 56.0155 | g_Loss: 273.7731 | l_Loss: 31.9919 | 
21-12-24 21:09:11.717 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:09:11.718 - INFO: Train epoch 2147:   Loss: 622.3124 | r_Loss: 63.9751 | g_Loss: 275.2998 | l_Loss: 27.1373 | 
21-12-24 21:10:25.256 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:10:25.257 - INFO: Train epoch 2148:   Loss: 594.3797 | r_Loss: 58.3430 | g_Loss: 269.0681 | l_Loss: 33.5965 | 
21-12-24 21:11:39.266 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:11:39.268 - INFO: Train epoch 2149:   Loss: 555.4528 | r_Loss: 55.2923 | g_Loss: 251.7839 | l_Loss: 27.2076 | 
21-12-24 21:12:52.838 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:12:52.839 - INFO: Train epoch 2150:   Loss: 637.5213 | r_Loss: 69.1145 | g_Loss: 258.1448 | l_Loss: 33.8042 | 
21-12-24 21:14:06.454 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:14:06.456 - INFO: Train epoch 2151:   Loss: 581.9780 | r_Loss: 57.7892 | g_Loss: 262.0933 | l_Loss: 30.9385 | 
21-12-24 21:15:19.765 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:15:19.766 - INFO: Train epoch 2152:   Loss: 554.7170 | r_Loss: 53.2336 | g_Loss: 253.9608 | l_Loss: 34.5883 | 
21-12-24 21:16:33.797 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:16:33.798 - INFO: Train epoch 2153:   Loss: 591.5201 | r_Loss: 57.8580 | g_Loss: 263.8130 | l_Loss: 38.4172 | 
21-12-24 21:17:47.610 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:17:47.611 - INFO: Train epoch 2154:   Loss: 569.0643 | r_Loss: 56.6049 | g_Loss: 255.5427 | l_Loss: 30.4971 | 
21-12-24 21:19:00.944 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:19:00.945 - INFO: Train epoch 2155:   Loss: 543.3545 | r_Loss: 54.1023 | g_Loss: 251.7685 | l_Loss: 21.0745 | 
21-12-24 21:20:14.725 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:20:14.726 - INFO: Train epoch 2156:   Loss: 598.1650 | r_Loss: 61.0127 | g_Loss: 253.7936 | l_Loss: 39.3081 | 
21-12-24 21:21:28.041 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:21:28.042 - INFO: Train epoch 2157:   Loss: 679.5854 | r_Loss: 78.0138 | g_Loss: 261.8476 | l_Loss: 27.6686 | 
21-12-24 21:22:41.456 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:22:41.457 - INFO: Train epoch 2158:   Loss: 509.4235 | r_Loss: 48.6873 | g_Loss: 238.9889 | l_Loss: 26.9980 | 
21-12-24 21:23:55.254 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:23:55.255 - INFO: Train epoch 2159:   Loss: 544.3661 | r_Loss: 51.7189 | g_Loss: 253.1107 | l_Loss: 32.6609 | 
21-12-24 21:25:45.299 - INFO: TEST:   PSNR_S: 45.9178 | PSNR_C: 37.9525 | 
21-12-24 21:25:45.300 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:25:45.301 - INFO: Train epoch 2160:   Loss: 584.2674 | r_Loss: 59.4333 | g_Loss: 260.3028 | l_Loss: 26.7979 | 
21-12-24 21:26:58.668 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:26:58.669 - INFO: Train epoch 2161:   Loss: 538.2529 | r_Loss: 53.0745 | g_Loss: 243.8108 | l_Loss: 29.0696 | 
21-12-24 21:28:12.361 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:28:12.362 - INFO: Train epoch 2162:   Loss: 548.1948 | r_Loss: 55.3505 | g_Loss: 239.3783 | l_Loss: 32.0643 | 
21-12-24 21:29:25.981 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:29:25.982 - INFO: Train epoch 2163:   Loss: 573.2271 | r_Loss: 58.2188 | g_Loss: 252.6660 | l_Loss: 29.4672 | 
21-12-24 21:30:39.603 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:30:39.603 - INFO: Train epoch 2164:   Loss: 568.6893 | r_Loss: 57.9410 | g_Loss: 244.0272 | l_Loss: 34.9569 | 
21-12-24 21:31:53.387 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:31:53.388 - INFO: Train epoch 2165:   Loss: 579.2773 | r_Loss: 60.1101 | g_Loss: 250.6473 | l_Loss: 28.0795 | 
21-12-24 21:33:07.198 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:33:07.199 - INFO: Train epoch 2166:   Loss: 593.2783 | r_Loss: 59.4033 | g_Loss: 258.0129 | l_Loss: 38.2488 | 
21-12-24 21:34:21.029 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:34:21.031 - INFO: Train epoch 2167:   Loss: 608.0112 | r_Loss: 64.8020 | g_Loss: 251.7587 | l_Loss: 32.2426 | 
21-12-24 21:35:34.842 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:35:34.843 - INFO: Train epoch 2168:   Loss: 547.3459 | r_Loss: 54.4931 | g_Loss: 244.5583 | l_Loss: 30.3222 | 
21-12-24 21:36:49.008 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:36:49.009 - INFO: Train epoch 2169:   Loss: 588.4928 | r_Loss: 60.1922 | g_Loss: 256.0623 | l_Loss: 31.4698 | 
21-12-24 21:38:02.822 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:38:02.824 - INFO: Train epoch 2170:   Loss: 529.8836 | r_Loss: 52.6310 | g_Loss: 239.5981 | l_Loss: 27.1307 | 
21-12-24 21:39:16.732 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:39:16.733 - INFO: Train epoch 2171:   Loss: 535.6014 | r_Loss: 53.9670 | g_Loss: 238.4299 | l_Loss: 27.3367 | 
21-12-24 21:40:30.320 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:40:30.322 - INFO: Train epoch 2172:   Loss: 556.3048 | r_Loss: 59.1984 | g_Loss: 232.0845 | l_Loss: 28.2283 | 
21-12-24 21:41:43.853 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:41:43.855 - INFO: Train epoch 2173:   Loss: 532.2913 | r_Loss: 52.9403 | g_Loss: 240.5537 | l_Loss: 27.0361 | 
21-12-24 21:42:57.258 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:42:57.260 - INFO: Train epoch 2174:   Loss: 562.1075 | r_Loss: 57.4236 | g_Loss: 245.0388 | l_Loss: 29.9509 | 
21-12-24 21:44:10.469 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:44:10.470 - INFO: Train epoch 2175:   Loss: 548.4500 | r_Loss: 57.3763 | g_Loss: 235.8689 | l_Loss: 25.6999 | 
21-12-24 21:45:23.852 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:45:23.854 - INFO: Train epoch 2176:   Loss: 563.7111 | r_Loss: 58.7508 | g_Loss: 236.6083 | l_Loss: 33.3487 | 
21-12-24 21:46:37.709 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:46:37.710 - INFO: Train epoch 2177:   Loss: 589.8760 | r_Loss: 62.5430 | g_Loss: 250.1051 | l_Loss: 27.0556 | 
21-12-24 21:47:51.465 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:47:51.466 - INFO: Train epoch 2178:   Loss: 561.9281 | r_Loss: 57.7367 | g_Loss: 241.2211 | l_Loss: 32.0233 | 
21-12-24 21:49:05.315 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:49:05.316 - INFO: Train epoch 2179:   Loss: 575.0350 | r_Loss: 59.4882 | g_Loss: 244.3447 | l_Loss: 33.2493 | 
21-12-24 21:50:18.718 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:50:18.719 - INFO: Train epoch 2180:   Loss: 585.6135 | r_Loss: 63.3769 | g_Loss: 235.9704 | l_Loss: 32.7587 | 
21-12-24 21:51:32.582 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:51:32.583 - INFO: Train epoch 2181:   Loss: 541.4172 | r_Loss: 53.0427 | g_Loss: 248.3192 | l_Loss: 27.8846 | 
21-12-24 21:52:46.093 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:52:46.094 - INFO: Train epoch 2182:   Loss: 589.6021 | r_Loss: 60.4910 | g_Loss: 254.7928 | l_Loss: 32.3542 | 
21-12-24 21:53:59.383 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:53:59.384 - INFO: Train epoch 2183:   Loss: 685.3559 | r_Loss: 81.8635 | g_Loss: 251.0409 | l_Loss: 24.9976 | 
21-12-24 21:55:13.444 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:55:13.445 - INFO: Train epoch 2184:   Loss: 550.1818 | r_Loss: 56.8561 | g_Loss: 237.1976 | l_Loss: 28.7039 | 
21-12-24 21:56:27.081 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:56:27.083 - INFO: Train epoch 2185:   Loss: 540.4189 | r_Loss: 54.6699 | g_Loss: 240.8385 | l_Loss: 26.2307 | 
21-12-24 21:57:40.820 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:57:40.821 - INFO: Train epoch 2186:   Loss: 520.5557 | r_Loss: 52.4013 | g_Loss: 227.3156 | l_Loss: 31.2335 | 
21-12-24 21:58:54.438 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 21:58:54.439 - INFO: Train epoch 2187:   Loss: 525.3087 | r_Loss: 51.6909 | g_Loss: 235.1048 | l_Loss: 31.7494 | 
21-12-24 22:00:07.681 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:00:07.682 - INFO: Train epoch 2188:   Loss: 535.2937 | r_Loss: 53.1692 | g_Loss: 233.3436 | l_Loss: 36.1042 | 
21-12-24 22:01:20.982 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:01:20.984 - INFO: Train epoch 2189:   Loss: 626.6628 | r_Loss: 66.9413 | g_Loss: 262.6578 | l_Loss: 29.2984 | 
21-12-24 22:03:10.074 - INFO: TEST:   PSNR_S: 45.9636 | PSNR_C: 38.2067 | 
21-12-24 22:03:10.075 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:03:10.075 - INFO: Train epoch 2190:   Loss: 547.6243 | r_Loss: 55.0541 | g_Loss: 234.4792 | l_Loss: 37.8746 | 
21-12-24 22:04:23.618 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:04:23.619 - INFO: Train epoch 2191:   Loss: 566.5976 | r_Loss: 58.4422 | g_Loss: 238.6080 | l_Loss: 35.7788 | 
21-12-24 22:05:37.065 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:05:37.067 - INFO: Train epoch 2192:   Loss: 552.9415 | r_Loss: 55.8960 | g_Loss: 241.1275 | l_Loss: 32.3338 | 
21-12-24 22:06:50.793 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:06:50.794 - INFO: Train epoch 2193:   Loss: 604.0481 | r_Loss: 66.1002 | g_Loss: 242.2749 | l_Loss: 31.2724 | 
21-12-24 22:08:04.251 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:08:04.252 - INFO: Train epoch 2194:   Loss: 567.7270 | r_Loss: 60.0358 | g_Loss: 237.0575 | l_Loss: 30.4906 | 
21-12-24 22:09:17.907 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:09:17.908 - INFO: Train epoch 2195:   Loss: 608.3099 | r_Loss: 67.9516 | g_Loss: 238.2721 | l_Loss: 30.2797 | 
21-12-24 22:10:31.395 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:10:31.396 - INFO: Train epoch 2196:   Loss: 733.5658 | r_Loss: 81.4677 | g_Loss: 291.4242 | l_Loss: 34.8032 | 
21-12-24 22:11:44.620 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:11:44.621 - INFO: Train epoch 2197:   Loss: 562.1504 | r_Loss: 59.1387 | g_Loss: 236.2972 | l_Loss: 30.1598 | 
21-12-24 22:12:58.032 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:12:58.032 - INFO: Train epoch 2198:   Loss: 522.5658 | r_Loss: 50.9258 | g_Loss: 240.7770 | l_Loss: 27.1600 | 
21-12-24 22:14:11.275 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:14:11.276 - INFO: Train epoch 2199:   Loss: 507.9292 | r_Loss: 49.8444 | g_Loss: 222.9874 | l_Loss: 35.7200 | 
21-12-24 22:15:24.382 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:15:24.383 - INFO: Train epoch 2200:   Loss: 525.8504 | r_Loss: 52.6209 | g_Loss: 237.3011 | l_Loss: 25.4446 | 
21-12-24 22:16:38.066 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:16:38.067 - INFO: Train epoch 2201:   Loss: 555.4937 | r_Loss: 56.6271 | g_Loss: 239.8141 | l_Loss: 32.5441 | 
21-12-24 22:17:51.596 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:17:51.597 - INFO: Train epoch 2202:   Loss: 9370.6059 | r_Loss: 1671.2400 | g_Loss: 909.2712 | l_Loss: 105.1348 | 
21-12-24 22:19:05.445 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:19:05.446 - INFO: Train epoch 2203:   Loss: 1082.3501 | r_Loss: 96.9667 | g_Loss: 527.4396 | l_Loss: 70.0770 | 
21-12-24 22:20:19.400 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:20:19.401 - INFO: Train epoch 2204:   Loss: 892.8647 | r_Loss: 78.3881 | g_Loss: 444.5592 | l_Loss: 56.3651 | 
21-12-24 22:21:32.940 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:21:32.941 - INFO: Train epoch 2205:   Loss: 847.7340 | r_Loss: 73.9008 | g_Loss: 428.0726 | l_Loss: 50.1573 | 
21-12-24 22:22:46.504 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:22:46.505 - INFO: Train epoch 2206:   Loss: 821.9043 | r_Loss: 72.1013 | g_Loss: 406.1403 | l_Loss: 55.2576 | 
21-12-24 22:23:59.916 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:23:59.917 - INFO: Train epoch 2207:   Loss: 781.5538 | r_Loss: 67.7019 | g_Loss: 397.2187 | l_Loss: 45.8255 | 
21-12-24 22:25:13.585 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:25:13.586 - INFO: Train epoch 2208:   Loss: 747.7819 | r_Loss: 63.8553 | g_Loss: 380.0820 | l_Loss: 48.4235 | 
21-12-24 22:26:27.654 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:26:27.655 - INFO: Train epoch 2209:   Loss: 736.9800 | r_Loss: 62.8040 | g_Loss: 372.5997 | l_Loss: 50.3601 | 
21-12-24 22:27:41.487 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:27:41.488 - INFO: Train epoch 2210:   Loss: 691.3281 | r_Loss: 61.1668 | g_Loss: 343.8013 | l_Loss: 41.6928 | 
21-12-24 22:28:55.649 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:28:55.650 - INFO: Train epoch 2211:   Loss: 683.6727 | r_Loss: 59.4943 | g_Loss: 343.9326 | l_Loss: 42.2685 | 
21-12-24 22:30:09.355 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:30:09.356 - INFO: Train epoch 2212:   Loss: 667.3518 | r_Loss: 58.8452 | g_Loss: 336.4959 | l_Loss: 36.6299 | 
21-12-24 22:31:23.069 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:31:23.070 - INFO: Train epoch 2213:   Loss: 628.3894 | r_Loss: 54.1896 | g_Loss: 319.9674 | l_Loss: 37.4739 | 
21-12-24 22:32:36.912 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:32:36.913 - INFO: Train epoch 2214:   Loss: 673.9013 | r_Loss: 59.2793 | g_Loss: 335.8018 | l_Loss: 41.7031 | 
21-12-24 22:33:50.570 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:33:50.571 - INFO: Train epoch 2215:   Loss: 612.5422 | r_Loss: 52.8346 | g_Loss: 310.9995 | l_Loss: 37.3695 | 
21-12-24 22:35:04.378 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:35:04.379 - INFO: Train epoch 2216:   Loss: 663.9732 | r_Loss: 58.1644 | g_Loss: 322.3055 | l_Loss: 50.8457 | 
21-12-24 22:36:17.961 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:36:17.962 - INFO: Train epoch 2217:   Loss: 620.7436 | r_Loss: 55.5886 | g_Loss: 303.0856 | l_Loss: 39.7149 | 
21-12-24 22:37:31.412 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:37:31.413 - INFO: Train epoch 2218:   Loss: 617.1009 | r_Loss: 53.7173 | g_Loss: 308.9065 | l_Loss: 39.6080 | 
21-12-24 22:38:44.807 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:38:44.808 - INFO: Train epoch 2219:   Loss: 628.9003 | r_Loss: 56.1106 | g_Loss: 316.2271 | l_Loss: 32.1205 | 
21-12-24 22:40:34.463 - INFO: TEST:   PSNR_S: 45.8428 | PSNR_C: 37.0659 | 
21-12-24 22:40:34.464 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:40:34.465 - INFO: Train epoch 2220:   Loss: 641.6720 | r_Loss: 57.3796 | g_Loss: 313.1102 | l_Loss: 41.6638 | 
21-12-24 22:41:47.866 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:41:47.867 - INFO: Train epoch 2221:   Loss: 584.5024 | r_Loss: 51.8279 | g_Loss: 292.5428 | l_Loss: 32.8201 | 
21-12-24 22:43:01.380 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:43:01.381 - INFO: Train epoch 2222:   Loss: 600.3762 | r_Loss: 54.3070 | g_Loss: 289.8842 | l_Loss: 38.9571 | 
21-12-24 22:44:14.917 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:44:14.918 - INFO: Train epoch 2223:   Loss: 615.2847 | r_Loss: 54.4142 | g_Loss: 304.9131 | l_Loss: 38.3005 | 
21-12-24 22:45:28.232 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:45:28.234 - INFO: Train epoch 2224:   Loss: 576.7264 | r_Loss: 53.2380 | g_Loss: 278.4628 | l_Loss: 32.0734 | 
21-12-24 22:46:41.769 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:46:41.770 - INFO: Train epoch 2225:   Loss: 600.9607 | r_Loss: 55.4335 | g_Loss: 285.1135 | l_Loss: 38.6799 | 
21-12-24 22:47:55.136 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:47:55.137 - INFO: Train epoch 2226:   Loss: 580.1297 | r_Loss: 54.6379 | g_Loss: 271.4629 | l_Loss: 35.4771 | 
21-12-24 22:49:08.486 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:49:08.487 - INFO: Train epoch 2227:   Loss: 562.3743 | r_Loss: 51.4520 | g_Loss: 270.9107 | l_Loss: 34.2035 | 
21-12-24 22:50:22.192 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:50:22.193 - INFO: Train epoch 2228:   Loss: 560.2322 | r_Loss: 50.5938 | g_Loss: 272.7748 | l_Loss: 34.4886 | 
21-12-24 22:51:35.566 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:51:35.567 - INFO: Train epoch 2229:   Loss: 563.3413 | r_Loss: 51.3419 | g_Loss: 272.2261 | l_Loss: 34.4056 | 
21-12-24 22:52:49.673 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:52:49.674 - INFO: Train epoch 2230:   Loss: 593.2982 | r_Loss: 56.6171 | g_Loss: 276.2645 | l_Loss: 33.9481 | 
21-12-24 22:54:03.362 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:54:03.364 - INFO: Train epoch 2231:   Loss: 558.3047 | r_Loss: 52.5051 | g_Loss: 265.9420 | l_Loss: 29.8370 | 
21-12-24 22:55:16.933 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:55:16.935 - INFO: Train epoch 2232:   Loss: 590.0099 | r_Loss: 56.6229 | g_Loss: 270.2928 | l_Loss: 36.6025 | 
21-12-24 22:56:30.319 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:56:30.320 - INFO: Train epoch 2233:   Loss: 517.4610 | r_Loss: 48.5732 | g_Loss: 241.0689 | l_Loss: 33.5259 | 
21-12-24 22:57:43.695 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:57:43.696 - INFO: Train epoch 2234:   Loss: 537.5602 | r_Loss: 49.2045 | g_Loss: 253.6910 | l_Loss: 37.8468 | 
21-12-24 22:58:57.238 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 22:58:57.239 - INFO: Train epoch 2235:   Loss: 561.6677 | r_Loss: 53.4126 | g_Loss: 260.1073 | l_Loss: 34.4973 | 
21-12-24 23:00:10.605 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:00:10.606 - INFO: Train epoch 2236:   Loss: 544.1911 | r_Loss: 52.4044 | g_Loss: 250.4940 | l_Loss: 31.6753 | 
21-12-24 23:01:24.215 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:01:24.216 - INFO: Train epoch 2237:   Loss: 563.1623 | r_Loss: 54.5182 | g_Loss: 258.7159 | l_Loss: 31.8554 | 
21-12-24 23:02:37.834 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:02:37.835 - INFO: Train epoch 2238:   Loss: 554.7508 | r_Loss: 53.4426 | g_Loss: 260.8233 | l_Loss: 26.7145 | 
21-12-24 23:03:51.352 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:03:51.353 - INFO: Train epoch 2239:   Loss: 533.6239 | r_Loss: 51.2934 | g_Loss: 246.9779 | l_Loss: 30.1789 | 
21-12-24 23:05:05.037 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:05:05.038 - INFO: Train epoch 2240:   Loss: 567.9228 | r_Loss: 55.4309 | g_Loss: 255.9483 | l_Loss: 34.8202 | 
21-12-24 23:06:18.314 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:06:18.315 - INFO: Train epoch 2241:   Loss: 580.2831 | r_Loss: 58.5438 | g_Loss: 251.1991 | l_Loss: 36.3653 | 
21-12-24 23:07:31.338 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:07:31.339 - INFO: Train epoch 2242:   Loss: 574.5751 | r_Loss: 56.6286 | g_Loss: 259.3903 | l_Loss: 32.0417 | 
21-12-24 23:08:45.132 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:08:45.134 - INFO: Train epoch 2243:   Loss: 575.7647 | r_Loss: 62.4061 | g_Loss: 236.8497 | l_Loss: 26.8846 | 
21-12-24 23:09:59.138 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:09:59.140 - INFO: Train epoch 2244:   Loss: 527.2879 | r_Loss: 49.7797 | g_Loss: 245.6156 | l_Loss: 32.7741 | 
21-12-24 23:11:12.590 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:11:12.591 - INFO: Train epoch 2245:   Loss: 555.4638 | r_Loss: 53.8722 | g_Loss: 255.0856 | l_Loss: 31.0173 | 
21-12-24 23:12:25.858 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:12:25.860 - INFO: Train epoch 2246:   Loss: 527.2573 | r_Loss: 51.8597 | g_Loss: 237.4236 | l_Loss: 30.5350 | 
21-12-24 23:13:39.245 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:13:39.247 - INFO: Train epoch 2247:   Loss: 523.5084 | r_Loss: 51.7867 | g_Loss: 237.2720 | l_Loss: 27.3031 | 
21-12-24 23:14:52.498 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:14:52.499 - INFO: Train epoch 2248:   Loss: 562.0823 | r_Loss: 55.1117 | g_Loss: 251.6608 | l_Loss: 34.8631 | 
21-12-24 23:16:06.257 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:16:06.257 - INFO: Train epoch 2249:   Loss: 525.1644 | r_Loss: 50.9943 | g_Loss: 238.9767 | l_Loss: 31.2159 | 
21-12-24 23:17:55.885 - INFO: TEST:   PSNR_S: 45.8980 | PSNR_C: 38.0806 | 
21-12-24 23:17:55.887 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:17:55.887 - INFO: Train epoch 2250:   Loss: 537.3012 | r_Loss: 54.8207 | g_Loss: 236.6396 | l_Loss: 26.5581 | 
21-12-24 23:19:09.461 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:19:09.462 - INFO: Train epoch 2251:   Loss: 552.0327 | r_Loss: 57.1658 | g_Loss: 236.8245 | l_Loss: 29.3795 | 
21-12-24 23:20:23.161 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:20:23.162 - INFO: Train epoch 2252:   Loss: 565.1949 | r_Loss: 55.4526 | g_Loss: 256.6073 | l_Loss: 31.3247 | 
21-12-24 23:21:37.190 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:21:37.191 - INFO: Train epoch 2253:   Loss: 516.6459 | r_Loss: 50.7900 | g_Loss: 235.5577 | l_Loss: 27.1383 | 
21-12-24 23:22:50.976 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:22:50.977 - INFO: Train epoch 2254:   Loss: 584.8931 | r_Loss: 63.2864 | g_Loss: 238.6480 | l_Loss: 29.8132 | 
21-12-24 23:24:04.789 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:24:04.791 - INFO: Train epoch 2255:   Loss: 524.7437 | r_Loss: 51.5452 | g_Loss: 240.0989 | l_Loss: 26.9188 | 
21-12-24 23:25:18.204 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:25:18.205 - INFO: Train epoch 2256:   Loss: 560.4452 | r_Loss: 53.8183 | g_Loss: 254.0767 | l_Loss: 37.2768 | 
21-12-24 23:26:31.845 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:26:31.846 - INFO: Train epoch 2257:   Loss: 548.5045 | r_Loss: 56.1922 | g_Loss: 241.9424 | l_Loss: 25.6012 | 
21-12-24 23:27:45.637 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:27:45.638 - INFO: Train epoch 2258:   Loss: 568.8508 | r_Loss: 56.8831 | g_Loss: 250.9516 | l_Loss: 33.4835 | 
21-12-24 23:28:59.260 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:28:59.261 - INFO: Train epoch 2259:   Loss: 515.3305 | r_Loss: 50.8720 | g_Loss: 233.2791 | l_Loss: 27.6914 | 
21-12-24 23:30:12.810 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:30:12.811 - INFO: Train epoch 2260:   Loss: 656.6171 | r_Loss: 78.3992 | g_Loss: 238.1589 | l_Loss: 26.4623 | 
21-12-24 23:31:26.125 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:31:26.125 - INFO: Train epoch 2261:   Loss: 529.3076 | r_Loss: 51.5866 | g_Loss: 239.9126 | l_Loss: 31.4623 | 
21-12-24 23:32:39.253 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:32:39.254 - INFO: Train epoch 2262:   Loss: 624.5541 | r_Loss: 68.5543 | g_Loss: 256.7944 | l_Loss: 24.9881 | 
21-12-24 23:33:52.593 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:33:52.594 - INFO: Train epoch 2263:   Loss: 499.3988 | r_Loss: 45.5909 | g_Loss: 239.5720 | l_Loss: 31.8725 | 
21-12-24 23:35:05.862 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:35:05.863 - INFO: Train epoch 2264:   Loss: 505.4967 | r_Loss: 48.0329 | g_Loss: 238.1155 | l_Loss: 27.2165 | 
21-12-24 23:36:19.913 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:36:19.914 - INFO: Train epoch 2265:   Loss: 531.9198 | r_Loss: 54.0208 | g_Loss: 238.0679 | l_Loss: 23.7480 | 
21-12-24 23:37:33.379 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:37:33.380 - INFO: Train epoch 2266:   Loss: 551.1444 | r_Loss: 53.6198 | g_Loss: 248.4166 | l_Loss: 34.6288 | 
21-12-24 23:38:47.015 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:38:47.016 - INFO: Train epoch 2267:   Loss: 563.3258 | r_Loss: 58.9211 | g_Loss: 240.8125 | l_Loss: 27.9077 | 
21-12-24 23:40:01.005 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:40:01.006 - INFO: Train epoch 2268:   Loss: 540.5093 | r_Loss: 56.0102 | g_Loss: 229.7371 | l_Loss: 30.7212 | 
21-12-24 23:41:14.506 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:41:14.507 - INFO: Train epoch 2269:   Loss: 549.7305 | r_Loss: 56.5312 | g_Loss: 240.3619 | l_Loss: 26.7126 | 
21-12-24 23:42:28.085 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:42:28.086 - INFO: Train epoch 2270:   Loss: 535.3517 | r_Loss: 56.1207 | g_Loss: 223.3306 | l_Loss: 31.4176 | 
21-12-24 23:43:41.540 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:43:41.541 - INFO: Train epoch 2271:   Loss: 548.0916 | r_Loss: 55.6452 | g_Loss: 237.6634 | l_Loss: 32.2023 | 
21-12-24 23:44:55.232 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:44:55.233 - INFO: Train epoch 2272:   Loss: 632.3427 | r_Loss: 65.9823 | g_Loss: 264.1156 | l_Loss: 38.3156 | 
21-12-24 23:46:08.847 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:46:08.848 - INFO: Train epoch 2273:   Loss: 571.3724 | r_Loss: 59.1456 | g_Loss: 239.2592 | l_Loss: 36.3852 | 
21-12-24 23:47:22.646 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:47:22.647 - INFO: Train epoch 2274:   Loss: 527.6513 | r_Loss: 53.1407 | g_Loss: 234.5072 | l_Loss: 27.4406 | 
21-12-24 23:48:35.819 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:48:35.820 - INFO: Train epoch 2275:   Loss: 546.1312 | r_Loss: 55.8121 | g_Loss: 234.2498 | l_Loss: 32.8211 | 
21-12-24 23:49:49.096 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:49:49.097 - INFO: Train epoch 2276:   Loss: 518.3216 | r_Loss: 51.3318 | g_Loss: 227.0802 | l_Loss: 34.5822 | 
21-12-24 23:51:02.063 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:51:02.064 - INFO: Train epoch 2277:   Loss: 526.9438 | r_Loss: 53.4114 | g_Loss: 227.3622 | l_Loss: 32.5245 | 
21-12-24 23:52:15.441 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:52:15.442 - INFO: Train epoch 2278:   Loss: 600.4945 | r_Loss: 65.8542 | g_Loss: 243.7953 | l_Loss: 27.4281 | 
21-12-24 23:53:28.843 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:53:28.844 - INFO: Train epoch 2279:   Loss: 556.3044 | r_Loss: 57.5616 | g_Loss: 242.9264 | l_Loss: 25.5698 | 
21-12-24 23:55:18.298 - INFO: TEST:   PSNR_S: 46.0362 | PSNR_C: 38.3627 | 
21-12-24 23:55:18.300 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:55:18.300 - INFO: Train epoch 2280:   Loss: 490.9463 | r_Loss: 47.2177 | g_Loss: 226.0017 | l_Loss: 28.8563 | 
21-12-24 23:56:31.701 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:56:31.702 - INFO: Train epoch 2281:   Loss: 551.1044 | r_Loss: 57.0590 | g_Loss: 236.5189 | l_Loss: 29.2906 | 
21-12-24 23:57:45.108 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:57:45.109 - INFO: Train epoch 2282:   Loss: 15554.4924 | r_Loss: 2878.5883 | g_Loss: 1027.4469 | l_Loss: 134.1048 | 
21-12-24 23:58:58.461 - INFO: Learning rate: 6.30957344480193e-06
21-12-24 23:58:58.462 - INFO: Train epoch 2283:   Loss: 1198.2839 | r_Loss: 114.8008 | g_Loss: 562.5998 | l_Loss: 61.6802 | 
21-12-25 00:00:11.974 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:00:11.975 - INFO: Train epoch 2284:   Loss: 981.8724 | r_Loss: 86.3036 | g_Loss: 489.1636 | l_Loss: 61.1907 | 
21-12-25 00:01:25.401 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:01:25.402 - INFO: Train epoch 2285:   Loss: 871.0357 | r_Loss: 73.4831 | g_Loss: 453.7753 | l_Loss: 49.8450 | 
21-12-25 00:02:38.419 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:02:38.420 - INFO: Train epoch 2286:   Loss: 909.6067 | r_Loss: 77.4449 | g_Loss: 470.1583 | l_Loss: 52.2241 | 
21-12-25 00:03:51.422 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:03:51.422 - INFO: Train epoch 2287:   Loss: 812.1475 | r_Loss: 67.3929 | g_Loss: 421.2068 | l_Loss: 53.9761 | 
21-12-25 00:05:05.105 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:05:05.107 - INFO: Train epoch 2288:   Loss: 771.8752 | r_Loss: 61.9152 | g_Loss: 411.0179 | l_Loss: 51.2811 | 
21-12-25 00:06:18.350 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:06:18.352 - INFO: Train epoch 2289:   Loss: 753.2390 | r_Loss: 59.4849 | g_Loss: 409.7472 | l_Loss: 46.0674 | 
21-12-25 00:07:31.663 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:07:31.663 - INFO: Train epoch 2290:   Loss: 741.5206 | r_Loss: 59.2353 | g_Loss: 399.4246 | l_Loss: 45.9194 | 
21-12-25 00:08:44.928 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:08:44.929 - INFO: Train epoch 2291:   Loss: 754.2676 | r_Loss: 62.7355 | g_Loss: 391.9558 | l_Loss: 48.6341 | 
21-12-25 00:09:57.958 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:09:57.959 - INFO: Train epoch 2292:   Loss: 704.1602 | r_Loss: 59.5543 | g_Loss: 362.7389 | l_Loss: 43.6496 | 
21-12-25 00:11:11.378 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:11:11.379 - INFO: Train epoch 2293:   Loss: 726.4703 | r_Loss: 60.7820 | g_Loss: 377.1049 | l_Loss: 45.4555 | 
21-12-25 00:12:24.846 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:12:24.848 - INFO: Train epoch 2294:   Loss: 663.6293 | r_Loss: 54.3206 | g_Loss: 340.6436 | l_Loss: 51.3829 | 
21-12-25 00:13:37.707 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:13:37.708 - INFO: Train epoch 2295:   Loss: 660.2029 | r_Loss: 53.6791 | g_Loss: 342.7833 | l_Loss: 49.0244 | 
21-12-25 00:14:50.712 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:14:50.714 - INFO: Train epoch 2296:   Loss: 700.9518 | r_Loss: 60.8736 | g_Loss: 349.7464 | l_Loss: 46.8376 | 
21-12-25 00:16:04.099 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:16:04.100 - INFO: Train epoch 2297:   Loss: 608.3557 | r_Loss: 48.7966 | g_Loss: 325.8371 | l_Loss: 38.5356 | 
21-12-25 00:17:17.544 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:17:17.545 - INFO: Train epoch 2298:   Loss: 631.1877 | r_Loss: 52.9217 | g_Loss: 332.0702 | l_Loss: 34.5092 | 
21-12-25 00:18:31.117 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:18:31.118 - INFO: Train epoch 2299:   Loss: 632.1683 | r_Loss: 53.0559 | g_Loss: 326.9994 | l_Loss: 39.8895 | 
21-12-25 00:19:44.227 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:19:44.228 - INFO: Train epoch 2300:   Loss: 593.7039 | r_Loss: 47.4100 | g_Loss: 315.3421 | l_Loss: 41.3120 | 
21-12-25 00:20:57.998 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:20:57.999 - INFO: Train epoch 2301:   Loss: 638.5890 | r_Loss: 54.6725 | g_Loss: 319.4002 | l_Loss: 45.8262 | 
21-12-25 00:22:11.478 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:22:11.479 - INFO: Train epoch 2302:   Loss: 602.2284 | r_Loss: 51.1254 | g_Loss: 313.5902 | l_Loss: 33.0111 | 
21-12-25 00:23:24.815 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:23:24.817 - INFO: Train epoch 2303:   Loss: 617.5701 | r_Loss: 52.0699 | g_Loss: 313.2749 | l_Loss: 43.9456 | 
21-12-25 00:24:38.731 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:24:38.732 - INFO: Train epoch 2304:   Loss: 578.2340 | r_Loss: 47.9656 | g_Loss: 302.8288 | l_Loss: 35.5773 | 
21-12-25 00:25:52.188 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:25:52.189 - INFO: Train epoch 2305:   Loss: 574.3229 | r_Loss: 49.0097 | g_Loss: 296.4165 | l_Loss: 32.8581 | 
21-12-25 00:27:05.917 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:27:05.918 - INFO: Train epoch 2306:   Loss: 584.3216 | r_Loss: 49.0761 | g_Loss: 299.2289 | l_Loss: 39.7121 | 
21-12-25 00:28:19.179 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:28:19.180 - INFO: Train epoch 2307:   Loss: 581.7638 | r_Loss: 51.0953 | g_Loss: 290.6789 | l_Loss: 35.6085 | 
21-12-25 00:29:32.480 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:29:32.481 - INFO: Train epoch 2308:   Loss: 593.0509 | r_Loss: 50.6168 | g_Loss: 298.0807 | l_Loss: 41.8864 | 
21-12-25 00:30:45.980 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:30:45.981 - INFO: Train epoch 2309:   Loss: 543.5585 | r_Loss: 44.6192 | g_Loss: 286.3366 | l_Loss: 34.1259 | 
21-12-25 00:32:35.389 - INFO: TEST:   PSNR_S: 46.1986 | PSNR_C: 37.1016 | 
21-12-25 00:32:35.390 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:32:35.391 - INFO: Train epoch 2310:   Loss: 555.2208 | r_Loss: 47.7247 | g_Loss: 282.1182 | l_Loss: 34.4790 | 
21-12-25 00:33:49.347 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:33:49.348 - INFO: Train epoch 2311:   Loss: 587.5638 | r_Loss: 51.1533 | g_Loss: 293.3883 | l_Loss: 38.4090 | 
21-12-25 00:35:02.716 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:35:02.718 - INFO: Train epoch 2312:   Loss: 565.1783 | r_Loss: 47.4908 | g_Loss: 285.5307 | l_Loss: 42.1935 | 
21-12-25 00:36:16.415 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:36:16.416 - INFO: Train epoch 2313:   Loss: 612.9824 | r_Loss: 56.1397 | g_Loss: 298.5032 | l_Loss: 33.7805 | 
21-12-25 00:37:30.063 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:37:30.065 - INFO: Train epoch 2314:   Loss: 593.2221 | r_Loss: 52.4669 | g_Loss: 291.9922 | l_Loss: 38.8954 | 
21-12-25 00:38:43.708 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:38:43.709 - INFO: Train epoch 2315:   Loss: 599.6309 | r_Loss: 55.7542 | g_Loss: 287.9778 | l_Loss: 32.8822 | 
21-12-25 00:39:57.598 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:39:57.599 - INFO: Train epoch 2316:   Loss: 547.5635 | r_Loss: 47.7261 | g_Loss: 275.9639 | l_Loss: 32.9689 | 
21-12-25 00:41:11.207 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:41:11.208 - INFO: Train epoch 2317:   Loss: 537.9161 | r_Loss: 47.4418 | g_Loss: 269.7167 | l_Loss: 30.9902 | 
21-12-25 00:42:24.926 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:42:24.928 - INFO: Train epoch 2318:   Loss: 597.1974 | r_Loss: 53.7531 | g_Loss: 289.7165 | l_Loss: 38.7156 | 
21-12-25 00:43:38.558 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:43:38.559 - INFO: Train epoch 2319:   Loss: 539.3552 | r_Loss: 47.6779 | g_Loss: 270.5255 | l_Loss: 30.4402 | 
21-12-25 00:44:52.308 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:44:52.309 - INFO: Train epoch 2320:   Loss: 574.9428 | r_Loss: 53.8114 | g_Loss: 271.4746 | l_Loss: 34.4113 | 
21-12-25 00:46:05.654 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:46:05.655 - INFO: Train epoch 2321:   Loss: 549.3258 | r_Loss: 48.6596 | g_Loss: 270.5582 | l_Loss: 35.4696 | 
21-12-25 00:47:19.147 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:47:19.149 - INFO: Train epoch 2322:   Loss: 558.1726 | r_Loss: 51.0731 | g_Loss: 270.2212 | l_Loss: 32.5858 | 
21-12-25 00:48:32.808 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:48:32.809 - INFO: Train epoch 2323:   Loss: 547.3787 | r_Loss: 51.4249 | g_Loss: 258.9902 | l_Loss: 31.2639 | 
21-12-25 00:49:46.563 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:49:46.565 - INFO: Train epoch 2324:   Loss: 535.1498 | r_Loss: 47.4887 | g_Loss: 261.5795 | l_Loss: 36.1269 | 
21-12-25 00:51:00.072 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:51:00.072 - INFO: Train epoch 2325:   Loss: 538.3331 | r_Loss: 48.9646 | g_Loss: 262.0272 | l_Loss: 31.4828 | 
21-12-25 00:52:13.929 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:52:13.930 - INFO: Train epoch 2326:   Loss: 551.3453 | r_Loss: 49.5943 | g_Loss: 265.0771 | l_Loss: 38.2965 | 
21-12-25 00:53:27.530 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:53:27.532 - INFO: Train epoch 2327:   Loss: 582.5015 | r_Loss: 56.8783 | g_Loss: 268.8203 | l_Loss: 29.2897 | 
21-12-25 00:54:41.391 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:54:41.393 - INFO: Train epoch 2328:   Loss: 557.3595 | r_Loss: 51.9218 | g_Loss: 271.0646 | l_Loss: 26.6862 | 
21-12-25 00:55:54.644 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:55:54.645 - INFO: Train epoch 2329:   Loss: 529.8472 | r_Loss: 48.3402 | g_Loss: 259.8088 | l_Loss: 28.3373 | 
21-12-25 00:57:07.764 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:57:07.764 - INFO: Train epoch 2330:   Loss: 569.4316 | r_Loss: 54.7389 | g_Loss: 259.8483 | l_Loss: 35.8886 | 
21-12-25 00:58:21.210 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:58:21.212 - INFO: Train epoch 2331:   Loss: 555.7792 | r_Loss: 52.2239 | g_Loss: 267.2934 | l_Loss: 27.3662 | 
21-12-25 00:59:34.389 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 00:59:34.390 - INFO: Train epoch 2332:   Loss: 540.7391 | r_Loss: 49.9480 | g_Loss: 258.9018 | l_Loss: 32.0973 | 
21-12-25 01:00:47.800 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:00:47.800 - INFO: Train epoch 2333:   Loss: 526.3609 | r_Loss: 49.4269 | g_Loss: 245.5421 | l_Loss: 33.6842 | 
21-12-25 01:02:01.282 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:02:01.283 - INFO: Train epoch 2334:   Loss: 571.5211 | r_Loss: 56.7671 | g_Loss: 256.0060 | l_Loss: 31.6793 | 
21-12-25 01:03:14.767 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:03:14.769 - INFO: Train epoch 2335:   Loss: 548.4768 | r_Loss: 50.5715 | g_Loss: 256.1366 | l_Loss: 39.4829 | 
21-12-25 01:04:28.541 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:04:28.542 - INFO: Train epoch 2336:   Loss: 515.4724 | r_Loss: 49.8374 | g_Loss: 239.5876 | l_Loss: 26.6977 | 
21-12-25 01:05:41.985 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:05:41.986 - INFO: Train epoch 2337:   Loss: 526.6212 | r_Loss: 50.2350 | g_Loss: 245.6817 | l_Loss: 29.7647 | 
21-12-25 01:06:55.431 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:06:55.432 - INFO: Train epoch 2338:   Loss: 537.1566 | r_Loss: 51.7775 | g_Loss: 248.6192 | l_Loss: 29.6498 | 
21-12-25 01:08:08.964 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:08:08.965 - INFO: Train epoch 2339:   Loss: 582.4573 | r_Loss: 56.8437 | g_Loss: 265.0380 | l_Loss: 33.2006 | 
21-12-25 01:09:58.495 - INFO: TEST:   PSNR_S: 46.0104 | PSNR_C: 38.0676 | 
21-12-25 01:09:58.497 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:09:58.498 - INFO: Train epoch 2340:   Loss: 517.0374 | r_Loss: 49.1681 | g_Loss: 242.8224 | l_Loss: 28.3744 | 
21-12-25 01:11:12.224 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:11:12.225 - INFO: Train epoch 2341:   Loss: 506.4868 | r_Loss: 48.0991 | g_Loss: 237.0196 | l_Loss: 28.9719 | 
21-12-25 01:12:25.665 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:12:25.666 - INFO: Train epoch 2342:   Loss: 556.9807 | r_Loss: 56.8940 | g_Loss: 246.1493 | l_Loss: 26.3613 | 
21-12-25 01:13:38.825 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:13:38.826 - INFO: Train epoch 2343:   Loss: 558.9902 | r_Loss: 55.7772 | g_Loss: 253.5045 | l_Loss: 26.5996 | 
21-12-25 01:14:52.224 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:14:52.225 - INFO: Train epoch 2344:   Loss: 560.5134 | r_Loss: 54.5148 | g_Loss: 254.4769 | l_Loss: 33.4627 | 
21-12-25 01:16:05.582 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:16:05.583 - INFO: Train epoch 2345:   Loss: 515.2969 | r_Loss: 50.3586 | g_Loss: 227.4313 | l_Loss: 36.0728 | 
21-12-25 01:17:18.613 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:17:18.614 - INFO: Train epoch 2346:   Loss: 548.7055 | r_Loss: 53.1401 | g_Loss: 253.3370 | l_Loss: 29.6683 | 
21-12-25 01:18:31.925 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:18:31.927 - INFO: Train epoch 2347:   Loss: 545.9482 | r_Loss: 57.0997 | g_Loss: 237.9592 | l_Loss: 22.4907 | 
21-12-25 01:19:45.597 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:19:45.598 - INFO: Train epoch 2348:   Loss: 545.1905 | r_Loss: 52.8707 | g_Loss: 245.8277 | l_Loss: 35.0092 | 
21-12-25 01:20:58.896 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:20:58.898 - INFO: Train epoch 2349:   Loss: 580.4621 | r_Loss: 61.8283 | g_Loss: 243.6077 | l_Loss: 27.7127 | 
21-12-25 01:22:12.648 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:22:12.649 - INFO: Train epoch 2350:   Loss: 528.6869 | r_Loss: 53.3037 | g_Loss: 235.2375 | l_Loss: 26.9309 | 
21-12-25 01:23:26.391 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:23:26.392 - INFO: Train epoch 2351:   Loss: 521.1472 | r_Loss: 50.3607 | g_Loss: 238.1780 | l_Loss: 31.1655 | 
21-12-25 01:24:39.760 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:24:39.761 - INFO: Train epoch 2352:   Loss: 517.1468 | r_Loss: 51.2368 | g_Loss: 235.0523 | l_Loss: 25.9107 | 
21-12-25 01:25:52.814 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:25:52.815 - INFO: Train epoch 2353:   Loss: 509.4376 | r_Loss: 51.5281 | g_Loss: 228.1673 | l_Loss: 23.6301 | 
21-12-25 01:27:06.145 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:27:06.147 - INFO: Train epoch 2354:   Loss: 554.5674 | r_Loss: 55.2176 | g_Loss: 245.7976 | l_Loss: 32.6820 | 
21-12-25 01:28:19.828 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:28:19.829 - INFO: Train epoch 2355:   Loss: 521.4647 | r_Loss: 52.4332 | g_Loss: 225.3365 | l_Loss: 33.9623 | 
21-12-25 01:29:33.026 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:29:33.028 - INFO: Train epoch 2356:   Loss: 529.6195 | r_Loss: 52.1303 | g_Loss: 243.1336 | l_Loss: 25.8343 | 
21-12-25 01:30:46.598 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:30:46.599 - INFO: Train epoch 2357:   Loss: 502.4849 | r_Loss: 49.9504 | g_Loss: 224.0133 | l_Loss: 28.7194 | 
21-12-25 01:32:00.213 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:32:00.215 - INFO: Train epoch 2358:   Loss: 628.0449 | r_Loss: 69.8588 | g_Loss: 246.0062 | l_Loss: 32.7445 | 
21-12-25 01:33:13.908 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:33:13.909 - INFO: Train epoch 2359:   Loss: 526.6016 | r_Loss: 52.9491 | g_Loss: 235.4561 | l_Loss: 26.3997 | 
21-12-25 01:34:27.627 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:34:27.628 - INFO: Train epoch 2360:   Loss: 542.9046 | r_Loss: 55.9923 | g_Loss: 238.8892 | l_Loss: 24.0539 | 
21-12-25 01:35:40.886 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:35:40.887 - INFO: Train epoch 2361:   Loss: 529.8635 | r_Loss: 54.6967 | g_Loss: 226.1261 | l_Loss: 30.2537 | 
21-12-25 01:36:54.292 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:36:54.293 - INFO: Train epoch 2362:   Loss: 520.6398 | r_Loss: 53.7164 | g_Loss: 229.8283 | l_Loss: 22.2295 | 
21-12-25 01:38:07.580 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:38:07.581 - INFO: Train epoch 2363:   Loss: 525.8970 | r_Loss: 53.6390 | g_Loss: 229.5370 | l_Loss: 28.1652 | 
21-12-25 01:39:20.971 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:39:20.972 - INFO: Train epoch 2364:   Loss: 628.1777 | r_Loss: 70.8524 | g_Loss: 244.6956 | l_Loss: 29.2201 | 
21-12-25 01:40:34.465 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:40:34.466 - INFO: Train epoch 2365:   Loss: 514.8101 | r_Loss: 52.3690 | g_Loss: 221.2188 | l_Loss: 31.7462 | 
21-12-25 01:41:47.981 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:41:47.982 - INFO: Train epoch 2366:   Loss: 511.4810 | r_Loss: 51.0970 | g_Loss: 225.1586 | l_Loss: 30.8375 | 
21-12-25 01:43:01.819 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:43:01.821 - INFO: Train epoch 2367:   Loss: 541.2529 | r_Loss: 55.6238 | g_Loss: 232.7183 | l_Loss: 30.4156 | 
21-12-25 01:44:15.364 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:44:15.365 - INFO: Train epoch 2368:   Loss: 570.0276 | r_Loss: 62.1307 | g_Loss: 234.1528 | l_Loss: 25.2213 | 
21-12-25 01:45:28.745 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:45:28.746 - INFO: Train epoch 2369:   Loss: 503.1099 | r_Loss: 50.6191 | g_Loss: 224.2231 | l_Loss: 25.7915 | 
21-12-25 01:47:18.162 - INFO: TEST:   PSNR_S: 46.2189 | PSNR_C: 38.3961 | 
21-12-25 01:47:18.164 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:47:18.164 - INFO: Train epoch 2370:   Loss: 684.9937 | r_Loss: 67.6447 | g_Loss: 308.7837 | l_Loss: 37.9868 | 
21-12-25 01:48:31.649 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:48:31.650 - INFO: Train epoch 2371:   Loss: 494.8132 | r_Loss: 46.9838 | g_Loss: 226.0647 | l_Loss: 33.8297 | 
21-12-25 01:49:45.109 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:49:45.110 - INFO: Train epoch 2372:   Loss: 528.4147 | r_Loss: 53.0512 | g_Loss: 227.7566 | l_Loss: 35.4022 | 
21-12-25 01:50:58.565 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:50:58.566 - INFO: Train epoch 2373:   Loss: 564.7626 | r_Loss: 62.4788 | g_Loss: 224.7520 | l_Loss: 27.6164 | 
21-12-25 01:52:11.741 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:52:11.742 - INFO: Train epoch 2374:   Loss: 511.6679 | r_Loss: 51.7063 | g_Loss: 221.3911 | l_Loss: 31.7452 | 
21-12-25 01:53:25.447 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:53:25.448 - INFO: Train epoch 2375:   Loss: 601.4907 | r_Loss: 66.9559 | g_Loss: 237.9568 | l_Loss: 28.7544 | 
21-12-25 01:54:38.691 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:54:38.692 - INFO: Train epoch 2376:   Loss: 534.5712 | r_Loss: 54.6470 | g_Loss: 233.9944 | l_Loss: 27.3416 | 
21-12-25 01:55:52.028 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:55:52.029 - INFO: Train epoch 2377:   Loss: 504.3491 | r_Loss: 51.2720 | g_Loss: 218.4094 | l_Loss: 29.5798 | 
21-12-25 01:57:05.291 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:57:05.292 - INFO: Train epoch 2378:   Loss: 500.0047 | r_Loss: 51.7101 | g_Loss: 217.2800 | l_Loss: 24.1742 | 
21-12-25 01:58:18.679 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:58:18.680 - INFO: Train epoch 2379:   Loss: 543.4469 | r_Loss: 57.2796 | g_Loss: 226.4684 | l_Loss: 30.5807 | 
21-12-25 01:59:32.577 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 01:59:32.578 - INFO: Train epoch 2380:   Loss: 17131.7992 | r_Loss: 3139.5422 | g_Loss: 1281.5427 | l_Loss: 152.5447 | 
21-12-25 02:00:45.984 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:00:45.985 - INFO: Train epoch 2381:   Loss: 1354.0727 | r_Loss: 148.4921 | g_Loss: 545.0004 | l_Loss: 66.6116 | 
21-12-25 02:01:59.383 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:01:59.384 - INFO: Train epoch 2382:   Loss: 1093.5193 | r_Loss: 118.3370 | g_Loss: 450.7965 | l_Loss: 51.0379 | 
21-12-25 02:03:13.325 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:03:13.326 - INFO: Train epoch 2383:   Loss: 992.9677 | r_Loss: 102.1385 | g_Loss: 426.4975 | l_Loss: 55.7777 | 
21-12-25 02:04:26.901 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:04:26.902 - INFO: Train epoch 2384:   Loss: 936.2768 | r_Loss: 94.5277 | g_Loss: 414.4832 | l_Loss: 49.1550 | 
21-12-25 02:05:40.542 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:05:40.543 - INFO: Train epoch 2385:   Loss: 927.6883 | r_Loss: 92.5370 | g_Loss: 407.4944 | l_Loss: 57.5088 | 
21-12-25 02:06:53.844 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:06:53.845 - INFO: Train epoch 2386:   Loss: 865.9554 | r_Loss: 83.9064 | g_Loss: 397.3185 | l_Loss: 49.1048 | 
21-12-25 02:08:07.329 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:08:07.330 - INFO: Train epoch 2387:   Loss: 804.4538 | r_Loss: 77.2732 | g_Loss: 372.0340 | l_Loss: 46.0536 | 
21-12-25 02:09:20.984 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:09:20.985 - INFO: Train epoch 2388:   Loss: 775.2244 | r_Loss: 73.7620 | g_Loss: 369.1002 | l_Loss: 37.3143 | 
21-12-25 02:10:34.228 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:10:34.229 - INFO: Train epoch 2389:   Loss: 752.0492 | r_Loss: 70.0609 | g_Loss: 359.4300 | l_Loss: 42.3147 | 
21-12-25 02:11:47.588 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:11:47.589 - INFO: Train epoch 2390:   Loss: 807.1253 | r_Loss: 76.1131 | g_Loss: 376.4182 | l_Loss: 50.1417 | 
21-12-25 02:13:00.806 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:13:00.807 - INFO: Train epoch 2391:   Loss: 734.2572 | r_Loss: 68.2701 | g_Loss: 347.7280 | l_Loss: 45.1789 | 
21-12-25 02:14:14.519 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:14:14.520 - INFO: Train epoch 2392:   Loss: 726.2777 | r_Loss: 66.3163 | g_Loss: 351.7452 | l_Loss: 42.9510 | 
21-12-25 02:15:27.741 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:15:27.742 - INFO: Train epoch 2393:   Loss: 724.3739 | r_Loss: 65.6116 | g_Loss: 344.6818 | l_Loss: 51.6343 | 
21-12-25 02:16:40.854 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:16:40.855 - INFO: Train epoch 2394:   Loss: 710.3021 | r_Loss: 63.4905 | g_Loss: 349.3163 | l_Loss: 43.5332 | 
21-12-25 02:17:53.861 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:17:53.862 - INFO: Train epoch 2395:   Loss: 683.6789 | r_Loss: 63.2281 | g_Loss: 330.0240 | l_Loss: 37.5146 | 
21-12-25 02:19:06.879 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:19:06.880 - INFO: Train epoch 2396:   Loss: 650.8397 | r_Loss: 57.9640 | g_Loss: 322.6667 | l_Loss: 38.3530 | 
21-12-25 02:20:19.583 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:20:19.583 - INFO: Train epoch 2397:   Loss: 664.6895 | r_Loss: 59.4739 | g_Loss: 324.9543 | l_Loss: 42.3659 | 
21-12-25 02:21:32.147 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:21:32.148 - INFO: Train epoch 2398:   Loss: 653.5669 | r_Loss: 59.6754 | g_Loss: 317.2515 | l_Loss: 37.9386 | 
21-12-25 02:22:44.218 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:22:44.219 - INFO: Train epoch 2399:   Loss: 642.1230 | r_Loss: 57.2631 | g_Loss: 314.2686 | l_Loss: 41.5388 | 
21-12-25 02:24:30.730 - INFO: TEST:   PSNR_S: 45.2787 | PSNR_C: 36.7859 | 
21-12-25 02:24:30.731 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:24:30.731 - INFO: Train epoch 2400:   Loss: 654.0469 | r_Loss: 59.5798 | g_Loss: 312.9599 | l_Loss: 43.1881 | 
21-12-25 02:25:42.957 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:25:42.958 - INFO: Train epoch 2401:   Loss: 667.6104 | r_Loss: 59.3684 | g_Loss: 320.3366 | l_Loss: 50.4320 | 
21-12-25 02:26:55.028 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:26:55.028 - INFO: Train epoch 2402:   Loss: 639.4166 | r_Loss: 58.8107 | g_Loss: 312.6307 | l_Loss: 32.7326 | 
21-12-25 02:28:06.870 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:28:06.871 - INFO: Train epoch 2403:   Loss: 594.1779 | r_Loss: 53.5819 | g_Loss: 292.2786 | l_Loss: 33.9896 | 
21-12-25 02:29:18.790 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:29:18.791 - INFO: Train epoch 2404:   Loss: 644.9507 | r_Loss: 57.1595 | g_Loss: 321.2288 | l_Loss: 37.9246 | 
21-12-25 02:30:30.656 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:30:30.657 - INFO: Train epoch 2405:   Loss: 579.7712 | r_Loss: 51.3926 | g_Loss: 292.3069 | l_Loss: 30.5011 | 
21-12-25 02:31:42.571 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:31:42.572 - INFO: Train epoch 2406:   Loss: 618.8758 | r_Loss: 55.1945 | g_Loss: 299.7042 | l_Loss: 43.1990 | 
21-12-25 02:32:54.347 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:32:54.347 - INFO: Train epoch 2407:   Loss: 610.3457 | r_Loss: 56.6872 | g_Loss: 291.2435 | l_Loss: 35.6663 | 
21-12-25 02:34:05.992 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:34:05.993 - INFO: Train epoch 2408:   Loss: 587.7059 | r_Loss: 51.3578 | g_Loss: 289.5920 | l_Loss: 41.3251 | 
21-12-25 02:35:17.690 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:35:17.691 - INFO: Train epoch 2409:   Loss: 579.8854 | r_Loss: 52.9739 | g_Loss: 278.9579 | l_Loss: 36.0582 | 
21-12-25 02:36:29.471 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:36:29.472 - INFO: Train epoch 2410:   Loss: 544.1614 | r_Loss: 47.5717 | g_Loss: 272.5361 | l_Loss: 33.7670 | 
21-12-25 02:37:41.115 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:37:41.116 - INFO: Train epoch 2411:   Loss: 624.0030 | r_Loss: 57.9281 | g_Loss: 299.1735 | l_Loss: 35.1889 | 
21-12-25 02:38:52.934 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:38:52.935 - INFO: Train epoch 2412:   Loss: 579.6454 | r_Loss: 52.7104 | g_Loss: 279.1813 | l_Loss: 36.9121 | 
21-12-25 02:40:04.854 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:40:04.854 - INFO: Train epoch 2413:   Loss: 582.4074 | r_Loss: 53.0019 | g_Loss: 281.6038 | l_Loss: 35.7939 | 
21-12-25 02:41:16.615 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:41:16.616 - INFO: Train epoch 2414:   Loss: 565.8019 | r_Loss: 51.5643 | g_Loss: 274.3907 | l_Loss: 33.5898 | 
21-12-25 02:42:28.478 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:42:28.479 - INFO: Train epoch 2415:   Loss: 549.3623 | r_Loss: 50.1367 | g_Loss: 261.7040 | l_Loss: 36.9748 | 
21-12-25 02:43:40.096 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:43:40.097 - INFO: Train epoch 2416:   Loss: 594.7902 | r_Loss: 56.1263 | g_Loss: 281.6216 | l_Loss: 32.5372 | 
21-12-25 02:44:51.694 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:44:51.695 - INFO: Train epoch 2417:   Loss: 564.3811 | r_Loss: 53.3273 | g_Loss: 268.5734 | l_Loss: 29.1712 | 
21-12-25 02:46:03.241 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:46:03.242 - INFO: Train epoch 2418:   Loss: 525.3645 | r_Loss: 47.3125 | g_Loss: 260.3907 | l_Loss: 28.4116 | 
21-12-25 02:47:14.830 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:47:14.831 - INFO: Train epoch 2419:   Loss: 531.3112 | r_Loss: 47.7483 | g_Loss: 261.5718 | l_Loss: 30.9979 | 
21-12-25 02:48:26.317 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:48:26.318 - INFO: Train epoch 2420:   Loss: 541.5926 | r_Loss: 49.0787 | g_Loss: 260.0481 | l_Loss: 36.1512 | 
21-12-25 02:49:37.822 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:49:37.823 - INFO: Train epoch 2421:   Loss: 576.4315 | r_Loss: 55.9181 | g_Loss: 268.2088 | l_Loss: 28.6324 | 
21-12-25 02:50:49.361 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:50:49.361 - INFO: Train epoch 2422:   Loss: 547.1382 | r_Loss: 51.7960 | g_Loss: 259.1356 | l_Loss: 29.0225 | 
21-12-25 02:52:00.878 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:52:00.879 - INFO: Train epoch 2423:   Loss: 593.3767 | r_Loss: 56.3466 | g_Loss: 277.9003 | l_Loss: 33.7436 | 
21-12-25 02:53:12.342 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:53:12.343 - INFO: Train epoch 2424:   Loss: 533.5651 | r_Loss: 48.5347 | g_Loss: 258.7306 | l_Loss: 32.1610 | 
21-12-25 02:54:24.059 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:54:24.060 - INFO: Train epoch 2425:   Loss: 557.9335 | r_Loss: 51.8688 | g_Loss: 260.4701 | l_Loss: 38.1194 | 
21-12-25 02:55:35.929 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:55:35.930 - INFO: Train epoch 2426:   Loss: 522.1577 | r_Loss: 49.3278 | g_Loss: 247.4121 | l_Loss: 28.1064 | 
21-12-25 02:56:47.401 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:56:47.402 - INFO: Train epoch 2427:   Loss: 539.6482 | r_Loss: 53.3056 | g_Loss: 248.9155 | l_Loss: 24.2049 | 
21-12-25 02:57:59.142 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:57:59.143 - INFO: Train epoch 2428:   Loss: 564.6049 | r_Loss: 52.8897 | g_Loss: 262.0299 | l_Loss: 38.1262 | 
21-12-25 02:59:10.645 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 02:59:10.645 - INFO: Train epoch 2429:   Loss: 555.5662 | r_Loss: 53.8182 | g_Loss: 257.8520 | l_Loss: 28.6233 | 
21-12-25 03:00:55.816 - INFO: TEST:   PSNR_S: 46.2312 | PSNR_C: 37.8948 | 
21-12-25 03:00:55.817 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:00:55.818 - INFO: Train epoch 2430:   Loss: 532.3445 | r_Loss: 50.5117 | g_Loss: 249.3390 | l_Loss: 30.4469 | 
21-12-25 03:02:07.336 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:02:07.337 - INFO: Train epoch 2431:   Loss: 514.0337 | r_Loss: 48.7943 | g_Loss: 242.5386 | l_Loss: 27.5235 | 
21-12-25 03:03:18.788 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:03:18.789 - INFO: Train epoch 2432:   Loss: 578.6062 | r_Loss: 58.4545 | g_Loss: 257.0507 | l_Loss: 29.2828 | 
21-12-25 03:04:30.247 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:04:30.249 - INFO: Train epoch 2433:   Loss: 529.6103 | r_Loss: 49.0216 | g_Loss: 252.6170 | l_Loss: 31.8851 | 
21-12-25 03:05:41.741 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:05:41.742 - INFO: Train epoch 2434:   Loss: 521.5708 | r_Loss: 50.4497 | g_Loss: 238.2762 | l_Loss: 31.0459 | 
21-12-25 03:06:53.385 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:06:53.386 - INFO: Train epoch 2435:   Loss: 570.0690 | r_Loss: 57.7315 | g_Loss: 246.4012 | l_Loss: 35.0105 | 
21-12-25 03:08:04.962 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:08:04.962 - INFO: Train epoch 2436:   Loss: 542.2142 | r_Loss: 50.5726 | g_Loss: 248.6518 | l_Loss: 40.6995 | 
21-12-25 03:09:16.277 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:09:16.277 - INFO: Train epoch 2437:   Loss: 504.9266 | r_Loss: 49.4760 | g_Loss: 234.3069 | l_Loss: 23.2397 | 
21-12-25 03:10:27.791 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:10:27.792 - INFO: Train epoch 2438:   Loss: 577.9817 | r_Loss: 56.9366 | g_Loss: 261.4588 | l_Loss: 31.8401 | 
21-12-25 03:11:39.246 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:11:39.247 - INFO: Train epoch 2439:   Loss: 673.2995 | r_Loss: 80.5479 | g_Loss: 232.7726 | l_Loss: 37.7872 | 
21-12-25 03:12:50.700 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:12:50.701 - INFO: Train epoch 2440:   Loss: 525.6825 | r_Loss: 50.6070 | g_Loss: 247.7199 | l_Loss: 24.9279 | 
21-12-25 03:14:02.169 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:14:02.170 - INFO: Train epoch 2441:   Loss: 512.4093 | r_Loss: 47.3163 | g_Loss: 244.8047 | l_Loss: 31.0230 | 
21-12-25 03:15:13.561 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:15:13.562 - INFO: Train epoch 2442:   Loss: 514.8897 | r_Loss: 50.0098 | g_Loss: 239.8084 | l_Loss: 25.0324 | 
21-12-25 03:16:24.939 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:16:24.940 - INFO: Train epoch 2443:   Loss: 544.6029 | r_Loss: 50.9063 | g_Loss: 251.9552 | l_Loss: 38.1164 | 
21-12-25 03:17:36.463 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:17:36.464 - INFO: Train epoch 2444:   Loss: 534.7057 | r_Loss: 53.0721 | g_Loss: 242.7644 | l_Loss: 26.5807 | 
21-12-25 03:18:47.793 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:18:47.793 - INFO: Train epoch 2445:   Loss: 518.2090 | r_Loss: 49.8617 | g_Loss: 244.2150 | l_Loss: 24.6855 | 
21-12-25 03:19:59.163 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:19:59.164 - INFO: Train epoch 2446:   Loss: 521.1395 | r_Loss: 53.1068 | g_Loss: 230.0529 | l_Loss: 25.5525 | 
21-12-25 03:21:10.622 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:21:10.623 - INFO: Train epoch 2447:   Loss: 531.7296 | r_Loss: 51.7054 | g_Loss: 240.0103 | l_Loss: 33.1921 | 
21-12-25 03:22:21.896 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:22:21.896 - INFO: Train epoch 2448:   Loss: 559.8033 | r_Loss: 59.3945 | g_Loss: 235.4942 | l_Loss: 27.3366 | 
21-12-25 03:23:33.336 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:23:33.336 - INFO: Train epoch 2449:   Loss: 528.3924 | r_Loss: 52.1222 | g_Loss: 242.0923 | l_Loss: 25.6891 | 
21-12-25 03:24:44.756 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:24:44.757 - INFO: Train epoch 2450:   Loss: 534.9117 | r_Loss: 52.9691 | g_Loss: 241.8709 | l_Loss: 28.1954 | 
21-12-25 03:25:56.316 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:25:56.317 - INFO: Train epoch 2451:   Loss: 490.9374 | r_Loss: 46.4724 | g_Loss: 229.9270 | l_Loss: 28.6485 | 
21-12-25 03:27:07.723 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:27:07.724 - INFO: Train epoch 2452:   Loss: 532.4828 | r_Loss: 54.0895 | g_Loss: 229.9531 | l_Loss: 32.0822 | 
21-12-25 03:28:19.125 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:28:19.126 - INFO: Train epoch 2453:   Loss: 549.6169 | r_Loss: 56.0847 | g_Loss: 237.3832 | l_Loss: 31.8104 | 
21-12-25 03:29:30.590 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:29:30.591 - INFO: Train epoch 2454:   Loss: 524.9898 | r_Loss: 52.1665 | g_Loss: 233.4181 | l_Loss: 30.7394 | 
21-12-25 03:30:42.141 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:30:42.142 - INFO: Train epoch 2455:   Loss: 559.2126 | r_Loss: 58.5581 | g_Loss: 233.9642 | l_Loss: 32.4578 | 
21-12-25 03:31:53.512 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:31:53.513 - INFO: Train epoch 2456:   Loss: 519.1936 | r_Loss: 52.7651 | g_Loss: 231.8444 | l_Loss: 23.5236 | 
21-12-25 03:33:04.867 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:33:04.868 - INFO: Train epoch 2457:   Loss: 527.9630 | r_Loss: 53.8230 | g_Loss: 230.5837 | l_Loss: 28.2642 | 
21-12-25 03:34:16.317 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:34:16.318 - INFO: Train epoch 2458:   Loss: 625.9518 | r_Loss: 72.2532 | g_Loss: 232.3272 | l_Loss: 32.3585 | 
21-12-25 03:35:27.727 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:35:27.727 - INFO: Train epoch 2459:   Loss: 519.4454 | r_Loss: 53.9559 | g_Loss: 226.0782 | l_Loss: 23.5877 | 
21-12-25 03:37:12.771 - INFO: TEST:   PSNR_S: 46.0173 | PSNR_C: 38.4116 | 
21-12-25 03:37:12.772 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:37:12.772 - INFO: Train epoch 2460:   Loss: 480.5616 | r_Loss: 47.4966 | g_Loss: 216.6562 | l_Loss: 26.4224 | 
21-12-25 03:38:24.182 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:38:24.182 - INFO: Train epoch 2461:   Loss: 537.2360 | r_Loss: 53.9596 | g_Loss: 234.6886 | l_Loss: 32.7496 | 
21-12-25 03:39:35.628 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:39:35.629 - INFO: Train epoch 2462:   Loss: 506.6324 | r_Loss: 51.2584 | g_Loss: 224.2261 | l_Loss: 26.1140 | 
21-12-25 03:40:46.933 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:40:46.934 - INFO: Train epoch 2463:   Loss: 500.7383 | r_Loss: 50.4235 | g_Loss: 214.5202 | l_Loss: 34.1009 | 
21-12-25 03:41:58.393 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:41:58.393 - INFO: Train epoch 2464:   Loss: 524.6089 | r_Loss: 53.9049 | g_Loss: 228.4945 | l_Loss: 26.5900 | 
21-12-25 03:43:09.876 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:43:09.876 - INFO: Train epoch 2465:   Loss: 743.3496 | r_Loss: 94.0937 | g_Loss: 239.6599 | l_Loss: 33.2211 | 
21-12-25 03:44:21.291 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:44:21.292 - INFO: Train epoch 2466:   Loss: 505.0781 | r_Loss: 49.9287 | g_Loss: 227.0026 | l_Loss: 28.4318 | 
21-12-25 03:45:32.757 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:45:32.758 - INFO: Train epoch 2467:   Loss: 519.1321 | r_Loss: 51.7094 | g_Loss: 234.2461 | l_Loss: 26.3391 | 
21-12-25 03:46:44.318 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:46:44.319 - INFO: Train epoch 2468:   Loss: 501.6971 | r_Loss: 50.0969 | g_Loss: 217.7218 | l_Loss: 33.4907 | 
21-12-25 03:47:55.707 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:47:55.707 - INFO: Train epoch 2469:   Loss: 524.8399 | r_Loss: 53.5454 | g_Loss: 227.7760 | l_Loss: 29.3370 | 
21-12-25 03:49:07.321 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:49:07.322 - INFO: Train epoch 2470:   Loss: 473.0527 | r_Loss: 46.4562 | g_Loss: 214.8187 | l_Loss: 25.9533 | 
21-12-25 03:50:18.766 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:50:18.767 - INFO: Train epoch 2471:   Loss: 473.1289 | r_Loss: 47.9015 | g_Loss: 208.9659 | l_Loss: 24.6554 | 
21-12-25 03:51:30.234 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:51:30.234 - INFO: Train epoch 2472:   Loss: 509.8244 | r_Loss: 53.8440 | g_Loss: 218.8916 | l_Loss: 21.7126 | 
21-12-25 03:52:41.633 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:52:41.634 - INFO: Train epoch 2473:   Loss: 480.3654 | r_Loss: 47.6069 | g_Loss: 217.6837 | l_Loss: 24.6472 | 
21-12-25 03:53:53.121 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:53:53.122 - INFO: Train epoch 2474:   Loss: 532.7886 | r_Loss: 55.9290 | g_Loss: 229.0749 | l_Loss: 24.0690 | 
21-12-25 03:55:04.480 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:55:04.481 - INFO: Train epoch 2475:   Loss: 521.8409 | r_Loss: 52.9491 | g_Loss: 228.9115 | l_Loss: 28.1838 | 
21-12-25 03:56:15.755 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:56:15.756 - INFO: Train epoch 2476:   Loss: 513.2559 | r_Loss: 52.9538 | g_Loss: 220.4581 | l_Loss: 28.0289 | 
21-12-25 03:57:27.322 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:57:27.322 - INFO: Train epoch 2477:   Loss: 544.3214 | r_Loss: 57.5259 | g_Loss: 230.0454 | l_Loss: 26.6463 | 
21-12-25 03:58:38.672 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:58:38.673 - INFO: Train epoch 2478:   Loss: 586.4746 | r_Loss: 63.0860 | g_Loss: 239.8973 | l_Loss: 31.1474 | 
21-12-25 03:59:50.069 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 03:59:50.070 - INFO: Train epoch 2479:   Loss: 530.5715 | r_Loss: 54.0560 | g_Loss: 234.3665 | l_Loss: 25.9248 | 
21-12-25 04:01:01.453 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:01:01.453 - INFO: Train epoch 2480:   Loss: 668.3572 | r_Loss: 61.3316 | g_Loss: 315.9031 | l_Loss: 45.7962 | 
21-12-25 04:02:12.828 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:02:12.828 - INFO: Train epoch 2481:   Loss: 508.3763 | r_Loss: 50.7561 | g_Loss: 222.3467 | l_Loss: 32.2491 | 
21-12-25 04:03:24.232 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:03:24.232 - INFO: Train epoch 2482:   Loss: 475.1092 | r_Loss: 47.0861 | g_Loss: 206.7997 | l_Loss: 32.8792 | 
21-12-25 04:04:35.576 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:04:35.576 - INFO: Train epoch 2483:   Loss: 503.1578 | r_Loss: 53.0187 | g_Loss: 210.2706 | l_Loss: 27.7937 | 
21-12-25 04:05:46.983 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:05:46.984 - INFO: Train epoch 2484:   Loss: 526.7890 | r_Loss: 54.0608 | g_Loss: 228.1291 | l_Loss: 28.3561 | 
21-12-25 04:06:58.366 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:06:58.366 - INFO: Train epoch 2485:   Loss: 485.5523 | r_Loss: 49.6892 | g_Loss: 211.2431 | l_Loss: 25.8632 | 
21-12-25 04:08:09.652 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:08:09.653 - INFO: Train epoch 2486:   Loss: 529.5257 | r_Loss: 55.8876 | g_Loss: 223.3456 | l_Loss: 26.7420 | 
21-12-25 04:09:21.192 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:09:21.193 - INFO: Train epoch 2487:   Loss: 505.2279 | r_Loss: 52.2013 | g_Loss: 213.5878 | l_Loss: 30.6338 | 
21-12-25 04:10:32.606 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:10:32.607 - INFO: Train epoch 2488:   Loss: 526.7861 | r_Loss: 55.2119 | g_Loss: 219.5051 | l_Loss: 31.2215 | 
21-12-25 04:11:44.149 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:11:44.150 - INFO: Train epoch 2489:   Loss: 553.6469 | r_Loss: 60.3268 | g_Loss: 227.6807 | l_Loss: 24.3319 | 
21-12-25 04:13:29.208 - INFO: TEST:   PSNR_S: 46.2123 | PSNR_C: 38.6302 | 
21-12-25 04:13:29.209 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:13:29.210 - INFO: Train epoch 2490:   Loss: 514.5740 | r_Loss: 53.1071 | g_Loss: 223.0622 | l_Loss: 25.9765 | 
21-12-25 04:14:40.624 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:14:40.625 - INFO: Train epoch 2491:   Loss: 526.3600 | r_Loss: 54.1835 | g_Loss: 223.6600 | l_Loss: 31.7824 | 
21-12-25 04:15:51.954 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:15:51.954 - INFO: Train epoch 2492:   Loss: 488.9454 | r_Loss: 51.0736 | g_Loss: 210.2411 | l_Loss: 23.3363 | 
21-12-25 04:17:03.375 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:17:03.375 - INFO: Train epoch 2493:   Loss: 479.5040 | r_Loss: 46.9320 | g_Loss: 216.9067 | l_Loss: 27.9373 | 
21-12-25 04:18:14.833 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:18:14.833 - INFO: Train epoch 2494:   Loss: 495.4421 | r_Loss: 50.6100 | g_Loss: 214.4719 | l_Loss: 27.9201 | 
21-12-25 04:19:26.162 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:19:26.163 - INFO: Train epoch 2495:   Loss: 564.3062 | r_Loss: 63.3150 | g_Loss: 224.1447 | l_Loss: 23.5866 | 
21-12-25 04:20:37.650 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:20:37.651 - INFO: Train epoch 2496:   Loss: 543.7808 | r_Loss: 54.6692 | g_Loss: 241.6121 | l_Loss: 28.8225 | 
21-12-25 04:21:49.096 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:21:49.096 - INFO: Train epoch 2497:   Loss: 495.5474 | r_Loss: 50.8746 | g_Loss: 212.8009 | l_Loss: 28.3734 | 
21-12-25 04:23:00.489 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:23:00.489 - INFO: Train epoch 2498:   Loss: 480.0072 | r_Loss: 47.5579 | g_Loss: 215.0250 | l_Loss: 27.1925 | 
21-12-25 04:24:11.834 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:24:11.834 - INFO: Train epoch 2499:   Loss: 475.4468 | r_Loss: 47.4292 | g_Loss: 211.7769 | l_Loss: 26.5237 | 
21-12-25 04:25:23.162 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:25:23.162 - INFO: Train epoch 2500:   Loss: 533.3730 | r_Loss: 55.9585 | g_Loss: 221.2485 | l_Loss: 32.3322 | 
21-12-25 04:26:34.742 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:26:34.742 - INFO: Train epoch 2501:   Loss: 506.4606 | r_Loss: 52.6755 | g_Loss: 214.1828 | l_Loss: 28.9002 | 
21-12-25 04:27:46.165 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:27:46.165 - INFO: Train epoch 2502:   Loss: 498.9593 | r_Loss: 50.3467 | g_Loss: 220.2078 | l_Loss: 27.0179 | 
21-12-25 04:28:57.591 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:28:57.592 - INFO: Train epoch 2503:   Loss: 532.4010 | r_Loss: 55.9234 | g_Loss: 224.7217 | l_Loss: 28.0623 | 
21-12-25 04:30:09.004 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:30:09.004 - INFO: Train epoch 2504:   Loss: 461.4187 | r_Loss: 44.9227 | g_Loss: 207.2576 | l_Loss: 29.5478 | 
21-12-25 04:31:20.512 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:31:20.513 - INFO: Train epoch 2505:   Loss: 524.5798 | r_Loss: 54.4271 | g_Loss: 223.8492 | l_Loss: 28.5951 | 
21-12-25 04:32:31.840 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:32:31.840 - INFO: Train epoch 2506:   Loss: 476.8811 | r_Loss: 51.0325 | g_Loss: 197.9785 | l_Loss: 23.7403 | 
21-12-25 04:33:43.271 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:33:43.272 - INFO: Train epoch 2507:   Loss: 492.0287 | r_Loss: 51.4170 | g_Loss: 208.5277 | l_Loss: 26.4160 | 
21-12-25 04:34:54.729 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:34:54.730 - INFO: Train epoch 2508:   Loss: 487.8066 | r_Loss: 50.1894 | g_Loss: 206.7255 | l_Loss: 30.1339 | 
21-12-25 04:36:06.098 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:36:06.099 - INFO: Train epoch 2509:   Loss: 13097.1434 | r_Loss: 2344.3612 | g_Loss: 1215.0778 | l_Loss: 160.2598 | 
21-12-25 04:37:17.482 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:37:17.482 - INFO: Train epoch 2510:   Loss: 1194.0798 | r_Loss: 112.8265 | g_Loss: 559.8247 | l_Loss: 70.1225 | 
21-12-25 04:38:29.021 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:38:29.021 - INFO: Train epoch 2511:   Loss: 941.5805 | r_Loss: 85.3703 | g_Loss: 460.8761 | l_Loss: 53.8527 | 
21-12-25 04:39:40.674 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:39:40.674 - INFO: Train epoch 2512:   Loss: 851.2711 | r_Loss: 74.6786 | g_Loss: 424.6540 | l_Loss: 53.2242 | 
21-12-25 04:40:52.311 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:40:52.311 - INFO: Train epoch 2513:   Loss: 778.8825 | r_Loss: 69.1431 | g_Loss: 387.0597 | l_Loss: 46.1073 | 
21-12-25 04:42:03.712 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:42:03.713 - INFO: Train epoch 2514:   Loss: 759.5253 | r_Loss: 65.2503 | g_Loss: 378.5102 | l_Loss: 54.7635 | 
21-12-25 04:43:15.208 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:43:15.209 - INFO: Train epoch 2515:   Loss: 731.5379 | r_Loss: 62.1045 | g_Loss: 375.5555 | l_Loss: 45.4601 | 
21-12-25 04:44:26.746 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:44:26.747 - INFO: Train epoch 2516:   Loss: 647.4414 | r_Loss: 53.0709 | g_Loss: 345.4238 | l_Loss: 36.6629 | 
21-12-25 04:45:38.215 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:45:38.215 - INFO: Train epoch 2517:   Loss: 665.4657 | r_Loss: 58.1614 | g_Loss: 335.2191 | l_Loss: 39.4396 | 
21-12-25 04:46:49.664 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:46:49.664 - INFO: Train epoch 2518:   Loss: 657.7020 | r_Loss: 55.3500 | g_Loss: 342.4506 | l_Loss: 38.5014 | 
21-12-25 04:48:01.069 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:48:01.069 - INFO: Train epoch 2519:   Loss: 655.9516 | r_Loss: 56.0691 | g_Loss: 331.4765 | l_Loss: 44.1297 | 
21-12-25 04:49:46.100 - INFO: TEST:   PSNR_S: 45.7236 | PSNR_C: 36.6793 | 
21-12-25 04:49:46.101 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:49:46.101 - INFO: Train epoch 2520:   Loss: 652.0832 | r_Loss: 55.9563 | g_Loss: 329.1806 | l_Loss: 43.1209 | 
21-12-25 04:50:57.629 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:50:57.630 - INFO: Train epoch 2521:   Loss: 639.8889 | r_Loss: 51.1653 | g_Loss: 323.0834 | l_Loss: 60.9792 | 
21-12-25 04:52:09.041 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:52:09.041 - INFO: Train epoch 2522:   Loss: 591.3267 | r_Loss: 49.1194 | g_Loss: 307.3110 | l_Loss: 38.4188 | 
21-12-25 04:53:20.539 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:53:20.539 - INFO: Train epoch 2523:   Loss: 595.2659 | r_Loss: 50.8392 | g_Loss: 297.3971 | l_Loss: 43.6727 | 
21-12-25 04:54:31.995 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:54:31.995 - INFO: Train epoch 2524:   Loss: 577.7930 | r_Loss: 49.9393 | g_Loss: 295.4700 | l_Loss: 32.6266 | 
21-12-25 04:55:43.544 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:55:43.545 - INFO: Train epoch 2525:   Loss: 598.3942 | r_Loss: 52.9362 | g_Loss: 300.9795 | l_Loss: 32.7338 | 
21-12-25 04:56:54.986 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:56:54.986 - INFO: Train epoch 2526:   Loss: 614.4634 | r_Loss: 54.2106 | g_Loss: 304.5258 | l_Loss: 38.8844 | 
21-12-25 04:58:06.564 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:58:06.564 - INFO: Train epoch 2527:   Loss: 611.7775 | r_Loss: 53.8316 | g_Loss: 299.6432 | l_Loss: 42.9762 | 
21-12-25 04:59:18.016 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 04:59:18.017 - INFO: Train epoch 2528:   Loss: 552.2027 | r_Loss: 47.3481 | g_Loss: 277.4898 | l_Loss: 37.9725 | 
21-12-25 05:00:29.455 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:00:29.455 - INFO: Train epoch 2529:   Loss: 569.2748 | r_Loss: 48.8736 | g_Loss: 287.3879 | l_Loss: 37.5190 | 
21-12-25 05:01:40.954 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:01:40.955 - INFO: Train epoch 2530:   Loss: 542.1972 | r_Loss: 46.8958 | g_Loss: 275.5151 | l_Loss: 32.2030 | 
21-12-25 05:02:52.359 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:02:52.360 - INFO: Train epoch 2531:   Loss: 558.2325 | r_Loss: 49.5172 | g_Loss: 275.3353 | l_Loss: 35.3113 | 
21-12-25 05:04:03.819 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:04:03.820 - INFO: Train epoch 2532:   Loss: 561.5244 | r_Loss: 48.6323 | g_Loss: 279.7420 | l_Loss: 38.6209 | 
21-12-25 05:05:15.195 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:05:15.195 - INFO: Train epoch 2533:   Loss: 565.6875 | r_Loss: 49.1383 | g_Loss: 276.8189 | l_Loss: 43.1771 | 
21-12-25 05:06:26.557 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:06:26.558 - INFO: Train epoch 2534:   Loss: 549.9360 | r_Loss: 50.3353 | g_Loss: 264.0376 | l_Loss: 34.2218 | 
21-12-25 05:07:37.994 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:07:37.995 - INFO: Train epoch 2535:   Loss: 530.7122 | r_Loss: 47.3468 | g_Loss: 260.4886 | l_Loss: 33.4895 | 
21-12-25 05:08:49.475 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:08:49.475 - INFO: Train epoch 2536:   Loss: 535.9086 | r_Loss: 47.1095 | g_Loss: 271.9069 | l_Loss: 28.4544 | 
21-12-25 05:10:01.137 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:10:01.137 - INFO: Train epoch 2537:   Loss: 512.9748 | r_Loss: 46.8392 | g_Loss: 247.0396 | l_Loss: 31.7392 | 
21-12-25 05:11:12.436 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:11:12.436 - INFO: Train epoch 2538:   Loss: 539.4229 | r_Loss: 47.7586 | g_Loss: 265.9704 | l_Loss: 34.6596 | 
21-12-25 05:12:23.791 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:12:23.792 - INFO: Train epoch 2539:   Loss: 495.6705 | r_Loss: 44.6151 | g_Loss: 242.2830 | l_Loss: 30.3119 | 
21-12-25 05:13:35.213 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:13:35.213 - INFO: Train epoch 2540:   Loss: 547.7158 | r_Loss: 51.3487 | g_Loss: 259.2914 | l_Loss: 31.6809 | 
21-12-25 05:14:46.600 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:14:46.601 - INFO: Train epoch 2541:   Loss: 527.1906 | r_Loss: 49.1135 | g_Loss: 253.3648 | l_Loss: 28.2582 | 
21-12-25 05:15:58.114 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:15:58.114 - INFO: Train epoch 2542:   Loss: 574.2122 | r_Loss: 55.3446 | g_Loss: 265.0724 | l_Loss: 32.4166 | 
21-12-25 05:17:09.491 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:17:09.492 - INFO: Train epoch 2543:   Loss: 514.8705 | r_Loss: 47.7477 | g_Loss: 249.2892 | l_Loss: 26.8427 | 
21-12-25 05:18:20.893 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:18:20.893 - INFO: Train epoch 2544:   Loss: 525.5632 | r_Loss: 49.5486 | g_Loss: 249.6842 | l_Loss: 28.1358 | 
21-12-25 05:19:32.209 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:19:32.209 - INFO: Train epoch 2545:   Loss: 511.1071 | r_Loss: 48.5656 | g_Loss: 242.1515 | l_Loss: 26.1276 | 
21-12-25 05:20:43.544 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:20:43.545 - INFO: Train epoch 2546:   Loss: 513.1423 | r_Loss: 47.2593 | g_Loss: 241.7085 | l_Loss: 35.1373 | 
21-12-25 05:21:54.978 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:21:54.979 - INFO: Train epoch 2547:   Loss: 506.1609 | r_Loss: 46.6949 | g_Loss: 241.8422 | l_Loss: 30.8443 | 
21-12-25 05:23:06.483 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:23:06.484 - INFO: Train epoch 2548:   Loss: 530.6919 | r_Loss: 50.6240 | g_Loss: 245.7501 | l_Loss: 31.8219 | 
21-12-25 05:24:17.831 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:24:17.832 - INFO: Train epoch 2549:   Loss: 487.7698 | r_Loss: 46.8241 | g_Loss: 230.3702 | l_Loss: 23.2794 | 
21-12-25 05:26:02.900 - INFO: TEST:   PSNR_S: 46.5937 | PSNR_C: 38.2370 | 
21-12-25 05:26:02.901 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:26:02.901 - INFO: Train epoch 2550:   Loss: 531.1269 | r_Loss: 48.7786 | g_Loss: 248.9608 | l_Loss: 38.2731 | 
21-12-25 05:27:14.439 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:27:14.439 - INFO: Train epoch 2551:   Loss: 482.3716 | r_Loss: 44.5362 | g_Loss: 231.0791 | l_Loss: 28.6116 | 
21-12-25 05:28:25.844 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:28:25.845 - INFO: Train epoch 2552:   Loss: 490.1549 | r_Loss: 47.0435 | g_Loss: 228.5041 | l_Loss: 26.4334 | 
21-12-25 05:29:37.224 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:29:37.225 - INFO: Train epoch 2553:   Loss: 462.1085 | r_Loss: 43.5549 | g_Loss: 220.4955 | l_Loss: 23.8386 | 
21-12-25 05:30:48.562 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:30:48.563 - INFO: Train epoch 2554:   Loss: 499.2391 | r_Loss: 49.9231 | g_Loss: 221.1464 | l_Loss: 28.4769 | 
21-12-25 05:32:00.039 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:32:00.041 - INFO: Train epoch 2555:   Loss: 475.3848 | r_Loss: 45.4951 | g_Loss: 223.4284 | l_Loss: 24.4810 | 
21-12-25 05:33:11.525 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:33:11.525 - INFO: Train epoch 2556:   Loss: 516.8465 | r_Loss: 53.4023 | g_Loss: 226.6185 | l_Loss: 23.2163 | 
21-12-25 05:34:22.891 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:34:22.892 - INFO: Train epoch 2557:   Loss: 486.2054 | r_Loss: 46.8697 | g_Loss: 218.3355 | l_Loss: 33.5216 | 
21-12-25 05:35:34.263 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:35:34.263 - INFO: Train epoch 2558:   Loss: 475.4511 | r_Loss: 46.1694 | g_Loss: 217.8461 | l_Loss: 26.7578 | 
21-12-25 05:36:45.616 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:36:45.616 - INFO: Train epoch 2559:   Loss: 513.6005 | r_Loss: 49.6571 | g_Loss: 238.2963 | l_Loss: 27.0185 | 
21-12-25 05:37:57.012 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:37:57.013 - INFO: Train epoch 2560:   Loss: 512.8723 | r_Loss: 54.1288 | g_Loss: 216.3177 | l_Loss: 25.9105 | 
21-12-25 05:39:08.497 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:39:08.498 - INFO: Train epoch 2561:   Loss: 499.4531 | r_Loss: 49.1149 | g_Loss: 227.3793 | l_Loss: 26.4992 | 
21-12-25 05:40:19.931 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:40:19.932 - INFO: Train epoch 2562:   Loss: 483.1891 | r_Loss: 46.5005 | g_Loss: 226.5556 | l_Loss: 24.1312 | 
21-12-25 05:41:31.288 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:41:31.289 - INFO: Train epoch 2563:   Loss: 461.0279 | r_Loss: 44.7644 | g_Loss: 208.5552 | l_Loss: 28.6505 | 
21-12-25 05:42:42.640 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:42:42.641 - INFO: Train epoch 2564:   Loss: 541.9674 | r_Loss: 58.2442 | g_Loss: 221.8322 | l_Loss: 28.9143 | 
21-12-25 05:43:54.076 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:43:54.077 - INFO: Train epoch 2565:   Loss: 523.6719 | r_Loss: 54.3297 | g_Loss: 219.8252 | l_Loss: 32.1980 | 
21-12-25 05:45:05.514 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:45:05.515 - INFO: Train epoch 2566:   Loss: 499.1330 | r_Loss: 49.8580 | g_Loss: 224.1069 | l_Loss: 25.7363 | 
21-12-25 05:46:16.886 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:46:16.887 - INFO: Train epoch 2567:   Loss: 516.6525 | r_Loss: 51.0949 | g_Loss: 221.9862 | l_Loss: 39.1917 | 
21-12-25 05:47:28.308 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:47:28.309 - INFO: Train epoch 2568:   Loss: 516.3277 | r_Loss: 52.6685 | g_Loss: 223.2629 | l_Loss: 29.7222 | 
21-12-25 05:48:39.780 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:48:39.781 - INFO: Train epoch 2569:   Loss: 466.9882 | r_Loss: 49.2118 | g_Loss: 195.8327 | l_Loss: 25.0965 | 
21-12-25 05:49:51.294 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:49:51.296 - INFO: Train epoch 2570:   Loss: 477.0271 | r_Loss: 47.2952 | g_Loss: 217.6071 | l_Loss: 22.9442 | 
21-12-25 05:51:02.805 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:51:02.806 - INFO: Train epoch 2571:   Loss: 484.2273 | r_Loss: 48.3601 | g_Loss: 211.2136 | l_Loss: 31.2133 | 
21-12-25 05:52:14.179 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:52:14.179 - INFO: Train epoch 2572:   Loss: 506.5063 | r_Loss: 49.7142 | g_Loss: 226.6092 | l_Loss: 31.3260 | 
21-12-25 05:53:25.523 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:53:25.524 - INFO: Train epoch 2573:   Loss: 496.8390 | r_Loss: 48.9168 | g_Loss: 221.2764 | l_Loss: 30.9786 | 
21-12-25 05:54:36.841 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:54:36.842 - INFO: Train epoch 2574:   Loss: 482.4966 | r_Loss: 50.7039 | g_Loss: 204.0507 | l_Loss: 24.9266 | 
21-12-25 05:55:48.311 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:55:48.312 - INFO: Train epoch 2575:   Loss: 520.6712 | r_Loss: 53.7816 | g_Loss: 226.4334 | l_Loss: 25.3300 | 
21-12-25 05:56:59.672 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:56:59.673 - INFO: Train epoch 2576:   Loss: 516.2070 | r_Loss: 53.8902 | g_Loss: 215.6935 | l_Loss: 31.0623 | 
21-12-25 05:58:11.080 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:58:11.081 - INFO: Train epoch 2577:   Loss: 501.9393 | r_Loss: 51.8336 | g_Loss: 218.9374 | l_Loss: 23.8340 | 
21-12-25 05:59:22.474 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 05:59:22.475 - INFO: Train epoch 2578:   Loss: 488.1436 | r_Loss: 50.7385 | g_Loss: 211.8733 | l_Loss: 22.5776 | 
21-12-25 06:00:33.843 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:00:33.843 - INFO: Train epoch 2579:   Loss: 479.0214 | r_Loss: 46.4909 | g_Loss: 216.5606 | l_Loss: 30.0061 | 
21-12-25 06:02:18.984 - INFO: TEST:   PSNR_S: 46.4461 | PSNR_C: 38.6120 | 
21-12-25 06:02:18.985 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:02:18.985 - INFO: Train epoch 2580:   Loss: 606.9228 | r_Loss: 60.3255 | g_Loss: 265.5506 | l_Loss: 39.7447 | 
21-12-25 06:03:30.433 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:03:30.433 - INFO: Train epoch 2581:   Loss: 487.4634 | r_Loss: 47.9742 | g_Loss: 216.3393 | l_Loss: 31.2533 | 
21-12-25 06:04:41.815 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:04:41.816 - INFO: Train epoch 2582:   Loss: 510.1474 | r_Loss: 53.5990 | g_Loss: 219.4326 | l_Loss: 22.7199 | 
21-12-25 06:05:53.258 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:05:53.259 - INFO: Train epoch 2583:   Loss: 518.7743 | r_Loss: 50.1761 | g_Loss: 235.0498 | l_Loss: 32.8439 | 
21-12-25 06:07:04.697 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:07:04.698 - INFO: Train epoch 2584:   Loss: 461.0568 | r_Loss: 45.6634 | g_Loss: 211.1503 | l_Loss: 21.5893 | 
21-12-25 06:08:16.130 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:08:16.131 - INFO: Train epoch 2585:   Loss: 495.9845 | r_Loss: 51.7031 | g_Loss: 213.6740 | l_Loss: 23.7949 | 
21-12-25 06:09:27.483 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:09:27.483 - INFO: Train epoch 2586:   Loss: 566.4190 | r_Loss: 64.8021 | g_Loss: 215.0585 | l_Loss: 27.3500 | 
21-12-25 06:10:38.921 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:10:38.921 - INFO: Train epoch 2587:   Loss: 499.5660 | r_Loss: 50.1578 | g_Loss: 222.7348 | l_Loss: 26.0423 | 
21-12-25 06:11:50.379 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:11:50.380 - INFO: Train epoch 2588:   Loss: 494.9228 | r_Loss: 50.7825 | g_Loss: 213.8579 | l_Loss: 27.1525 | 
21-12-25 06:13:01.951 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:13:01.951 - INFO: Train epoch 2589:   Loss: 483.1206 | r_Loss: 48.5553 | g_Loss: 218.5085 | l_Loss: 21.8358 | 
21-12-25 06:14:13.315 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:14:13.316 - INFO: Train epoch 2590:   Loss: 495.2682 | r_Loss: 51.6453 | g_Loss: 213.5151 | l_Loss: 23.5263 | 
21-12-25 06:15:24.597 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:15:24.598 - INFO: Train epoch 2591:   Loss: 478.1581 | r_Loss: 46.3452 | g_Loss: 216.7897 | l_Loss: 29.6421 | 
21-12-25 06:16:35.939 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:16:35.940 - INFO: Train epoch 2592:   Loss: 464.6821 | r_Loss: 48.0434 | g_Loss: 198.6745 | l_Loss: 25.7906 | 
21-12-25 06:17:47.284 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:17:47.285 - INFO: Train epoch 2593:   Loss: 465.2062 | r_Loss: 46.2408 | g_Loss: 207.1541 | l_Loss: 26.8480 | 
21-12-25 06:18:58.753 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:18:58.753 - INFO: Train epoch 2594:   Loss: 495.6946 | r_Loss: 50.8641 | g_Loss: 214.2105 | l_Loss: 27.1634 | 
21-12-25 06:20:10.152 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:20:10.152 - INFO: Train epoch 2595:   Loss: 514.1759 | r_Loss: 55.4325 | g_Loss: 212.3857 | l_Loss: 24.6279 | 
21-12-25 06:21:21.688 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:21:21.689 - INFO: Train epoch 2596:   Loss: 479.0516 | r_Loss: 46.7633 | g_Loss: 212.2360 | l_Loss: 32.9994 | 
21-12-25 06:22:33.009 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:22:33.009 - INFO: Train epoch 2597:   Loss: 518.1991 | r_Loss: 50.8990 | g_Loss: 231.1784 | l_Loss: 32.5257 | 
21-12-25 06:23:44.409 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:23:44.409 - INFO: Train epoch 2598:   Loss: 527.7706 | r_Loss: 54.0034 | g_Loss: 229.4943 | l_Loss: 28.2594 | 
21-12-25 06:24:55.809 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:24:55.809 - INFO: Train epoch 2599:   Loss: 525.3492 | r_Loss: 55.0709 | g_Loss: 221.5410 | l_Loss: 28.4537 | 
21-12-25 06:26:07.258 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:26:07.259 - INFO: Train epoch 2600:   Loss: 463.5351 | r_Loss: 46.5877 | g_Loss: 205.4933 | l_Loss: 25.1030 | 
21-12-25 06:27:18.758 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:27:18.759 - INFO: Train epoch 2601:   Loss: 525.4193 | r_Loss: 53.8425 | g_Loss: 222.0408 | l_Loss: 34.1662 | 
21-12-25 06:28:30.142 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:28:30.143 - INFO: Train epoch 2602:   Loss: 487.3361 | r_Loss: 49.8235 | g_Loss: 211.9399 | l_Loss: 26.2785 | 
21-12-25 06:29:41.682 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:29:41.683 - INFO: Train epoch 2603:   Loss: 536.6281 | r_Loss: 57.3719 | g_Loss: 219.0536 | l_Loss: 30.7153 | 
21-12-25 06:30:53.026 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:30:53.026 - INFO: Train epoch 2604:   Loss: 483.2541 | r_Loss: 49.0865 | g_Loss: 209.2957 | l_Loss: 28.5258 | 
21-12-25 06:32:04.397 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:32:04.397 - INFO: Train epoch 2605:   Loss: 480.9482 | r_Loss: 48.5561 | g_Loss: 212.4468 | l_Loss: 25.7211 | 
21-12-25 06:33:15.870 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:33:15.871 - INFO: Train epoch 2606:   Loss: 503.1602 | r_Loss: 51.7712 | g_Loss: 215.7921 | l_Loss: 28.5121 | 
21-12-25 06:34:27.379 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:34:27.380 - INFO: Train epoch 2607:   Loss: 531.0892 | r_Loss: 56.5596 | g_Loss: 220.3909 | l_Loss: 27.9003 | 
21-12-25 06:35:38.752 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:35:38.753 - INFO: Train epoch 2608:   Loss: 587.5714 | r_Loss: 60.1132 | g_Loss: 258.2395 | l_Loss: 28.7658 | 
21-12-25 06:36:50.074 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:36:50.075 - INFO: Train epoch 2609:   Loss: 468.2030 | r_Loss: 46.8871 | g_Loss: 205.1697 | l_Loss: 28.5977 | 
21-12-25 06:38:35.097 - INFO: TEST:   PSNR_S: 46.5736 | PSNR_C: 38.9002 | 
21-12-25 06:38:35.098 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:38:35.098 - INFO: Train epoch 2610:   Loss: 476.0072 | r_Loss: 48.6917 | g_Loss: 208.3713 | l_Loss: 24.1773 | 
21-12-25 06:39:46.481 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:39:46.482 - INFO: Train epoch 2611:   Loss: 461.5085 | r_Loss: 46.8730 | g_Loss: 200.9263 | l_Loss: 26.2174 | 
21-12-25 06:40:57.795 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:40:57.795 - INFO: Train epoch 2612:   Loss: 495.7278 | r_Loss: 51.3066 | g_Loss: 214.9249 | l_Loss: 24.2698 | 
21-12-25 06:42:09.088 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:42:09.089 - INFO: Train epoch 2613:   Loss: 501.4949 | r_Loss: 55.2506 | g_Loss: 203.6176 | l_Loss: 21.6241 | 
21-12-25 06:43:20.434 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:43:20.434 - INFO: Train epoch 2614:   Loss: 468.2513 | r_Loss: 47.0554 | g_Loss: 207.9060 | l_Loss: 25.0684 | 
21-12-25 06:44:31.802 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:44:31.802 - INFO: Train epoch 2615:   Loss: 486.8071 | r_Loss: 49.6962 | g_Loss: 214.1289 | l_Loss: 24.1969 | 
21-12-25 06:45:43.111 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:45:43.112 - INFO: Train epoch 2616:   Loss: 548.9882 | r_Loss: 49.9268 | g_Loss: 264.6555 | l_Loss: 34.6988 | 
21-12-25 06:46:54.746 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:46:54.748 - INFO: Train epoch 2617:   Loss: 447.3260 | r_Loss: 43.0672 | g_Loss: 203.5805 | l_Loss: 28.4093 | 
21-12-25 06:48:06.139 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:48:06.140 - INFO: Train epoch 2618:   Loss: 460.5417 | r_Loss: 47.0120 | g_Loss: 202.7282 | l_Loss: 22.7536 | 
21-12-25 06:49:17.602 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:49:17.603 - INFO: Train epoch 2619:   Loss: 460.0218 | r_Loss: 48.9335 | g_Loss: 193.7499 | l_Loss: 21.6042 | 
21-12-25 06:50:29.080 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:50:29.080 - INFO: Train epoch 2620:   Loss: 467.9618 | r_Loss: 48.1959 | g_Loss: 199.5298 | l_Loss: 27.4524 | 
21-12-25 06:51:40.450 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:51:40.451 - INFO: Train epoch 2621:   Loss: 491.0335 | r_Loss: 51.3541 | g_Loss: 206.1034 | l_Loss: 28.1594 | 
21-12-25 06:52:51.831 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:52:51.832 - INFO: Train epoch 2622:   Loss: 489.9996 | r_Loss: 51.2547 | g_Loss: 209.6301 | l_Loss: 24.0959 | 
21-12-25 06:54:03.151 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:54:03.151 - INFO: Train epoch 2623:   Loss: 516.1200 | r_Loss: 55.0183 | g_Loss: 215.2084 | l_Loss: 25.8203 | 
21-12-25 06:55:14.408 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:55:14.409 - INFO: Train epoch 2624:   Loss: 495.7952 | r_Loss: 52.8863 | g_Loss: 207.8607 | l_Loss: 23.5030 | 
21-12-25 06:56:25.792 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:56:25.792 - INFO: Train epoch 2625:   Loss: 489.1505 | r_Loss: 52.6698 | g_Loss: 199.1264 | l_Loss: 26.6749 | 
21-12-25 06:57:37.073 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:57:37.073 - INFO: Train epoch 2626:   Loss: 504.3033 | r_Loss: 52.5722 | g_Loss: 216.0066 | l_Loss: 25.4358 | 
21-12-25 06:58:48.448 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:58:48.448 - INFO: Train epoch 2627:   Loss: 496.2779 | r_Loss: 52.7514 | g_Loss: 210.6037 | l_Loss: 21.9175 | 
21-12-25 06:59:59.940 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 06:59:59.941 - INFO: Train epoch 2628:   Loss: 17473.6444 | r_Loss: 3289.7952 | g_Loss: 917.7672 | l_Loss: 106.9012 | 
21-12-25 07:01:11.326 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:01:11.327 - INFO: Train epoch 2629:   Loss: 1555.9582 | r_Loss: 159.4541 | g_Loss: 670.2982 | l_Loss: 88.3896 | 
21-12-25 07:02:22.687 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:02:22.688 - INFO: Train epoch 2630:   Loss: 1101.5687 | r_Loss: 109.6845 | g_Loss: 490.5104 | l_Loss: 62.6356 | 
21-12-25 07:03:34.214 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:03:34.215 - INFO: Train epoch 2631:   Loss: 1020.7480 | r_Loss: 101.5129 | g_Loss: 459.5558 | l_Loss: 53.6278 | 
21-12-25 07:04:45.623 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:04:45.623 - INFO: Train epoch 2632:   Loss: 908.4358 | r_Loss: 88.9581 | g_Loss: 414.0130 | l_Loss: 49.6320 | 
21-12-25 07:05:56.964 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:05:56.965 - INFO: Train epoch 2633:   Loss: 870.0538 | r_Loss: 84.8451 | g_Loss: 403.0071 | l_Loss: 42.8213 | 
21-12-25 07:07:08.325 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:07:08.325 - INFO: Train epoch 2634:   Loss: 839.9791 | r_Loss: 77.7293 | g_Loss: 395.9987 | l_Loss: 55.3338 | 
21-12-25 07:08:19.782 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:08:19.783 - INFO: Train epoch 2635:   Loss: 801.4016 | r_Loss: 75.3113 | g_Loss: 383.4948 | l_Loss: 41.3504 | 
21-12-25 07:09:31.150 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:09:31.151 - INFO: Train epoch 2636:   Loss: 756.8034 | r_Loss: 69.7931 | g_Loss: 355.7903 | l_Loss: 52.0473 | 
21-12-25 07:10:42.553 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:10:42.554 - INFO: Train epoch 2637:   Loss: 700.6959 | r_Loss: 62.8130 | g_Loss: 338.1548 | l_Loss: 48.4762 | 
21-12-25 07:11:54.111 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:11:54.111 - INFO: Train epoch 2638:   Loss: 702.9282 | r_Loss: 64.5779 | g_Loss: 337.0010 | l_Loss: 43.0379 | 
21-12-25 07:13:05.689 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:13:05.690 - INFO: Train epoch 2639:   Loss: 699.7741 | r_Loss: 65.3985 | g_Loss: 329.8719 | l_Loss: 42.9099 | 
21-12-25 07:14:50.739 - INFO: TEST:   PSNR_S: 44.8195 | PSNR_C: 36.6805 | 
21-12-25 07:14:50.740 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:14:50.740 - INFO: Train epoch 2640:   Loss: 689.5708 | r_Loss: 63.1441 | g_Loss: 327.5522 | l_Loss: 46.2981 | 
21-12-25 07:16:02.089 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:16:02.089 - INFO: Train epoch 2641:   Loss: 642.2575 | r_Loss: 57.7631 | g_Loss: 313.3600 | l_Loss: 40.0822 | 
21-12-25 07:17:13.575 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:17:13.576 - INFO: Train epoch 2642:   Loss: 627.9345 | r_Loss: 56.3645 | g_Loss: 306.0630 | l_Loss: 40.0491 | 
21-12-25 07:18:24.966 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:18:24.967 - INFO: Train epoch 2643:   Loss: 615.8475 | r_Loss: 56.5429 | g_Loss: 301.2605 | l_Loss: 31.8723 | 
21-12-25 07:19:36.268 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:19:36.268 - INFO: Train epoch 2644:   Loss: 627.9972 | r_Loss: 57.2363 | g_Loss: 305.6271 | l_Loss: 36.1888 | 
21-12-25 07:20:47.607 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:20:47.608 - INFO: Train epoch 2645:   Loss: 580.0189 | r_Loss: 51.6630 | g_Loss: 287.3789 | l_Loss: 34.3250 | 
21-12-25 07:21:58.925 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:21:58.925 - INFO: Train epoch 2646:   Loss: 573.2387 | r_Loss: 49.9311 | g_Loss: 292.8654 | l_Loss: 30.7181 | 
21-12-25 07:23:10.278 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:23:10.279 - INFO: Train epoch 2647:   Loss: 583.9423 | r_Loss: 53.2909 | g_Loss: 283.5048 | l_Loss: 33.9831 | 
21-12-25 07:24:21.485 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:24:21.486 - INFO: Train epoch 2648:   Loss: 605.5548 | r_Loss: 56.4830 | g_Loss: 288.4339 | l_Loss: 34.7060 | 
21-12-25 07:25:32.790 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:25:32.791 - INFO: Train epoch 2649:   Loss: 605.6663 | r_Loss: 55.3324 | g_Loss: 296.7406 | l_Loss: 32.2636 | 
21-12-25 07:26:44.169 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:26:44.170 - INFO: Train epoch 2650:   Loss: 580.4760 | r_Loss: 51.9225 | g_Loss: 288.2142 | l_Loss: 32.6493 | 
21-12-25 07:27:55.686 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:27:55.687 - INFO: Train epoch 2651:   Loss: 551.0341 | r_Loss: 47.2282 | g_Loss: 278.1238 | l_Loss: 36.7693 | 
21-12-25 07:29:07.103 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:29:07.104 - INFO: Train epoch 2652:   Loss: 564.2302 | r_Loss: 48.7854 | g_Loss: 281.6596 | l_Loss: 38.6434 | 
21-12-25 07:30:18.473 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:30:18.474 - INFO: Train epoch 2653:   Loss: 544.8573 | r_Loss: 47.7582 | g_Loss: 273.7220 | l_Loss: 32.3446 | 
21-12-25 07:31:29.930 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:31:29.931 - INFO: Train epoch 2654:   Loss: 574.9890 | r_Loss: 52.4701 | g_Loss: 287.2211 | l_Loss: 25.4176 | 
21-12-25 07:32:41.374 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:32:41.374 - INFO: Train epoch 2655:   Loss: 560.6316 | r_Loss: 52.6549 | g_Loss: 264.1081 | l_Loss: 33.2489 | 
21-12-25 07:33:53.111 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:33:53.112 - INFO: Train epoch 2656:   Loss: 529.5768 | r_Loss: 47.7969 | g_Loss: 261.0363 | l_Loss: 29.5557 | 
21-12-25 07:35:06.619 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:35:06.621 - INFO: Train epoch 2657:   Loss: 490.0669 | r_Loss: 42.7911 | g_Loss: 248.8164 | l_Loss: 27.2949 | 
21-12-25 07:36:20.297 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:36:20.298 - INFO: Train epoch 2658:   Loss: 513.5538 | r_Loss: 45.3402 | g_Loss: 252.0729 | l_Loss: 34.7799 | 
21-12-25 07:37:33.793 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:37:33.794 - INFO: Train epoch 2659:   Loss: 548.8489 | r_Loss: 49.6901 | g_Loss: 267.5171 | l_Loss: 32.8814 | 
21-12-25 07:38:47.406 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:38:47.407 - INFO: Train epoch 2660:   Loss: 519.3883 | r_Loss: 46.9668 | g_Loss: 255.6516 | l_Loss: 28.9024 | 
21-12-25 07:40:00.621 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:40:00.623 - INFO: Train epoch 2661:   Loss: 532.3969 | r_Loss: 49.4813 | g_Loss: 253.9351 | l_Loss: 31.0553 | 
21-12-25 07:41:14.527 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:41:14.528 - INFO: Train epoch 2662:   Loss: 513.5206 | r_Loss: 45.7538 | g_Loss: 254.8653 | l_Loss: 29.8861 | 
21-12-25 07:42:27.622 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:42:27.623 - INFO: Train epoch 2663:   Loss: 485.3695 | r_Loss: 43.6021 | g_Loss: 235.9925 | l_Loss: 31.3667 | 
21-12-25 07:43:40.826 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:43:40.828 - INFO: Train epoch 2664:   Loss: 547.2359 | r_Loss: 51.7315 | g_Loss: 253.8906 | l_Loss: 34.6878 | 
21-12-25 07:44:54.061 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:44:54.062 - INFO: Train epoch 2665:   Loss: 497.6874 | r_Loss: 43.9821 | g_Loss: 245.9662 | l_Loss: 31.8105 | 
21-12-25 07:46:07.160 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:46:07.161 - INFO: Train epoch 2666:   Loss: 557.8218 | r_Loss: 53.2416 | g_Loss: 258.5951 | l_Loss: 33.0190 | 
21-12-25 07:47:20.566 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:47:20.567 - INFO: Train epoch 2667:   Loss: 509.8515 | r_Loss: 46.8851 | g_Loss: 248.9672 | l_Loss: 26.4587 | 
21-12-25 07:48:34.021 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:48:34.022 - INFO: Train epoch 2668:   Loss: 518.6234 | r_Loss: 48.1671 | g_Loss: 245.5572 | l_Loss: 32.2307 | 
21-12-25 07:49:47.465 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:49:47.467 - INFO: Train epoch 2669:   Loss: 537.9017 | r_Loss: 49.9089 | g_Loss: 252.4515 | l_Loss: 35.9058 | 
21-12-25 07:51:37.387 - INFO: TEST:   PSNR_S: 46.4675 | PSNR_C: 38.1006 | 
21-12-25 07:51:37.388 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:51:37.388 - INFO: Train epoch 2670:   Loss: 517.0916 | r_Loss: 49.2986 | g_Loss: 244.0195 | l_Loss: 26.5792 | 
21-12-25 07:52:51.359 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:52:51.360 - INFO: Train epoch 2671:   Loss: 501.3219 | r_Loss: 48.1487 | g_Loss: 232.0338 | l_Loss: 28.5448 | 
21-12-25 07:54:04.961 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:54:04.962 - INFO: Train epoch 2672:   Loss: 495.6819 | r_Loss: 47.1518 | g_Loss: 226.0027 | l_Loss: 33.9201 | 
21-12-25 07:55:18.470 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:55:18.471 - INFO: Train epoch 2673:   Loss: 495.5098 | r_Loss: 46.2732 | g_Loss: 233.3655 | l_Loss: 30.7782 | 
21-12-25 07:56:31.975 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:56:31.976 - INFO: Train epoch 2674:   Loss: 493.6840 | r_Loss: 49.0881 | g_Loss: 225.8917 | l_Loss: 22.3515 | 
21-12-25 07:57:45.681 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:57:45.682 - INFO: Train epoch 2675:   Loss: 514.7359 | r_Loss: 49.2644 | g_Loss: 238.3315 | l_Loss: 30.0824 | 
21-12-25 07:58:59.566 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 07:58:59.567 - INFO: Train epoch 2676:   Loss: 533.1851 | r_Loss: 52.6055 | g_Loss: 234.5032 | l_Loss: 35.6543 | 
21-12-25 08:00:13.227 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:00:13.228 - INFO: Train epoch 2677:   Loss: 515.0404 | r_Loss: 48.6762 | g_Loss: 239.0149 | l_Loss: 32.6446 | 
21-12-25 08:01:26.802 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:01:26.804 - INFO: Train epoch 2678:   Loss: 443.4486 | r_Loss: 40.0162 | g_Loss: 217.4613 | l_Loss: 25.9063 | 
21-12-25 08:02:40.003 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:02:40.004 - INFO: Train epoch 2679:   Loss: 466.5776 | r_Loss: 43.5607 | g_Loss: 225.0300 | l_Loss: 23.7442 | 
21-12-25 08:03:53.254 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:03:53.255 - INFO: Train epoch 2680:   Loss: 481.5348 | r_Loss: 44.8223 | g_Loss: 230.3687 | l_Loss: 27.0547 | 
21-12-25 08:05:06.706 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:05:06.707 - INFO: Train epoch 2681:   Loss: 478.8752 | r_Loss: 46.2146 | g_Loss: 217.0235 | l_Loss: 30.7786 | 
21-12-25 08:06:19.824 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:06:19.825 - INFO: Train epoch 2682:   Loss: 495.4275 | r_Loss: 48.7154 | g_Loss: 223.9995 | l_Loss: 27.8510 | 
21-12-25 08:07:33.519 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:07:33.520 - INFO: Train epoch 2683:   Loss: 508.4954 | r_Loss: 49.1311 | g_Loss: 227.3670 | l_Loss: 35.4730 | 
21-12-25 08:08:47.097 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:08:47.098 - INFO: Train epoch 2684:   Loss: 519.2049 | r_Loss: 51.6811 | g_Loss: 234.7456 | l_Loss: 26.0540 | 
21-12-25 08:10:01.190 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:10:01.191 - INFO: Train epoch 2685:   Loss: 507.0130 | r_Loss: 50.8712 | g_Loss: 227.5824 | l_Loss: 25.0743 | 
21-12-25 08:11:14.735 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:11:14.736 - INFO: Train epoch 2686:   Loss: 475.2980 | r_Loss: 46.4129 | g_Loss: 217.7397 | l_Loss: 25.4937 | 
21-12-25 08:12:28.357 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:12:28.358 - INFO: Train epoch 2687:   Loss: 517.1740 | r_Loss: 51.6960 | g_Loss: 226.7598 | l_Loss: 31.9342 | 
21-12-25 08:13:42.038 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:13:42.039 - INFO: Train epoch 2688:   Loss: 473.1408 | r_Loss: 46.4924 | g_Loss: 214.9035 | l_Loss: 25.7753 | 
21-12-25 08:14:55.325 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:14:55.326 - INFO: Train epoch 2689:   Loss: 491.3363 | r_Loss: 49.0186 | g_Loss: 219.9917 | l_Loss: 26.2515 | 
21-12-25 08:16:09.202 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:16:09.203 - INFO: Train epoch 2690:   Loss: 486.9230 | r_Loss: 46.6619 | g_Loss: 224.9733 | l_Loss: 28.6402 | 
21-12-25 08:17:22.417 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:17:22.418 - INFO: Train epoch 2691:   Loss: 485.0293 | r_Loss: 48.4647 | g_Loss: 214.7118 | l_Loss: 27.9941 | 
21-12-25 08:18:35.879 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:18:35.880 - INFO: Train epoch 2692:   Loss: 467.2219 | r_Loss: 46.5818 | g_Loss: 212.9205 | l_Loss: 21.3924 | 
21-12-25 08:19:49.330 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:19:49.331 - INFO: Train epoch 2693:   Loss: 453.7348 | r_Loss: 44.5928 | g_Loss: 204.7856 | l_Loss: 25.9853 | 
21-12-25 08:21:02.486 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:21:02.487 - INFO: Train epoch 2694:   Loss: 516.0616 | r_Loss: 52.5295 | g_Loss: 224.6354 | l_Loss: 28.7789 | 
21-12-25 08:22:16.133 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:22:16.135 - INFO: Train epoch 2695:   Loss: 483.3773 | r_Loss: 48.4775 | g_Loss: 215.8105 | l_Loss: 25.1794 | 
21-12-25 08:23:30.062 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:23:30.062 - INFO: Train epoch 2696:   Loss: 500.3132 | r_Loss: 48.4745 | g_Loss: 229.3538 | l_Loss: 28.5867 | 
21-12-25 08:24:43.550 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:24:43.551 - INFO: Train epoch 2697:   Loss: 488.6547 | r_Loss: 47.6438 | g_Loss: 223.5544 | l_Loss: 26.8812 | 
21-12-25 08:25:56.805 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:25:56.806 - INFO: Train epoch 2698:   Loss: 485.8296 | r_Loss: 47.5704 | g_Loss: 222.1804 | l_Loss: 25.7973 | 
21-12-25 08:27:10.544 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:27:10.545 - INFO: Train epoch 2699:   Loss: 464.2048 | r_Loss: 44.8081 | g_Loss: 210.9088 | l_Loss: 29.2556 | 
21-12-25 08:29:00.224 - INFO: TEST:   PSNR_S: 46.4384 | PSNR_C: 38.6615 | 
21-12-25 08:29:00.226 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:29:00.226 - INFO: Train epoch 2700:   Loss: 508.4540 | r_Loss: 52.2328 | g_Loss: 224.0855 | l_Loss: 23.2044 | 
21-12-25 08:30:13.387 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:30:13.389 - INFO: Train epoch 2701:   Loss: 455.6639 | r_Loss: 46.7031 | g_Loss: 201.8943 | l_Loss: 20.2540 | 
21-12-25 08:31:26.939 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:31:26.941 - INFO: Train epoch 2702:   Loss: 468.5351 | r_Loss: 46.8879 | g_Loss: 211.0745 | l_Loss: 23.0212 | 
21-12-25 08:32:40.436 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:32:40.437 - INFO: Train epoch 2703:   Loss: 518.5437 | r_Loss: 53.4657 | g_Loss: 222.7540 | l_Loss: 28.4611 | 
21-12-25 08:33:54.051 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:33:54.052 - INFO: Train epoch 2704:   Loss: 477.0348 | r_Loss: 46.8953 | g_Loss: 212.7885 | l_Loss: 29.7698 | 
21-12-25 08:35:07.535 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:35:07.536 - INFO: Train epoch 2705:   Loss: 478.6535 | r_Loss: 49.3291 | g_Loss: 208.4573 | l_Loss: 23.5504 | 
21-12-25 08:36:20.977 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:36:20.979 - INFO: Train epoch 2706:   Loss: 456.4161 | r_Loss: 46.3455 | g_Loss: 202.6088 | l_Loss: 22.0798 | 
21-12-25 08:37:34.590 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:37:34.592 - INFO: Train epoch 2707:   Loss: 476.6995 | r_Loss: 48.5732 | g_Loss: 207.0518 | l_Loss: 26.7816 | 
21-12-25 08:38:48.696 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:38:48.697 - INFO: Train epoch 2708:   Loss: 492.2907 | r_Loss: 49.4946 | g_Loss: 216.3653 | l_Loss: 28.4525 | 
21-12-25 08:40:02.410 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:40:02.412 - INFO: Train epoch 2709:   Loss: 506.0249 | r_Loss: 54.0277 | g_Loss: 213.9007 | l_Loss: 21.9859 | 
21-12-25 08:41:15.922 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:41:15.924 - INFO: Train epoch 2710:   Loss: 486.1356 | r_Loss: 49.6746 | g_Loss: 209.3309 | l_Loss: 28.4319 | 
21-12-25 08:42:29.148 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:42:29.149 - INFO: Train epoch 2711:   Loss: 459.3936 | r_Loss: 45.9743 | g_Loss: 207.6655 | l_Loss: 21.8564 | 
21-12-25 08:43:42.838 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:43:42.839 - INFO: Train epoch 2712:   Loss: 515.9418 | r_Loss: 54.6799 | g_Loss: 216.5196 | l_Loss: 26.0228 | 
21-12-25 08:44:56.341 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:44:56.342 - INFO: Train epoch 2713:   Loss: 453.0714 | r_Loss: 45.5341 | g_Loss: 202.6036 | l_Loss: 22.7976 | 
21-12-25 08:46:09.934 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:46:09.936 - INFO: Train epoch 2714:   Loss: 482.6426 | r_Loss: 49.3251 | g_Loss: 210.5662 | l_Loss: 25.4509 | 
21-12-25 08:47:23.470 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:47:23.472 - INFO: Train epoch 2715:   Loss: 524.2757 | r_Loss: 56.7267 | g_Loss: 211.2584 | l_Loss: 29.3836 | 
21-12-25 08:48:37.365 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:48:37.366 - INFO: Train epoch 2716:   Loss: 456.0974 | r_Loss: 45.1423 | g_Loss: 203.7995 | l_Loss: 26.5866 | 
21-12-25 08:49:50.930 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:49:50.931 - INFO: Train epoch 2717:   Loss: 454.0880 | r_Loss: 44.2597 | g_Loss: 205.1025 | l_Loss: 27.6869 | 
21-12-25 08:51:04.471 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:51:04.472 - INFO: Train epoch 2718:   Loss: 513.0525 | r_Loss: 53.3520 | g_Loss: 221.8994 | l_Loss: 24.3933 | 
21-12-25 08:52:17.906 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:52:17.907 - INFO: Train epoch 2719:   Loss: 478.2975 | r_Loss: 48.2926 | g_Loss: 213.5096 | l_Loss: 23.3252 | 
21-12-25 08:53:31.479 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:53:31.481 - INFO: Train epoch 2720:   Loss: 509.7644 | r_Loss: 53.0184 | g_Loss: 221.9999 | l_Loss: 22.6725 | 
21-12-25 08:54:44.920 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:54:44.921 - INFO: Train epoch 2721:   Loss: 495.6359 | r_Loss: 50.5231 | g_Loss: 213.4139 | l_Loss: 29.6067 | 
21-12-25 08:55:58.493 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:55:58.494 - INFO: Train epoch 2722:   Loss: 488.6997 | r_Loss: 50.3069 | g_Loss: 208.1030 | l_Loss: 29.0620 | 
21-12-25 08:57:12.270 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:57:12.272 - INFO: Train epoch 2723:   Loss: 427.4884 | r_Loss: 41.6634 | g_Loss: 194.4649 | l_Loss: 24.7066 | 
21-12-25 08:58:25.767 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:58:25.768 - INFO: Train epoch 2724:   Loss: 546.6820 | r_Loss: 60.6736 | g_Loss: 216.0492 | l_Loss: 27.2648 | 
21-12-25 08:59:39.086 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 08:59:39.087 - INFO: Train epoch 2725:   Loss: 440.5881 | r_Loss: 43.6274 | g_Loss: 197.7720 | l_Loss: 24.6794 | 
21-12-25 09:00:52.495 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:00:52.497 - INFO: Train epoch 2726:   Loss: 461.5932 | r_Loss: 48.2919 | g_Loss: 198.8651 | l_Loss: 21.2686 | 
21-12-25 09:02:06.076 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:02:06.077 - INFO: Train epoch 2727:   Loss: 468.4431 | r_Loss: 46.8816 | g_Loss: 211.8835 | l_Loss: 22.1518 | 
21-12-25 09:03:19.397 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:03:19.398 - INFO: Train epoch 2728:   Loss: 478.2073 | r_Loss: 47.8581 | g_Loss: 212.6425 | l_Loss: 26.2743 | 
21-12-25 09:04:32.640 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:04:32.641 - INFO: Train epoch 2729:   Loss: 477.3383 | r_Loss: 49.7978 | g_Loss: 203.2278 | l_Loss: 25.1217 | 
21-12-25 09:06:22.280 - INFO: TEST:   PSNR_S: 46.8635 | PSNR_C: 38.8173 | 
21-12-25 09:06:22.281 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:06:22.281 - INFO: Train epoch 2730:   Loss: 692.8583 | r_Loss: 82.4746 | g_Loss: 248.9869 | l_Loss: 31.4985 | 
21-12-25 09:07:35.611 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:07:35.612 - INFO: Train epoch 2731:   Loss: 474.2635 | r_Loss: 47.8284 | g_Loss: 207.6040 | l_Loss: 27.5175 | 
21-12-25 09:08:49.442 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:08:49.444 - INFO: Train epoch 2732:   Loss: 439.8822 | r_Loss: 44.3799 | g_Loss: 199.5445 | l_Loss: 18.4380 | 
21-12-25 09:10:02.852 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:10:02.853 - INFO: Train epoch 2733:   Loss: 472.4011 | r_Loss: 46.0822 | g_Loss: 211.9969 | l_Loss: 29.9933 | 
21-12-25 09:11:16.458 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:11:16.459 - INFO: Train epoch 2734:   Loss: 488.4403 | r_Loss: 49.4366 | g_Loss: 214.5422 | l_Loss: 26.7153 | 
21-12-25 09:12:30.000 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:12:30.001 - INFO: Train epoch 2735:   Loss: 447.6338 | r_Loss: 44.9180 | g_Loss: 199.0901 | l_Loss: 23.9534 | 
21-12-25 09:13:43.995 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:13:43.996 - INFO: Train epoch 2736:   Loss: 447.8890 | r_Loss: 44.4735 | g_Loss: 201.4309 | l_Loss: 24.0908 | 
21-12-25 09:14:57.528 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:14:57.530 - INFO: Train epoch 2737:   Loss: 475.8793 | r_Loss: 49.6348 | g_Loss: 202.9841 | l_Loss: 24.7214 | 
21-12-25 09:16:10.674 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:16:10.676 - INFO: Train epoch 2738:   Loss: 479.4847 | r_Loss: 49.2509 | g_Loss: 212.3667 | l_Loss: 20.8634 | 
21-12-25 09:17:24.307 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:17:24.309 - INFO: Train epoch 2739:   Loss: 469.8435 | r_Loss: 49.1271 | g_Loss: 199.9245 | l_Loss: 24.2836 | 
21-12-25 09:18:37.856 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:18:37.858 - INFO: Train epoch 2740:   Loss: 498.9031 | r_Loss: 53.7568 | g_Loss: 204.7199 | l_Loss: 25.3991 | 
21-12-25 09:19:51.637 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:19:51.639 - INFO: Train epoch 2741:   Loss: 484.2422 | r_Loss: 48.6064 | g_Loss: 215.9395 | l_Loss: 25.2709 | 
21-12-25 09:21:04.880 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:21:04.880 - INFO: Train epoch 2742:   Loss: 507.0431 | r_Loss: 53.3761 | g_Loss: 214.1817 | l_Loss: 25.9806 | 
21-12-25 09:22:18.550 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:22:18.551 - INFO: Train epoch 2743:   Loss: 487.0943 | r_Loss: 50.2155 | g_Loss: 212.1295 | l_Loss: 23.8871 | 
21-12-25 09:23:31.953 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:23:31.954 - INFO: Train epoch 2744:   Loss: 470.2634 | r_Loss: 46.7981 | g_Loss: 205.6625 | l_Loss: 30.6101 | 
21-12-25 09:24:45.207 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:24:45.208 - INFO: Train epoch 2745:   Loss: 476.9521 | r_Loss: 49.9028 | g_Loss: 202.1049 | l_Loss: 25.3330 | 
21-12-25 09:25:58.588 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:25:58.589 - INFO: Train epoch 2746:   Loss: 450.8879 | r_Loss: 44.8835 | g_Loss: 200.8577 | l_Loss: 25.6126 | 
21-12-25 09:27:12.465 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:27:12.466 - INFO: Train epoch 2747:   Loss: 525.4247 | r_Loss: 57.3748 | g_Loss: 214.5028 | l_Loss: 24.0478 | 
21-12-25 09:28:26.253 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:28:26.254 - INFO: Train epoch 2748:   Loss: 544.0491 | r_Loss: 59.7761 | g_Loss: 221.2333 | l_Loss: 23.9356 | 
21-12-25 09:29:40.193 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:29:40.194 - INFO: Train epoch 2749:   Loss: 493.7104 | r_Loss: 50.4416 | g_Loss: 211.4403 | l_Loss: 30.0622 | 
21-12-25 09:30:54.167 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:30:54.168 - INFO: Train epoch 2750:   Loss: 437.8213 | r_Loss: 42.5260 | g_Loss: 196.2963 | l_Loss: 28.8950 | 
21-12-25 09:32:07.746 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:32:07.747 - INFO: Train epoch 2751:   Loss: 467.4733 | r_Loss: 45.9122 | g_Loss: 209.6056 | l_Loss: 28.3064 | 
21-12-25 09:33:21.279 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:33:21.280 - INFO: Train epoch 2752:   Loss: 469.2616 | r_Loss: 46.7846 | g_Loss: 209.4615 | l_Loss: 25.8772 | 
21-12-25 09:34:34.243 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:34:34.244 - INFO: Train epoch 2753:   Loss: 490.2742 | r_Loss: 50.0669 | g_Loss: 210.8956 | l_Loss: 29.0442 | 
21-12-25 09:35:47.996 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:35:47.997 - INFO: Train epoch 2754:   Loss: 539.8011 | r_Loss: 51.9971 | g_Loss: 250.4828 | l_Loss: 29.3325 | 
21-12-25 09:37:01.592 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:37:01.593 - INFO: Train epoch 2755:   Loss: 454.9907 | r_Loss: 44.6469 | g_Loss: 204.6962 | l_Loss: 27.0600 | 
21-12-25 09:38:14.700 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:38:14.702 - INFO: Train epoch 2756:   Loss: 435.5329 | r_Loss: 44.3764 | g_Loss: 188.7650 | l_Loss: 24.8857 | 
21-12-25 09:39:28.110 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:39:28.111 - INFO: Train epoch 2757:   Loss: 483.7542 | r_Loss: 52.3790 | g_Loss: 200.9783 | l_Loss: 20.8810 | 
21-12-25 09:40:41.635 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:40:41.636 - INFO: Train epoch 2758:   Loss: 462.2755 | r_Loss: 47.4588 | g_Loss: 202.0374 | l_Loss: 22.9443 | 
21-12-25 09:41:54.838 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:41:54.839 - INFO: Train epoch 2759:   Loss: 464.3709 | r_Loss: 47.9350 | g_Loss: 200.9198 | l_Loss: 23.7760 | 
21-12-25 09:43:44.400 - INFO: TEST:   PSNR_S: 33.9373 | PSNR_C: 30.9399 | 
21-12-25 09:43:44.402 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:43:44.403 - INFO: Train epoch 2760:   Loss: 9759.0377 | r_Loss: 1845.9202 | g_Loss: 467.6803 | l_Loss: 61.7570 | 
21-12-25 09:44:58.022 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:44:58.023 - INFO: Train epoch 2761:   Loss: 1564.9100 | r_Loss: 181.5903 | g_Loss: 577.4706 | l_Loss: 79.4876 | 
21-12-25 09:46:11.937 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:46:11.938 - INFO: Train epoch 2762:   Loss: 984.3894 | r_Loss: 101.1467 | g_Loss: 427.9016 | l_Loss: 50.7546 | 
21-12-25 09:47:25.054 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:47:25.055 - INFO: Train epoch 2763:   Loss: 855.4714 | r_Loss: 80.6128 | g_Loss: 399.5837 | l_Loss: 52.8238 | 
21-12-25 09:48:38.702 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:48:38.703 - INFO: Train epoch 2764:   Loss: 743.3869 | r_Loss: 68.0136 | g_Loss: 354.7352 | l_Loss: 48.5839 | 
21-12-25 09:49:51.780 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:49:51.781 - INFO: Train epoch 2765:   Loss: 708.5815 | r_Loss: 63.3438 | g_Loss: 339.8338 | l_Loss: 52.0289 | 
21-12-25 09:51:05.401 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:51:05.402 - INFO: Train epoch 2766:   Loss: 734.7515 | r_Loss: 67.5651 | g_Loss: 353.0044 | l_Loss: 43.9214 | 
21-12-25 09:52:18.930 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:52:18.931 - INFO: Train epoch 2767:   Loss: 700.1715 | r_Loss: 63.3429 | g_Loss: 333.9655 | l_Loss: 49.4915 | 
21-12-25 09:53:32.112 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:53:32.113 - INFO: Train epoch 2768:   Loss: 665.8520 | r_Loss: 60.3400 | g_Loss: 325.8033 | l_Loss: 38.3489 | 
21-12-25 09:54:45.771 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:54:45.773 - INFO: Train epoch 2769:   Loss: 686.2235 | r_Loss: 61.7587 | g_Loss: 332.0010 | l_Loss: 45.4288 | 
21-12-25 09:55:59.124 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:55:59.125 - INFO: Train epoch 2770:   Loss: 653.9449 | r_Loss: 60.4577 | g_Loss: 308.6261 | l_Loss: 43.0304 | 
21-12-25 09:57:12.401 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:57:12.401 - INFO: Train epoch 2771:   Loss: 617.5961 | r_Loss: 57.3774 | g_Loss: 297.2568 | l_Loss: 33.4521 | 
21-12-25 09:58:25.802 - INFO: Learning rate: 6.30957344480193e-06
21-12-25 09:58:25.803 - INFO: Train epoch 2772:   Loss: 577.6340 | r_Loss: 53.7793 | g_Loss: 280.5089 | l_Loss: 28.2287 | 
